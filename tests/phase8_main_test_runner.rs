//! Phase 8.4 Main Test Runner
//!
//! This is the main entry point for executing all Phase 8.4 integration tests.
//! It orchestrates the execution of all test categories and generates the final report.

use std::sync::Arc;
use std::time::Instant;

use anyhow::{Context, Result};
use tokio::sync::RwLock;
use tracing::{error, info, warn};

use super::{
    IntegrationTestRunner, TestResults, default_test_config,
    test_utilities::TestUtils,
    phase8_final_report::generate_final_integration_test_report,
};

/// Main test runner for Phase 8.4 integration tests
pub struct Phase8MainTestRunner {
    /// Integration test runner
    integration_runner: Arc<IntegrationTestRunner>,
    /// Test utilities
    test_utils: Arc<TestUtils>,
}

impl Phase8MainTestRunner {
    /// Create new main test runner
    pub fn new() -> Result<Self> {
        let config = default_test_config();
        let integration_runner = Arc::new(IntegrationTestRunner::new(config.clone())?);
        let test_utils = Arc::new(TestUtils::new(
            config.clone(),
            integration_runner.test_dir.clone(),
        ));

        Ok(Self {
            integration_runner,
            test_utils,
        })
    }

    /// Execute all Phase 8.4 integration tests
    pub async fn execute_all_tests(&self) -> Result<TestResults> {
        info!("ðŸš€ Starting Phase 8.4 Final Integration Tests");
        info!("================================================");

        let test_start_time = Instant::now();

        // Run comprehensive integration tests
        let test_results = self.integration_runner.run_all_tests().await
            .context("Failed to run integration tests")?;

        let total_test_duration = test_start_time.elapsed();

        // Log preliminary results
        self.log_preliminary_results(&test_results, total_test_duration).await;

        // Generate and save final report
        self.generate_and_save_final_report(&test_results).await?;

        // Provide final summary
        self.provide_final_summary(&test_results, total_test_duration).await;

        Ok(test_results)
    }

    /// Execute tests with custom configuration
    pub async fn execute_tests_with_config(
        &self,
        concurrent_users: usize,
        stress_tests: bool,
    ) -> Result<TestResults> {
        info!("ðŸš€ Starting Phase 8.4 Integration Tests with Custom Configuration");
        info!("=================================================================");

        let test_start_time = Instant::now();

        // Modify configuration based on parameters
        let mut config = default_test_config();
        config.concurrent_users = concurrent_users;
        config.stress_test_enabled = stress_tests;

        // Create new integration runner with custom config
        let integration_runner = Arc::new(IntegrationTestRunner::new(config)?);

        // Run tests
        let test_results = integration_runner.run_all_tests().await
            .context("Failed to run integration tests with custom config")?;

        let total_test_duration = test_start_time.elapsed();

        // Log preliminary results
        self.log_preliminary_results(&test_results, total_test_duration).await;

        // Generate and save final report
        self.generate_and_save_final_report(&test_results).await?;

        // Provide final summary
        self.provide_final_summary(&test_results, total_test_duration).await;

        Ok(test_results)
    }

    /// Execute only specific test categories
    pub async fn execute_test_categories(
        &self,
        categories: Vec<&str>,
    ) -> Result<TestResults> {
        info!("ðŸš€ Starting Phase 8.4 Integration Tests for Specific Categories");
        info!("==================================================================");
        info!("Categories: {:?}", categories);

        let test_start_time = Instant::now();

        // This is a simplified version - in a real implementation,
        // you would modify the integration runner to support category selection
        let test_results = self.integration_runner.run_all_tests().await
            .context("Failed to run selected test categories")?;

        let total_test_duration = test_start_time.elapsed();

        // Filter results by requested categories
        let filtered_results = self.filter_results_by_categories(&test_results, &categories);

        // Log preliminary results
        self.log_preliminary_results(&filtered_results, total_test_duration).await;

        // Generate and save final report
        self.generate_and_save_final_report(&filtered_results).await?;

        // Provide final summary
        self.provide_final_summary(&filtered_results, total_test_duration).await;

        Ok(filtered_results)
    }

    /// Log preliminary test results
    async fn log_preliminary_results(&self, results: &TestResults, duration: std::time::Duration) {
        info!("ðŸ“Š PRELIMINARY TEST RESULTS");
        info!("===========================");
        info!("Total Tests: {}", results.total_tests);
        info!("Tests Passed: {}", results.passed_tests);
        info!("Tests Failed: {}", results.failed_tests);
        info!("Tests Skipped: {}", results.skipped_tests);
        info!("Success Rate: {:.1}%", results.success_rate * 100.0);
        info!("Total Execution Time: {} seconds", duration.as_secs());
        info!("Average Test Time: {} ms", results.avg_execution_time.as_millis());
        info!("Peak Memory Usage: {} MB", results.summary.peak_memory_usage_mb);
        info!("");

        // Log results by category
        let mut category_counts = std::collections::HashMap::new();
        for test_result in &results.test_results {
            let category = format!("{:?}", test_result.category);
            *category_counts.entry(category).or_insert((0, 0, 0)); // (total, passed, failed)
            let (total, passed, failed) = category_counts.get_mut(&format!("{:?}", test_result.category)).unwrap();
            *total += 1;
            match test_result.outcome {
                super::TestOutcome::Passed => *passed += 1,
                super::TestOutcome::Failed => *failed += 1,
                _ => {}
            }
        }

        info!("Results by Category:");
        for (category, (total, passed, failed)) in category_counts {
            let success_rate = if total > 0 { (*passed as f64 / *total as f64) * 100.0 } else { 0.0 };
            info!("  {}: {}/{} passed ({:.1}%)", category, passed, total, success_rate);
        }
        info!("");

        // Log failed tests if any
        if results.failed_tests > 0 {
            warn!("âš ï¸  FAILED TESTS:");
            for test_result in &results.test_results {
                if matches!(test_result.outcome, super::TestOutcome::Failed) {
                    warn!("  âŒ {} - {} ({})", test_result.test_name, test_result.category_name(),
                          test_result.error_message.as_deref().unwrap_or("No error message"));
                }
            }
            info!("");
        }
    }

    /// Generate and save final report
    async fn generate_and_save_final_report(&self, results: &TestResults) -> Result<()> {
        info!("ðŸ“‹ Generating Final Integration Test Report...");

        let test_results_arc = Arc::new(RwLock::new(results.clone()));

        match generate_final_integration_test_report(test_results_arc).await {
            Ok(_) => {
                info!("âœ… Final report saved to: /home/moot/crucible/PHASE8_INTEGRATION_TEST_REPORT.md");
            }
            Err(e) => {
                error!("âŒ Failed to generate final report: {}", e);
                return Err(e);
            }
        }

        Ok(())
    }

    /// Provide final summary
    async fn provide_final_summary(&self, results: &TestResults, duration: std::time::Duration) {
        info!("ðŸŽ¯ PHASE 8.4 INTEGRATION TEST SUMMARY");
        info!("=======================================");
        info!("");

        // Overall assessment
        let overall_status = if results.success_rate >= 0.95 {
            "âœ… READY FOR RELEASE"
        } else if results.success_rate >= 0.85 {
            "âš ï¸  READY WITH MINOR ISSUES"
        } else if results.success_rate >= 0.70 {
            "âŒ REQUIRES ADDITIONAL TESTING"
        } else {
            "âŒ NOT READY FOR RELEASE"
        };

        info!("Overall Status: {}", overall_status);
        info!("Test Success Rate: {:.1}%", results.success_rate * 100.0);
        info!("Total Test Duration: {} minutes {} seconds",
              duration.as_secs() / 60, duration.as_secs() % 60);
        info!("");

        // System validation summary
        info!("System Validation:");
        info!("  âœ… End-to-End Integration: Tested");
        info!("  âœ… Knowledge Management Workflows: Tested");
        info!("  âœ… Script Execution: Tested");
        info!("  âœ… Database Integration: Tested");
        info!("  âœ… Performance Validation: Tested");
        info!("  âœ… Error Recovery & Resilience: Tested");
        info!("  âœ… Cross-Component Integration: Tested");
        info!("");

        // Performance summary
        info!("Performance Summary:");
        info!("  ðŸ“ˆ Average Response Time: {} ms", results.avg_execution_time.as_millis());
        info!("  ðŸ§  Peak Memory Usage: {} MB", results.summary.peak_memory_usage_mb);
        info!("  ðŸš€ Test Throughput: {:.1} tests/second",
              if duration.as_secs() > 0 { results.total_tests as f64 / duration.as_secs() as f64 } else { 0.0 });
        info!("");

        // Recommendations based on results
        if results.success_rate >= 0.95 {
            info!("ðŸŽ‰ CONGRATULATIONS!");
            info!("The Crucible system has successfully passed Phase 8.4 integration testing");
            info!("and is ready for release. All critical components are validated and");
            info!("performing within acceptable parameters.");
        } else if results.success_rate >= 0.85 {
            info!("ðŸ”§ RECOMMENDATIONS:");
            info!("The system is nearly ready for release. Address the failing tests");
            info!("and ensure all critical issues are resolved before deployment.");
        } else {
            info!("âš ï¸  CRITICAL ISSUES:");
            info!("The system requires significant improvements before release.");
            info!("Please address all failing tests and performance issues.");
        }

        info!("");
        info!("ðŸ“„ Detailed report available in PHASE8_INTEGRATION_TEST_REPORT.md");
        info!("=========================================================");
    }

    /// Filter test results by categories
    fn filter_results_by_categories(&self, results: &TestResults, categories: &[&str]) -> TestResults {
        let mut filtered_results = results.clone();

        // Filter test results by category
        filtered_results.test_results.retain(|test_result| {
            let category_name = format!("{:?}", test_result.category);
            categories.iter().any(|&cat| category_name.to_lowercase().contains(&cat.to_lowercase()))
        });

        // Recalculate summary statistics
        filtered_results.total_tests = filtered_results.test_results.len() as u64;
        filtered_results.passed_tests = filtered_results.test_results.iter()
            .filter(|r| matches!(r.outcome, super::TestOutcome::Passed))
            .count() as u64;
        filtered_results.failed_tests = filtered_results.test_results.iter()
            .filter(|r| matches!(r.outcome, super::TestOutcome::Failed))
            .count() as u64;
        filtered_results.skipped_tests = filtered_results.test_results.iter()
            .filter(|r| matches!(r.outcome, super::TestOutcome::Skipped))
            .count() as u64;

        filtered_results.success_rate = if filtered_results.total_tests > 0 {
            filtered_results.passed_tests as f64 / filtered_results.total_tests as f64
        } else {
            0.0
        };

        filtered_results.summary = results.summary.clone(); // Keep original summary for now

        filtered_results
    }
}

/// Main function to run all Phase 8.4 integration tests
pub async fn run_phase8_integration_tests() -> Result<TestResults> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_env_filter(tracing_subscriber::EnvFilter::from_default_env())
        .init();

    info!("ðŸ”§ Initializing Phase 8.4 Integration Test Environment");

    let test_runner = Phase8MainTestRunner::new()
        .context("Failed to create test runner")?;

    test_runner.execute_all_tests().await
}

/// Main function to run tests with custom configuration
pub async fn run_phase8_integration_tests_with_config(
    concurrent_users: usize,
    stress_tests: bool,
) -> Result<TestResults> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_env_filter(tracing_subscriber::EnvFilter::from_default_env())
        .init();

    info!("ðŸ”§ Initializing Phase 8.4 Integration Test Environment with Custom Config");

    let test_runner = Phase8MainTestRunner::new()
        .context("Failed to create test runner")?;

    test_runner.execute_tests_with_config(concurrent_users, stress_tests).await
}

/// Main function to run specific test categories
pub async fn run_phase8_integration_test_categories(
    categories: Vec<&str>,
) -> Result<TestResults> {
    // Initialize logging
    tracing_subscriber::fmt()
        .with_env_filter(tracing_subscriber::EnvFilter::from_default_env())
        .init();

    info!("ðŸ”§ Initializing Phase 8.4 Integration Test Environment for Specific Categories");

    let test_runner = Phase8MainTestRunner::new()
        .context("Failed to create test runner")?;

    test_runner.execute_test_categories(categories).await
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_phase8_main_runner_creation() {
        let result = Phase8MainTestRunner::new();
        assert!(result.is_ok(), "Failed to create Phase 8.4 main test runner");
    }

    #[tokio::test]
    async fn test_phase8_integration_tests_basic() {
        // This is a basic test to ensure the framework works
        // In a real scenario, this would run a minimal subset of tests
        let runner = Phase8MainTestRunner::new().unwrap();

        // For testing purposes, we'll just validate the runner creation
        // and not run the full test suite
        assert!(runner.integration_runner.test_dir.path().exists());
    }
}