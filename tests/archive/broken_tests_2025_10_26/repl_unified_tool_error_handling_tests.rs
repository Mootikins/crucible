//! Comprehensive Error Handling Tests for Unified REPL Tool System
//!
//! This module implements TDD-style tests for all error scenarios in the unified tool system:
//! - Missing tools
//! - Parameter conversion failures
//! - Tool execution failures
//! - Group initialization failures
//! - Cache invalidation scenarios
//!
//! Tests are written to FAIL FIRST, then the corresponding error handling will be implemented.


    UnifiedToolRegistry, ToolGroupRegistry, ToolGroupError, SystemToolGroup,
    ToolGroupCacheConfig, ToolGroup
};

/// Test context for error handling tests
struct ErrorHandlingTestContext {
    temp_dir: TempDir,
    tool_dir: PathBuf,
}

impl ErrorHandlingTestContext {
    fn new() -> Result<Self> {
        let temp_dir = TempDir::new()?;
        let tool_dir = temp_dir.path().join("tools");

        // Create directories
        std::fs::create_dir_all(&tool_dir)?;

        Ok(Self {
            temp_dir,
            tool_dir,
        })
    }
}

// =============================================================================
// MISSING TOOLS ERROR HANDLING TESTS
// =============================================================================

/// Test Case 1: Tool not found in any group (system tools, Rune tools, or MCP servers)
///
/// Expected behavior:
/// - Should return ToolGroupError::ToolNotFound with descriptive message
/// - Error should indicate which groups were searched
/// - Should maintain graceful degradation (system remains functional)
#[tokio::test]
async fn test_missing_tool_comprehensive_error() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;

    // Initialize unified registry
    let registry = UnifiedToolRegistry::new(context.tool_dir).await?;

    // Test non-existent tool
    let result = registry.execute_tool("definitely_nonexistent_tool_xyz", &[]).await;

    // This test should FAIL initially - the error handling needs improvement
    assert!(result.is_err(), "Should fail for non-existent tool");

    let error = result.unwrap_err();
    let error_msg = error.to_string();

    // Check for user-friendly error message
    assert!(error_msg.contains("not found"), "Error should mention tool not found");
    assert!(error_msg.contains("definitely_nonexistent_tool_xyz"), "Error should include tool name");

    // Should indicate which sources were searched
    assert!(error_msg.contains("system") || error_msg.contains("group") || error_msg.contains("rune"),
            "Error should mention which groups were searched");

    // Verify system is still functional after error
    let tools = registry.list_tools().await;
    assert!(!tools.is_empty(), "System should remain functional after missing tool error");

    println!("âœ… Missing tool error handling test passed: {}", error_msg);
    Ok(())
}

/// Test Case 2: Tool group lookup for missing tool
#[tokio::test]
async fn test_missing_tool_group_lookup() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;
    let registry = UnifiedToolRegistry::new(context.tool_dir).await?;

    // Test group lookup for non-existent tool
    let group_name = registry.get_tool_group("missing_tool_abc").await;

    // Should return None for missing tool
    assert!(group_name.is_none(), "Should return None for missing tool");

    // Test with tool that exists but group lookup fails
    let tools = registry.list_tools().await;
    if let Some(first_tool) = tools.first() {
        // Force a scenario where group lookup might fail
        // This tests the error path in get_tool_group
        let result = registry.execute_tool(first_tool, &[]).await;

        // Should either succeed or fail gracefully, not panic
        match result {
            Ok(_) => println!("âœ… Tool executed successfully"),
            Err(e) => println!("âœ… Tool failed gracefully: {}", e.to_string()),
        }
    }

    Ok(())
}

/// Test Case 3: Tool schema lookup for missing tool
#[tokio::test]
async fn test_missing_tool_schema_error() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;
    let _registry = UnifiedToolRegistry::new(context.tool_dir.clone()).await?;

    // Create group registry to test schema errors directly
    let group_registry = ToolGroupRegistry::new();

    // Test schema lookup for non-existent tool through group registry
    let schema_result = group_registry.get_tool_schema("nonexistent_tool").await;

    // Should return ToolNotFound error
    assert!(schema_result.is_err(), "Should fail for missing tool schema");

    if let Err(ToolGroupError::ToolNotFound(msg)) = schema_result {
        assert!(msg.contains("nonexistent_tool"), "Error should mention tool name");
    } else {
        panic!("Expected ToolNotFound error, got: {:?}", schema_result);
    }

    Ok(())
}

// =============================================================================
// PARAMETER CONVERSION FAILURE TESTS
// =============================================================================

/// Test Case 4: Parameter conversion failures for various tool types
#[tokio::test]
async fn test_parameter_conversion_failures() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;
    let registry = UnifiedToolRegistry::new(context.tool_dir).await?;

    // Test case 4a: Too many arguments for no-argument tool
    let result = registry.execute_tool("system_info", &["extra_arg".to_string()]).await;
    assert!(result.is_err(), "Should fail with extra arguments");

    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("argument") || error_msg.contains("parameter"),
            "Error should mention argument issue: {}", error_msg);

    // Test case 4b: Missing required arguments
    let result = registry.execute_tool("list_files", &[]).await;
    assert!(result.is_err(), "Should fail with missing arguments");

    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("argument") || error_msg.contains("required") || error_msg.contains("path"),
            "Error should mention missing required argument: {}", error_msg);

    // Test case 4c: Invalid argument types (this would be more relevant for schema validation)
    let result = registry.execute_tool("create_note", &[]).await;
    assert!(result.is_err(), "Should fail with insufficient arguments");

    // Test case 4d: Complex tool with multiple missing arguments
    let result = registry.execute_tool("create_note", &["only_one_arg".to_string()]).await;
    assert!(result.is_err(), "Should fail with insufficient arguments for complex tool");

    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("argument") || error_msg.contains("required"),
            "Error should mention argument requirements: {}", error_msg);

    println!("âœ… Parameter conversion error handling tests passed");
    Ok(())
}

/// Test Case 5: Parameter validation failures
#[tokio::test]
async fn test_parameter_validation_failures() -> Result<()> {
    let _context = ErrorHandlingTestContext::new()?;

    // Create SystemToolGroup directly to test parameter validation
    let mut system_group = SystemToolGroup::new();
    system_group.initialize().await?;

    // Test parameter validation for various tools
    let test_cases = vec![
        // (tool_name, args, expected_error_keyword)
        ("list_files", vec![], "required"),
        ("list_files", vec!["arg1".to_string(), "arg2".to_string()], "argument"),
        ("read_file", vec![], "required"),
        ("execute_command", vec![], "required"),
        ("create_note", vec![], "required"),
        ("create_note", vec!["path".to_string()], "required"),
        ("create_note", vec!["path".to_string(), "title".to_string()], "required"),
    ];

    for (tool_name, args, expected_keyword) in test_cases {
        let result = system_group.execute_tool(tool_name, &args).await;
        assert!(result.is_err(), "Tool '{}' should fail with args: {:?}", tool_name, args);

        let error_msg = result.unwrap_err().to_string();
        assert!(error_msg.contains(expected_keyword) || error_msg.contains("argument") || error_msg.contains("parameter"),
                "Error for '{}' should contain '{}': {}", tool_name, expected_keyword, error_msg);
    }

    println!("âœ… Parameter validation failure tests passed");
    Ok(())
}

// =============================================================================
// TOOL EXECUTION FAILURE TESTS
// =============================================================================

/// Test Case 6: Tool execution failures due to various reasons
#[tokio::test]
async fn test_tool_execution_failures() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;
    let registry = UnifiedToolRegistry::new(context.tool_dir).await?;

    // Test case 6a: Tool execution with invalid file path
    let result = registry.execute_tool("list_files", &["/definitely/invalid/path/that/does/not/exist/xyz123".to_string()]).await;
    assert!(result.is_err(), "Should fail with invalid path");

    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("failed") || error_msg.contains("error") || error_msg.contains("not found"),
            "Error should indicate execution failure: {}", error_msg);

    // Test case 6b: Tool execution with permission issues (try reading a protected file)
    let result = registry.execute_tool("read_file", &["/etc/shadow".to_string()]).await;

    // This might succeed or fail depending on the test environment,
    // but it should handle either case gracefully
    match result {
        Ok(_) => println!("âœ… Protected file read succeeded (running as privileged user)"),
        Err(e) => {
            let error_msg = e.to_string();
            assert!(error_msg.contains("permission") || error_msg.contains("denied") || error_msg.contains("failed") || error_msg.contains("error"),
                    "Error should be informative: {}", error_msg);
        }
    }

    // Test case 6c: Tool execution timeout simulation
    // This tests the system's ability to handle long-running operations
    let start_time = std::time::Instant::now();
    let result = registry.execute_tool("search_documents", &["query_that_might_take_long_time".to_string()]).await;
    let execution_time = start_time.elapsed();

    // Should either succeed or fail within reasonable time
    assert!(execution_time < Duration::from_secs(30), "Execution should complete within 30 seconds");

    match result {
        Ok(_) => println!("âœ… Long-running query completed in {:?}", execution_time),
        Err(e) => println!("âœ… Long-running query failed gracefully: {}", e.to_string()),
    }

    println!("âœ… Tool execution failure tests passed");
    Ok(())
}

/// Test Case 7: Crucible-tools backend failures
#[tokio::test]
async fn test_crucible_tools_backend_failures() -> Result<()> {
    let _context = ErrorHandlingTestContext::new()?;

    // Create SystemToolGroup and simulate various failure scenarios
    let mut system_group = SystemToolGroup::new();

    // Test with uninitialized group
    let result = system_group.execute_tool("system_info", &[]).await;
    assert!(result.is_err(), "Should fail with uninitialized group");

    if let Err(ToolGroupError::InitializationFailed(msg)) = result {
        assert!(msg.contains("not initialized"), "Error should mention initialization");
    } else {
        panic!("Expected InitializationFailed error, got: {:?}", result);
    }

    // Initialize the group
    system_group.initialize().await?;

    // Test execution of a tool that might not exist in crucible-tools
    let result = system_group.execute_tool("nonexistent_crucible_tool", &[]).await;
    assert!(result.is_err(), "Should fail for non-existent crucible tool");

    let error_msg = result.unwrap_err().to_string();
    assert!(error_msg.contains("not found") || error_msg.contains("failed") || error_msg.contains("error"),
            "Error should be descriptive: {}", error_msg);

    println!("âœ… Crucible-tools backend failure tests passed");
    Ok(())
}

// =============================================================================
// GROUP INITIALIZATION FAILURE TESTS
// =============================================================================

/// Test Case 8: ToolGroupRegistry initialization failures
#[tokio::test]
async fn test_tool_group_initialization_failures() -> Result<()> {
    let registry = ToolGroupRegistry::new();

    // Test case 8a: Execute tool when no groups are registered
    let result = registry.execute_tool("any_tool", &[]).await;
    assert!(result.is_err(), "Should fail when no groups are registered");

    if let Err(ToolGroupError::ToolNotFound(msg)) = result {
        assert!(msg.contains("not found") || msg.contains("any_tool"), "Error should mention tool not found");
    } else {
        panic!("Expected ToolNotFound error, got: {:?}", result);
    }

    // Test case 8b: Get schema when no groups are registered
    let result = registry.get_tool_schema("any_tool").await;
    assert!(result.is_err(), "Should fail when no groups are registered");

    // Test case 8c: List tools when no groups are registered (should succeed with empty list)
    let tools = registry.list_all_tools().await;
    assert!(tools.is_empty(), "Should return empty list when no groups are registered");

    // Test case 8d: Get tool group when no groups are registered
    let group_name = registry.get_tool_group("any_tool").await;
    assert!(group_name.is_none(), "Should return None when no groups are registered");

    println!("âœ… ToolGroupRegistry initialization failure tests passed");
    Ok(())
}

/// Test Case 9: SystemToolGroup initialization failures
#[tokio::test]
async fn test_system_tool_group_initialization_failures() -> Result<()> {
    // Test with corrupted cache configuration
    let invalid_cache_config = ToolGroupCacheConfig {
        discovery_ttl: Duration::from_secs(u64::MAX), // Invalid TTL (very large)
        schema_ttl: Duration::from_secs(u64::MAX),    // Invalid TTL (very large)
        max_schema_cache_size: 0,
        caching_enabled: true,
    };

    // This should either initialize successfully (ignoring invalid config)
    // or fail gracefully with a descriptive error
    let mut group1 = SystemToolGroup::with_cache_config(invalid_cache_config.clone());
    let init_result = group1.initialize().await;

    match init_result {
        Ok(()) => println!("âœ… SystemToolGroup initialized despite invalid cache config"),
        Err(e) => {
            let error_msg = e.to_string();
            assert!(error_msg.contains("initialization") || error_msg.contains("cache") || error_msg.contains("config"),
                    "Error should relate to initialization or config: {}", error_msg);
        }
    }

    // Test multiple initialization attempts
    let mut group2 = SystemToolGroup::with_cache_config(invalid_cache_config);
    let init1 = group2.initialize().await;
    let init2 = group2.initialize().await; // Should be idempotent

    // Both should succeed or both should fail with the same error
    match (init1, init2) {
        (Ok(()), Ok(())) => println!("âœ… Multiple initialization attempts handled correctly"),
        (Err(e1), Err(e2)) => {
            assert_eq!(e1.to_string(), e2.to_string(), "Both initialization attempts should fail with same error");
        }
        _ => panic!("Inconsistent initialization behavior"),
    }

    println!("âœ… SystemToolGroup initialization failure tests passed");
    Ok(())
}

// =============================================================================
// CACHE INVALIDATION TESTS
// =============================================================================

/// Test Case 10: Cache invalidation and refresh scenarios
#[tokio::test]
async fn test_cache_invalidation_scenarios() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;

    // Test with fast cache for quicker testing
    let registry = UnifiedToolRegistry::with_fast_cache(context.tool_dir.clone()).await?;

    // Test case 10a: Cache miss scenario
    let tools1 = registry.list_tools().await;
    let tools2 = registry.list_tools().await; // Should use cache

    assert_eq!(tools1.len(), tools2.len(), "Cache should return consistent results");

    // Wait for cache to expire (using fast cache TTL)
    sleep(Duration::from_secs(2)).await;

    let tools3 = registry.list_tools().await; // Should trigger cache refresh
    assert_eq!(tools1.len(), tools3.len(), "Cache refresh should return consistent results");

    println!("âœ… Cache expiration and refresh working correctly");

    // Test case 10b: Force cache refresh
    let mut registry_mut = UnifiedToolRegistry::with_fast_cache(context.tool_dir.clone()).await?;

    // List tools to populate cache
    let tools_before = registry_mut.list_tools().await;

    // Force refresh
    let refresh_result = registry_mut.refresh_all().await;
    assert!(refresh_result.is_ok(), "Force refresh should succeed");

    let tools_after = registry_mut.list_tools().await;
    assert_eq!(tools_before.len(), tools_after.len(), "Tools should be consistent after refresh");

    println!("âœ… Force cache refresh working correctly");

    // Test case 10c: Cache behavior with no caching
    let no_cache_registry = UnifiedToolRegistry::without_cache(context.tool_dir).await?;

    let tools_nc1 = no_cache_registry.list_tools().await;
    let tools_nc2 = no_cache_registry.list_tools().await; // Should always rediscover

    assert_eq!(tools_nc1.len(), tools_nc2.len(), "No-cache mode should return consistent results");

    println!("âœ… No-cache mode working correctly");

    Ok(())
}

/// Test Case 11: Cache corruption scenarios
#[tokio::test]
async fn test_cache_corruption_scenarios() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;

    // Create registry with very short cache TTL for testing
    let cache_config = ToolGroupCacheConfig {
        discovery_ttl: Duration::from_millis(100), // Very short TTL
        schema_ttl: Duration::from_millis(100),    // Very short TTL
        max_schema_cache_size: 2, // Small cache to test eviction
        caching_enabled: true,
    };

    let registry = UnifiedToolRegistry::with_cache_config(context.tool_dir, cache_config).await?;

    // Test cache behavior with rapid access
    for i in 0..5 {
        let tools = registry.list_tools().await;
        assert!(!tools.is_empty(), "Tools should be available on iteration {}", i);

        // Small delay to allow cache to expire between iterations
        sleep(Duration::from_millis(50)).await;
    }

    println!("âœ… Cache corruption handling tests passed");
    Ok(())
}

/// Test Case 12: Schema cache edge cases
#[tokio::test]
async fn test_schema_cache_edge_cases() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;
    let registry = UnifiedToolRegistry::new(context.tool_dir).await?;

    // Test schema lookup for various tool types
    let tools = registry.list_tools().await;

    if let Some(first_tool) = tools.first() {
        // Test valid tool schema lookup
        let group_name = registry.get_tool_group(first_tool).await;
        assert!(group_name.is_some(), "First tool should have a group");

        // This tests the schema retrieval path
        let result = registry.execute_tool(first_tool, &[]).await;
        match result {
            Ok(_) => println!("âœ… Tool execution succeeded (schema was accessible)"),
            Err(e) => println!("âœ… Tool execution failed gracefully: {}", e.to_string()),
        }
    }

    // Test schema cache behavior with multiple lookups
    for _ in 0..3 {
        let tools = registry.list_tools().await;
        if let Some(tool) = tools.first() {
            let group_name = registry.get_tool_group(tool).await;
            // Should consistently return the same group name or None
            match group_name {
                Some(name) => assert!(!name.is_empty(), "Group name should not be empty"),
                None => {}, // Valid for missing tools
            }
        }
    }

    println!("âœ… Schema cache edge case tests passed");
    Ok(())
}

// =============================================================================
// ERROR RECOVERY AND GRACEFUL DEGRADATION TESTS
// =============================================================================

/// Test Case 13: System recovery after errors
#[tokio::test]
async fn test_system_recovery_after_errors() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;
    let registry = UnifiedToolRegistry::new(context.tool_dir.clone()).await?;

    // Test case 13a: System remains functional after multiple errors
    let error_attempts = vec![
        ("nonexistent_tool_1", vec![]),
        ("nonexistent_tool_2", vec!["arg".to_string()]),
        ("list_files", vec![]), // Missing required argument
    ];

    for (tool_name, args) in error_attempts {
        let result = registry.execute_tool(tool_name, &args).await;
        assert!(result.is_err(), "Should fail for error case: {} with {:?}", tool_name, args);
    }

    // System should still be functional
    let tools = registry.list_tools().await;
    assert!(!tools.is_empty(), "System should have tools after error sequence");

    // Should be able to execute a valid tool
    if tools.contains(&"system_info".to_string()) {
        let result = registry.execute_tool("system_info", &[]).await;
        match result {
            Ok(_) => println!("âœ… System recovered and can execute valid tools"),
            Err(e) => println!("âš ï¸  System recovery limited: {}", e.to_string()),
        }
    }

    // Test case 13b: Partial functionality when some components fail
    let mut registry_mut = UnifiedToolRegistry::new(context.tool_dir.clone()).await?;

    // Disable unified mode to test fallback
    registry_mut.set_unified_mode(false);

    // Should still be able to list tools (even if empty)
    let tools = registry_mut.list_tools().await;
    println!("âœ… System maintains basic functionality in fallback mode: {} tools", tools.len());

    // Re-enable unified mode
    registry_mut.set_unified_mode(true);
    let tools_after_reenable = registry_mut.list_tools().await;
    assert!(!tools_after_reenable.is_empty(), "System should recover after re-enabling unified mode");

    println!("âœ… System recovery and graceful degradation tests passed");
    Ok(())
}

/// Test Case 14: Multiple registry instances error handling
#[tokio::test]
async fn test_multiple_registry_instances_error_handling() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;

    // Test creating multiple registry instances
    let registry1 = UnifiedToolRegistry::new(context.tool_dir.clone()).await?;
    let registry2 = UnifiedToolRegistry::new(context.tool_dir.clone()).await?;

    // Both should be able to list tools
    let tools1 = registry1.list_tools().await;
    let tools2 = registry2.list_tools().await;

    assert_eq!(tools1.len(), tools2.len(), "Multiple instances should see same tools");

    // Test error handling across instances
    let result1 = registry1.execute_tool("nonexistent_tool_1", &[]).await;
    let result2 = registry2.execute_tool("nonexistent_tool_2", &[]).await;

    assert!(result1.is_err(), "First instance should fail for missing tool");
    assert!(result2.is_err(), "Second instance should fail for missing tool");

    // Both instances should remain functional
    let tools1_after = registry1.list_tools().await;
    let tools2_after = registry2.list_tools().await;

    assert!(!tools1_after.is_empty(), "First instance should remain functional");
    assert!(!tools2_after.is_empty(), "Second instance should remain functional");

    println!("âœ… Multiple registry instances error handling tests passed");
    Ok(())
}

/// Test Case 14: Sequential access error handling (simplified from concurrent)
#[tokio::test]
async fn test_sequential_access_error_handling() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;

    // Test sequential tool execution with separate registry instances
    for i in 0..5 {
        let registry = UnifiedToolRegistry::new(context.tool_dir.clone()).await?;
        let _result = registry.execute_tool(&format!("nonexistent_tool_{}", i), &[]).await;
    }

    println!("âœ… Sequential access error handling tests passed");
    Ok(())
}

/// Test Case 15: Memory leak prevention in error scenarios
#[tokio::test]
async fn test_memory_leak_prevention_in_errors() -> Result<()> {
    let context = ErrorHandlingTestContext::new()?;

    // Create and destroy multiple registries to test memory cleanup
    for i in 0..10 {
        let registry = UnifiedToolRegistry::new(context.tool_dir.clone()).await?;

        // Perform operations that might allocate memory
        let _tools = registry.list_tools().await;

        // Trigger errors
        let _error_result = registry.execute_tool(&format!("nonexistent_tool_{}", i), &[]).await;

        // Registry should be dropped and cleaned up at end of loop
    }

    // Test cache cleanup with many entries
    let cache_config = ToolGroupCacheConfig {
        discovery_ttl: Duration::from_secs(1),
        schema_ttl: Duration::from_secs(1),
        max_schema_cache_size: 3, // Very small cache
        caching_enabled: true,
    };

    let registry = UnifiedToolRegistry::with_cache_config(context.tool_dir, cache_config).await?;

    // Access many different tools to trigger cache eviction
    let tools = registry.list_tools().await;
    for (i, tool) in tools.iter().take(10).enumerate() {
        if i % 2 == 0 {
            let _result = registry.execute_tool(tool, &[]).await;
        }
    }

    println!("âœ… Memory leak prevention tests passed");
    Ok(())
}

// =============================================================================
// MAIN TEST RUNNER
// =============================================================================

/// Run all error handling tests and report results
#[tokio::test]
async fn run_all_error_handling_tests() -> Result<()> {
    println!("ðŸ§ª Running comprehensive error handling tests for unified REPL tool system...\n");

    let test_results = vec![
        ("Missing Tools", test_missing_tool_comprehensive_error()),
        ("Tool Group Lookup", test_missing_tool_group_lookup()),
        ("Missing Tool Schema", test_missing_tool_schema_error()),
        ("Parameter Conversion", test_parameter_conversion_failures()),
        ("Parameter Validation", test_parameter_validation_failures()),
        ("Tool Execution", test_tool_execution_failures()),
        ("Backend Failures", test_crucible_tools_backend_failures()),
        ("Group Initialization", test_tool_group_initialization_failures()),
        ("SystemToolGroup Init", test_system_tool_group_initialization_failures()),
        ("Cache Invalidation", test_cache_invalidation_scenarios()),
        ("Cache Corruption", test_cache_corruption_scenarios()),
        ("Schema Cache Edge Cases", test_schema_cache_edge_cases()),
        ("System Recovery", test_system_recovery_after_errors()),
        ("Sequential Access", test_sequential_access_error_handling()),
        ("Memory Leak Prevention", test_memory_leak_prevention_in_errors()),
    ];

    let mut passed = 0;
    let mut failed = 0;

    for (test_name, test_fn) in test_results {
        match test_fn {
            Ok(()) => {
                println!("âœ… {}: PASSED", test_name);
                passed += 1;
            }
            Err(e) => {
                println!("âŒ {}: FAILED - {}", test_name, e);
                failed += 1;
            }
        }
    }

    println!("\nðŸ“Š Error Handling Test Results:");
    println!("   Total Tests: {}", passed + failed);
    println!("   Passed: {}", passed);
    println!("   Failed: {}", failed);

    if failed > 0 {
        println!("\nâš ï¸  Some error handling tests failed. These tests are designed to FAIL FIRST");
        println!("   to identify missing error handling. Implement the required error handling");
        println!("   features to make these tests pass.");
    }

    // For now, we expect some tests to fail since we're writing failing tests first
    if failed > 0 {
        println!("\nðŸŽ¯ TDD Progress: {} tests failing, ready for implementation phase", failed);
    } else {
        println!("\nðŸŽ‰ All error handling tests passed! System has comprehensive error handling.");
    }

    Ok(())
}
use anyhow::Result;
use std::path::PathBuf;
use std::time::Duration;
use tempfile::TempDir;
use tokio::time::sleep;
use crucible_cli::commands::repl::tools::{
use tokio::time::sleep;
use crate::test_utilities::{TestContext, MemoryUsage, PerformanceMeasurement, TestDataGenerator, AssertUtils};
