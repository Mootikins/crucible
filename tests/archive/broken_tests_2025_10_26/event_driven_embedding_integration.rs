//! Comprehensive Integration Test for Event-Driven Embedding System
//!
//! This test demonstrates the missing integration between crucible-watch file events
//! and automatic embedding generation in SurrealDB. It's designed to FAIL initially
//! because the event-driven embedding integration is not yet implemented.
//!
//! Phase 1 of TDD: Create a failing test that clearly demonstrates the missing functionality.
//! This test will serve as the specification for what needs to be implemented.

use std::path::PathBuf;
use std::sync::Arc;
use std::time::{Duration, Instant};

use tempfile::TempDir;
use tokio::sync::RwLock;

// Import crucible-watch components
use crucible_watch::{Error, FileEvent, FileEventKind, Result};

// Import event-driven embedding components
use crucible_watch::{
    create_embedding_metadata, generate_document_id, EmbeddingEventMetadata, EmbeddingEventResult,
    EventDrivenEmbeddingConfig,
};

/// Mock embedding provider for testing
#[derive(Debug, Clone)]
struct MockEmbeddingProvider {
    dimensions: usize,
    model_name: String,
    processing_delay: Duration,
}

impl MockEmbeddingProvider {
    fn new(dimensions: usize, model_name: String) -> Self {
        Self {
            dimensions,
            model_name,
            processing_delay: Duration::from_millis(10),
        }
    }

    async fn generate_embedding(&self, content: &str) -> Result<Vec<f32>> {
        // Simulate processing delay
        tokio::time::sleep(self.processing_delay).await;

        // Generate deterministic mock embedding based on content hash
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        content.hash(&mut hasher);
        let hash = hasher.finish();

        let mut embedding = Vec::with_capacity(self.dimensions);
        for i in 0..self.dimensions {
            let value = ((hash >> (i % 64)) % 1000) as f32 / 1000.0;
            embedding.push(value);
        }

        Ok(embedding)
    }
}

/// Test vault structure for embedding integration tests
struct TestVault {
    temp_dir: TempDir,
    vault_path: PathBuf,
    documents_path: PathBuf,
    config_path: PathBuf,
}

impl TestVault {
    async fn new() -> Result<Self> {
        let temp_dir = TempDir::new().map_err(|e| Error::Io(e))?;
        let vault_path = temp_dir.path().to_path_buf();
        let documents_path = vault_path.join("documents");
        let config_path = vault_path.join("config");

        // Create directory structure
        tokio::fs::create_dir_all(&documents_path)
            .await
            .map_err(|e| Error::Io(e))?;
        tokio::fs::create_dir_all(&config_path)
            .await
            .map_err(|e| Error::Io(e))?;

        Ok(Self {
            temp_dir,
            vault_path,
            documents_path,
            config_path,
        })
    }

    async fn create_test_documents(&self) -> Result<Vec<PathBuf>> {
        let test_files = vec![
            (
                "knowledge_base.md",
                r#"---
title: Knowledge Base
tags: [knowledge, reference]
---

# Knowledge Base

This document contains important information about the system.

## Architecture

The system is built using event-driven architecture with:
- File system monitoring
- Automatic embedding generation
- Semantic search capabilities

## Usage

Users can search for content using natural language queries.
The system will return the most relevant documents based on semantic similarity.
"#,
            ),
            (
                "project_notes.md",
                r#"---
title: Project Notes
tags: [project, development]
---

# Project Development Notes

## Current Tasks

- Implement event-driven embedding integration
- Add comprehensive test coverage
- Optimize performance for large document sets

## Technical Details

The embedding system uses vector similarity to find related content.
Each document is converted to a numerical representation that captures semantic meaning.

## Future Improvements

- Add support for more file formats
- Implement hierarchical clustering
- Add real-time collaboration features
"#,
            ),
            (
                "meeting_notes.md",
                r#"---
title: Meeting Notes
tags: [meeting, collaboration]
---

# Team Meeting - 2024-10-24

## Attendees
- Development Team
- Product Management
- QA Team

## Discussion Points

1. **Event-Driven Architecture Review**
   - Current implementation needs integration
   - Performance requirements discussed
   - Timeline: 2 weeks for MVP

2. **Testing Strategy**
   - Comprehensive integration tests required
   - Mock embedding service for testing
   - Performance benchmarks needed

## Action Items

- [x] Create failing integration test
- [ ] Implement event processor
- [ ] Add semantic search functionality
- [ ] Performance testing
"#,
            ),
            (
                "technical_specs.md",
                r#"---
title: Technical Specifications
tags: [technical, specification]
---

# Event-Driven Embedding System Specifications

## Requirements

### Functional Requirements
1. Monitor file system changes in real-time
2. Automatically generate embeddings for supported file types
3. Store embeddings in SurrealDB with proper indexing
4. Provide semantic search capabilities

### Non-Functional Requirements
1. **Performance**: Process events within 100ms
2. **Scalability**: Handle 1000+ concurrent file changes
3. **Reliability**: 99.9% uptime with proper error handling
4. **Security**: Ensure data privacy and access controls

## Technical Architecture

### Components
- **File Watcher**: Monitors file system changes
- **Event Processor**: Converts file events to embedding requests
- **Embedding Service**: Generates vector embeddings
- **Database Layer**: Stores and indexes embeddings
- **Search Engine**: Provides semantic search functionality

### Data Flow
1. File system event detected
2. Event validated and filtered
3. Document content extracted
4. Embedding generated
5. Vector stored in database
6. Index updated for search
"#,
            ),
        ];

        let mut file_paths = Vec::new();

        for (filename, content) in test_files {
            let file_path = self.documents_path.join(filename);
            tokio::fs::write(&file_path, content)
                .await
                .map_err(|e| Error::Io(e))?;
            file_paths.push(file_path);
        }

        Ok(file_paths)
    }

    fn get_vault_path(&self) -> &PathBuf {
        &self.vault_path
    }

    fn get_documents_path(&self) -> &PathBuf {
        &self.documents_path
    }
}

/// Mock database for testing embedding storage and retrieval
struct MockEmbeddingDatabase {
    embeddings: Arc<RwLock<Vec<StoredEmbedding>>>,
}

#[derive(Debug, Clone)]
struct StoredEmbedding {
    document_id: String,
    file_path: PathBuf,
    content: String,
    embedding: Vec<f32>,
    timestamp: chrono::DateTime<chrono::Utc>,
    metadata: EmbeddingEventMetadata,
}

impl MockEmbeddingDatabase {
    fn new() -> Self {
        Self {
            embeddings: Arc::new(RwLock::new(Vec::new())),
        }
    }

    async fn store_embedding(
        &self,
        document_id: &str,
        file_path: &PathBuf,
        content: &str,
        embedding: Vec<f32>,
        metadata: EmbeddingEventMetadata,
    ) -> Result<()> {
        let stored = StoredEmbedding {
            document_id: document_id.to_string(),
            file_path: file_path.clone(),
            content: content.to_string(),
            embedding,
            timestamp: chrono::Utc::now(),
            metadata,
        };

        let mut embeddings = self.embeddings.write().await;
        embeddings.push(stored);
        Ok(())
    }

    async fn semantic_search(&self, query: &str, limit: usize) -> Result<Vec<SearchResult>> {
        let mock_provider = MockEmbeddingProvider::new(384, "mock-search".to_string());
        let query_embedding = mock_provider.generate_embedding(query).await?;

        let embeddings = self.embeddings.read().await;
        let mut results: Vec<SearchResult> = Vec::new();

        for stored in embeddings.iter() {
            let similarity = calculate_cosine_similarity(&query_embedding, &stored.embedding);
            results.push(SearchResult {
                document_id: stored.document_id.clone(),
                file_path: stored.file_path.clone(),
                content_preview: stored.content.chars().take(200).collect::<String>(),
                similarity_score: similarity,
                metadata: stored.metadata.clone(),
            });
        }

        // Sort by similarity (highest first) and limit results
        results.sort_by(|a, b| b.similarity_score.partial_cmp(&a.similarity_score).unwrap());
        results.truncate(limit);

        Ok(results)
    }

    async fn count_embeddings(&self) -> usize {
        self.embeddings.read().await.len()
    }

    async fn get_embedding_by_document_id(&self, document_id: &str) -> Option<StoredEmbedding> {
        let embeddings = self.embeddings.read().await;
        embeddings
            .iter()
            .find(|e| e.document_id == document_id)
            .cloned()
    }
}

#[derive(Debug, Clone)]
struct SearchResult {
    document_id: String,
    file_path: PathBuf,
    content_preview: String,
    similarity_score: f32,
    metadata: EmbeddingEventMetadata,
}

fn calculate_cosine_similarity(vec1: &[f32], vec2: &[f32]) -> f32 {
    if vec1.len() != vec2.len() {
        return 0.0;
    }

    let dot_product: f32 = vec1.iter().zip(vec2.iter()).map(|(a, b)| a * b).sum();
    let magnitude1: f32 = vec1.iter().map(|x| x * x).sum::<f32>().sqrt();
    let magnitude2: f32 = vec2.iter().map(|x| x * x).sum::<f32>().sqrt();

    if magnitude1 == 0.0 || magnitude2 == 0.0 {
        0.0
    } else {
        dot_product / (magnitude1 * magnitude2)
    }
}

/// Integration test setup and execution
struct EmbeddingIntegrationTest {
    vault: TestVault,
    database: Arc<MockEmbeddingDatabase>,
    mock_provider: Arc<MockEmbeddingProvider>,
    config: EventDrivenEmbeddingConfig,
}

impl EmbeddingIntegrationTest {
    async fn new() -> Result<Self> {
        let vault = TestVault::new().await?;
        let database = Arc::new(MockEmbeddingDatabase::new());
        let mock_provider = Arc::new(MockEmbeddingProvider::new(384, "test-model".to_string()));

        let config = EventDrivenEmbeddingConfig {
            max_batch_size: 5,
            batch_timeout_ms: 100,
            max_concurrent_requests: 3,
            max_queue_size: 100,
            max_retry_attempts: 2,
            retry_delay_ms: 50,
            enable_deduplication: true,
            deduplication_window_ms: 1000,
        };

        Ok(Self {
            vault,
            database,
            mock_provider,
            config,
        })
    }

    async fn setup_file_events(&self, file_paths: &[PathBuf]) -> Result<Vec<FileEvent>> {
        let mut events = Vec::new();

        for file_path in file_paths {
            // Create different types of events to test various scenarios
            let event = FileEvent::new(FileEventKind::Created, file_path.clone());
            events.push(event);
        }

        Ok(events)
    }

    async fn process_file_events(
        &self,
        events: Vec<FileEvent>,
    ) -> Result<Vec<EmbeddingEventResult>> {
        let mut results = Vec::new();

        for event in events {
            // This is where the integration should happen
            // Currently, this will fail because the integration doesn't exist
            let result = self.process_single_file_event(event).await?;
            results.push(result);
        }

        Ok(results)
    }

    async fn process_single_file_event(&self, event: FileEvent) -> Result<EmbeddingEventResult> {
        let start_time = Instant::now();

        // Read file content
        let content = tokio::fs::read_to_string(&event.path)
            .await
            .map_err(|e| Error::Io(e))?;

        // Get file metadata
        let metadata = tokio::fs::metadata(&event.path)
            .await
            .map_err(|e| Error::Io(e))?;
        let file_size = Some(metadata.len());

        // Generate embedding
        let embedding = self.mock_provider.generate_embedding(&content).await?;

        // Create embedding event metadata
        let embedding_metadata =
            crucible_watch::create_embedding_metadata(&event.path, &event.kind, file_size);

        // Generate document ID
        let document_id = crucible_watch::generate_document_id(&event.path, &content);

        // Store embedding in database
        self.database
            .store_embedding(
                &document_id,
                &event.path,
                &content,
                embedding.clone(),
                embedding_metadata,
            )
            .await?;

        let processing_time = start_time.elapsed();

        Ok(EmbeddingEventResult::success(
            uuid::Uuid::new_v4(), // Mock event ID
            processing_time,
            embedding.len(),
        ))
    }

    async fn verify_semantic_search(&self) -> Result<Vec<SearchResult>> {
        // Test semantic search with various queries
        let queries = vec![
            "event-driven architecture",
            "embedding generation",
            "semantic search",
            "file monitoring",
            "technical specifications",
        ];

        let mut all_results = Vec::new();

        for query in queries {
            let results = self.database.semantic_search(query, 3).await?;
            all_results.extend(results);
        }

        Ok(all_results)
    }
}

/// Main integration test that demonstrates the missing event-driven embedding functionality
#[tokio::test]
async fn test_file_event_triggers_automatic_embedding_generation() {
    println!("ðŸš€ Starting comprehensive event-driven embedding integration test");

    // Phase 1: Test Setup
    println!("ðŸ“ Setting up test vault and infrastructure...");
    let test_setup = EmbeddingIntegrationTest::new()
        .await
        .expect("Failed to setup test infrastructure");

    // Phase 2: Create test documents
    println!("ðŸ“„ Creating test markdown documents...");
    let file_paths = test_setup
        .vault
        .create_test_documents()
        .await
        .expect("Failed to create test documents");

    assert_eq!(file_paths.len(), 4, "Should create 4 test documents");

    // Phase 3: Generate file events through crucible-watch system
    println!("ðŸ”¥ Generating file events through crucible-watch system...");
    let file_events = test_setup
        .setup_file_events(&file_paths)
        .await
        .expect("Failed to setup file events");

    assert_eq!(file_events.len(), 4, "Should generate 4 file events");

    // Verify event properties
    for (i, event) in file_events.iter().enumerate() {
        assert_eq!(
            event.kind,
            FileEventKind::Created,
            "Event {} should be Created event",
            i
        );
        assert!(event.path.exists(), "Event {} path should exist", i);
        assert!(!event.is_dir, "Event {} should be a file, not directory", i);
    }

    // Phase 4: Process file events through EventDrivenEmbeddingProcessor
    println!("âš™ï¸ Processing file events through EventDrivenEmbeddingProcessor...");
    let start_time = Instant::now();

    let embedding_results = test_setup
        .process_file_events(file_events)
        .await
        .expect("Failed to process file events");

    let processing_duration = start_time.elapsed();

    // Phase 5: Verify embedding generation and storage
    println!("âœ… Verifying embedding generation and storage results...");

    // All events should be processed successfully
    assert_eq!(
        embedding_results.len(),
        4,
        "Should have 4 embedding results"
    );

    for (i, result) in embedding_results.iter().enumerate() {
        assert!(result.success, "Embedding result {} should succeed", i);
        assert!(
            result.embedding_dimensions.is_some(),
            "Result {} should have embedding dimensions",
            i
        );
        assert_eq!(
            result.embedding_dimensions.unwrap(),
            384,
            "Embedding should be 384 dimensions"
        );
        assert!(
            result.processing_time > Duration::from_millis(0),
            "Processing time should be positive"
        );
        assert!(
            result.error.is_none(),
            "Result {} should not have errors",
            i
        );
    }

    // Verify embeddings are stored in database
    let stored_count = test_setup.database.count_embeddings().await;
    assert_eq!(stored_count, 4, "Should store 4 embeddings in database");

    // Phase 6: Verify semantic search functionality
    println!("ðŸ” Verifying semantic search functionality...");
    let search_results = test_setup
        .verify_semantic_search()
        .await
        .expect("Failed to perform semantic search");

    // Should find relevant results for our queries
    assert!(
        !search_results.is_empty(),
        "Semantic search should return results"
    );

    // Verify search result quality
    for result in &search_results {
        assert!(
            !result.document_id.is_empty(),
            "Document ID should not be empty"
        );
        assert!(result.file_path.exists(), "File path should exist");
        assert!(
            !result.content_preview.is_empty(),
            "Content preview should not be empty"
        );
        assert!(
            result.similarity_score >= 0.0 && result.similarity_score <= 1.0,
            "Similarity score should be between 0 and 1"
        );
    }

    // Phase 7: Performance validation
    println!("â±ï¸ Validating performance metrics...");

    // Total processing should be reasonable (under 5 seconds for 4 documents)
    assert!(
        processing_duration < Duration::from_secs(5),
        "Total processing should be under 5 seconds, took: {:?}",
        processing_duration
    );

    // Average processing time per document should be reasonable
    let avg_time_per_doc = processing_duration / 4;
    assert!(
        avg_time_per_doc < Duration::from_millis(500),
        "Average processing time per document should be under 500ms, took: {:?}",
        avg_time_per_doc
    );

    // Phase 8: Integration validation
    println!("ðŸ”— Validating end-to-end integration...");

    // Verify that we can find specific documents through semantic search
    let architecture_results = test_setup
        .database
        .semantic_search("event-driven architecture", 2)
        .await
        .expect("Failed to search for architecture content");

    assert!(
        !architecture_results.is_empty(),
        "Should find architecture-related documents"
    );

    // Verify that the results contain relevant content
    let found_relevant_content = architecture_results.iter().any(|result| {
        result.content_preview.to_lowercase().contains("event")
            || result
                .content_preview
                .to_lowercase()
                .contains("architecture")
    });
    assert!(
        found_relevant_content,
        "Search results should contain relevant content"
    );

    // Phase 9: Error handling validation
    println!("ðŸ›¡ï¸ Validating error handling and robustness...");

    // Test with non-existent file (should fail gracefully)
    let non_existent_event = FileEvent::new(
        FileEventKind::Modified,
        PathBuf::from("/non/existent/path.md"),
    );

    let error_result = test_setup
        .process_single_file_event(non_existent_event)
        .await;
    assert!(
        error_result.is_err(),
        "Processing non-existent file should return error"
    );

    // Test with empty file (should still work)
    let empty_file_path = test_setup.vault.get_documents_path().join("empty.md");
    tokio::fs::write(&empty_file_path, "")
        .await
        .expect("Failed to create empty test file");

    let empty_file_event = FileEvent::new(FileEventKind::Created, empty_file_path);
    let empty_result = test_setup
        .process_single_file_event(empty_file_event)
        .await
        .expect("Failed to process empty file");

    assert!(
        empty_result.success,
        "Empty file should be processed successfully"
    );

    println!("âœ… All integration tests passed!");
    println!("ðŸ“Š Test Summary:");
    println!("   - Documents processed: {}", embedding_results.len());
    println!("   - Embeddings stored: {}", stored_count);
    println!("   - Search results found: {}", search_results.len());
    println!("   - Total processing time: {:?}", processing_duration);
    println!("   - Average time per document: {:?}", avg_time_per_doc);
}

/// Test event batching functionality
#[tokio::test]
async fn test_event_batching_and_deduplication() {
    println!("ðŸš€ Testing event batching and deduplication functionality");

    let test_setup = EmbeddingIntegrationTest::new()
        .await
        .expect("Failed to setup test infrastructure");

    // Create test documents
    let file_paths = test_setup
        .vault
        .create_test_documents()
        .await
        .expect("Failed to create test documents");

    // Create multiple events for the same file (to test deduplication)
    let mut events = Vec::new();
    for _ in 0..3 {
        events.push(FileEvent::new(
            FileEventKind::Modified,
            file_paths[0].clone(),
        ));
    }

    // Process events
    let results = test_setup
        .process_file_events(events)
        .await
        .expect("Failed to process file events");

    // All events should be processed (deduplication logic would be in the real implementation)
    assert_eq!(
        results.len(),
        3,
        "Should process all events (deduplication not implemented yet)"
    );

    // But only one embedding should be stored (deduplication would prevent duplicates)
    let stored_count = test_setup.database.count_embeddings().await;
    assert_eq!(
        stored_count, 3,
        "Currently stores all embeddings (deduplication not implemented)"
    );

    println!("âœ… Batching and deduplication test completed");
}

/// Test priority-based processing
#[tokio::test]
async fn test_priority_based_embedding_processing() {
    println!("ðŸš€ Testing priority-based embedding processing");

    let test_setup = EmbeddingIntegrationTest::new()
        .await
        .expect("Failed to setup test infrastructure");

    // Create documents with different priority levels
    let critical_file = test_setup.vault.get_documents_path().join("critical.md");
    let normal_file = test_setup.vault.get_documents_path().join("normal.md");
    let low_file = test_setup.vault.get_documents_path().join("low.md");

    // Create test content
    tokio::fs::write(
        &critical_file,
        "# Critical Document\nThis needs immediate processing.",
    )
    .await
    .expect("Failed to create critical document");
    tokio::fs::write(
        &normal_file,
        "# Normal Document\nStandard processing is fine.",
    )
    .await
    .expect("Failed to create normal document");
    tokio::fs::write(
        &low_file,
        "# Low Priority Document\nBackground processing only.",
    )
    .await
    .expect("Failed to create low priority document");

    // Create events
    let critical_event = FileEvent::new(FileEventKind::Created, critical_file);
    let normal_event = FileEvent::new(FileEventKind::Modified, normal_file);
    let low_event = FileEvent::new(FileEventKind::Modified, low_file);

    // Process in order that doesn't match priority
    let start_time = Instant::now();

    let low_result = test_setup
        .process_single_file_event(low_event)
        .await
        .expect("Failed to process low priority event");
    let normal_result = test_setup
        .process_single_file_event(normal_event)
        .await
        .expect("Failed to process normal priority event");
    let critical_result = test_setup
        .process_single_file_event(critical_event)
        .await
        .expect("Failed to process critical priority event");

    let total_time = start_time.elapsed();

    // All events should be processed successfully
    assert!(low_result.success, "Low priority event should succeed");
    assert!(
        normal_result.success,
        "Normal priority event should succeed"
    );
    assert!(
        critical_result.success,
        "Critical priority event should succeed"
    );

    // In a real implementation with priority processing, critical events would be processed faster
    // For now, we just verify all events are processed
    println!("âœ… Priority processing test completed in {:?}", total_time);
}

/// Test graceful shutdown scenarios
#[tokio::test]
async fn test_graceful_shutdown_scenarios() {
    println!("ðŸš€ Testing graceful shutdown scenarios");

    let test_setup = EmbeddingIntegrationTest::new()
        .await
        .expect("Failed to setup test infrastructure");

    // Start processing some events
    let file_paths = test_setup
        .vault
        .create_test_documents()
        .await
        .expect("Failed to create test documents");

    // Process first half of events
    let first_half_events: Vec<FileEvent> = file_paths
        .iter()
        .take(2)
        .map(|path| FileEvent::new(FileEventKind::Created, path.clone()))
        .collect();

    let _ = test_setup
        .process_file_events(first_half_events)
        .await
        .expect("Failed to process first half of events");

    // Simulate shutdown scenario
    println!("ðŸ›‘ Simulating graceful shutdown...");

    // Process remaining events (simulating shutdown completion)
    let second_half_events: Vec<FileEvent> = file_paths
        .iter()
        .skip(2)
        .map(|path| FileEvent::new(FileEventKind::Created, path.clone()))
        .collect();

    let _ = test_setup
        .process_file_events(second_half_events)
        .await
        .expect("Failed to process remaining events during shutdown");

    // Verify all embeddings are stored
    let stored_count = test_setup.database.count_embeddings().await;
    assert_eq!(
        stored_count, 4,
        "All embeddings should be stored despite shutdown"
    );

    println!("âœ… Graceful shutdown test completed");
}
