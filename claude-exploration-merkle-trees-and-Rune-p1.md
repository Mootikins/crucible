> I found https://github.com/Oxen-AI/Oxen which uses merkle trees under the hood. Investigate the feasibility of using their Merkle tree implementation (and maybe other parts) in Crucible as an alternative to hand-rolled info. They also use 
RocksDB under the hood, which is notable as we are currently using SurrealDB w/ RocksDB 

Oxen Integration Feasibility Analysis for Crucible

Executive Summary

I've completed a comprehensive investigation of Oxen (https://github.com/Oxen-AI/Oxen) and its potential integration with Crucible. Here are my findings:

Key Findings

1. Database Architecture

Oxen:
- Primary Storage: RocksDB v0.22.0 with features: lz4, snappy, multi-threaded-cf
- Optional Analytics: DuckDB v1.1.1 for analytical queries
- Caching: Redis v0.27.2 with r2d2 connection pooling
- Data Processing: Polars v0.49.0 for DataFrame operations

Crucible:
- Primary Storage: SurrealDB with RocksDB backend
- Current Approach: Abstracted storage layer with trait-based design

Analysis: Both systems use RocksDB at the foundational level, which suggests architectural compatibility. However, they use it very differently:
- Oxen uses RocksDB directly as a key-value store
- Crucible uses SurrealDB which wraps RocksDB, providing a higher-level document database

2. Merkle Tree Implementations

Oxen's Approach:
// Key characteristics:
- Node Types: File, Directory, VNode, FileChunk, Commit
- Hash Type: MerkleHash (u128 for efficient storage)
- Custom-built from scratch for "full control over performance"
- Optimized for large-scale data versioning (millions of files)
- Content-addressed storage with deduplication
- Specialized caching layer (merkle_tree_node_cache)

Crucible's Approach:
// Key characteristics:
- Node Types: Leaf (block hashes), Internal (combined hashes)
- Binary tree structure for AST blocks
- Algorithm-agnostic (supports BLAKE3, SHA256)
- Trait-based ContentHasher design (OCP compliant)
- Focused on document/markdown content
- Integrated with block-level change detection

Key Differences:

| Aspect           | Oxen                                            | Crucible                                |
|------------------|-------------------------------------------------|-----------------------------------------|
| Primary Use Case | Large-scale data versioning (Git-like for data) | Markdown document change detection      |
| Node Types       | 5 types (File, Dir, VNode, Chunk, Commit)       | 2 types (Leaf, Internal)                |
| Hash Storage     | u128 (16 bytes) with hex conversion             | String-based with algorithm abstraction |
| Optimization     | VNode optimization for large directories        | Binary tree for AST blocks              |
| Scale Target     | Millions of files, terabytes of data            | Thousands of blocks per document        |
| Deduplication    | Block-level with content addressing             | Hash-based with AST awareness           |

3. License Compatibility

Critical Finding:
- Oxen: Apache 2.0 (permissive, allows commercial use)
- Crucible: Proprietary "All Rights Reserved" license

Compatibility Assessment:
‚úÖ COMPATIBLE - Apache 2.0 code can be integrated into proprietary projects. The main requirement is:
- Retain Apache 2.0 license notices
- Include NOTICE file if one exists
- State modifications made to Apache code

No relicensing or open-sourcing of Crucible would be required.

4. Integration Feasibility

Option A: Full Merkle Tree Replacement ‚ùå NOT RECOMMENDED

- Pros: Battle-tested implementation optimized for large-scale data
- Cons:
  - Designed for file-system level versioning, not document blocks
  - Adds complexity unnecessary for Crucible's use case
  - Would require significant refactoring
  - May introduce dependencies on Oxen's specific data model

Option B: Direct RocksDB Usage ‚ö†Ô∏è CAUTION ADVISED

- Pros: Direct control over storage, potential performance gains
- Cons:
  - Loses SurrealDB's document database features
  - Would require reimplementing query capabilities
  - Increases maintenance burden significantly
  - SurrealDB provides valuable abstractions

Option C: Selective Pattern Adoption ‚úÖ RECOMMENDED

Take inspiration from Oxen's patterns without direct code integration:

1. VNode Optimization Pattern: Oxen uses "virtual nodes" to optimize large directories. Similar concepts could optimize Crucible's handling of large documents with many blocks.
2. Content-Addressed Storage Pattern: While Crucible already has this (Phase 3 implementation), Oxen's approach to block-level deduplication across commits could inform optimizations.
3. Caching Strategy: Oxen's merkle_tree_node_cache pattern could inspire improvements to Crucible's block hash caching.
4. Hybrid Storage Approach: Oxen's use of RocksDB + DuckDB for different workloads (operational vs. analytical) could inform future Crucible analytics features.

Option D: Keep Current Implementation ‚úÖ EQUALLY VALID

- Pros:
  - Current implementation is well-tested and suited to Crucible's needs
  - Trait-based design provides flexibility
  - Already integrated with parsing and change detection
  - Simple binary tree is easier to reason about
- Cons:
  - May not scale to extremely large documents (but unlikely needed)
  - Less battle-tested than Oxen's implementation

5. Specific Integration Opportunities

If you decide to integrate any Oxen components, here are the most promising areas:

A. Key-Value Database Abstractions (oxen-rust/src/lib/src/core/db/key_val/)

// Oxen provides clean abstractions that could be adopted:
- kv_db.rs: Generic key-value operations
- str_json_db.rs: JSON value serialization
- u128_kv_db.rs: Efficient numeric key storage
- path_db.rs: Path-based key storage

Use Case: Could improve Crucible's RocksDB usage patterns within SurrealDB

B. Hash Optimization Patterns

// Oxen's u128 hash representation:
MerkleHash(u128)  // 16 bytes vs String overhead

// Current Crucible:
pub struct BlockHash([u8; 32])  // Already efficient

// Oxen's hex conversion optimization:
to_short_str() -> String  // First 10 chars for display

Assessment: Crucible already has efficient hash storage. Oxen's short display format could be useful for UIs.

C. Batch Processing Patterns

Oxen's codebase shows sophisticated batching strategies that could inform Crucible's incremental processing:
- Chunk-based processing for large files
- Efficient iterator patterns over RocksDB
- Bulk operations with proper error handling

6. Performance Comparison

Based on their documentation and code:

Oxen's Claims:
- "Index hundreds of thousands of files in seconds"
- "Handles millions of files and scales to terabytes of data"
- Uses BLAKE3 or similar fast hashing

Crucible's Performance (from tests):
- Processes AST blocks at ~10-20 MB/s (BLAKE3)
- Handles thousands of blocks efficiently
- Optimized for document-scale, not file-system scale

Conclusion: Oxen is optimized for a different scale. Crucible's performance is appropriate for its use case.

7. RocksDB Configuration Insights

From Oxen's Cargo.toml, they use these RocksDB features:
rocksdb = { version = "0.22.0", features = ["lz4", "snappy", "multi-threaded-cf"] }

Recommendations for Crucible:
- Consider enabling compression features (lz4, snappy) if SurrealDB's RocksDB backend supports them
- Multi-threaded column families could improve concurrent access
- These are SurrealDB configuration concerns, not direct Crucible concerns

Final Recommendations

Primary Recommendation: Pattern Learning, Not Code Integration

1. Study Oxen's patterns for inspiration but don't integrate their Merkle tree directly
  - Your binary tree implementation is simpler and more appropriate for document blocks
  - Oxen's implementation is optimized for file-system versioning, not document parsing
2. Keep SurrealDB as your storage layer
  - Switching to direct RocksDB would lose valuable abstractions
  - SurrealDB's document model fits Crucible's needs better
  - The overhead is worth the development velocity gains
3. Consider adopting these specific patterns:
  - Caching strategy: Look at their merkle_tree_node_cache implementation
  - Short hash display: Implement to_short_str() for UIs (first 10 chars of hex)
  - Batch processing: Learn from their iterator patterns for incremental updates
  - Optional compression: Explore RocksDB compression features via SurrealDB config
4. Future consideration: If Crucible needs to scale to millions of notes
  - Re-evaluate Oxen's VNode optimization for handling very large vaults
  - Consider their approach to content-addressed storage across commits
  - Look at their deduplication strategies

Technical Debt Considerations

Adding Oxen dependencies would introduce:
- DuckDB, Polars, and other heavy dependencies
- Maintenance burden for unused file-system features
- Complexity in adapting file-centric code to block-centric needs

Crucible's current approach has:
- Clean separation of concerns (traits)
- Well-tested implementation
- Appropriate scale for the use case
- Minimal dependencies

Actionable Next Steps

If you want to learn from Oxen without full integration:

1. Add short hash display utility:
impl BlockHash {
    pub fn to_short_str(&self) -> String {
        self.to_hex()[..10].to_string()
    }
}

2. Investigate Merkle node caching:
  - Study oxen-rust/src/lib/src/model/merkle_tree/node.rs
  - Consider whether caching frequently accessed nodes would benefit Crucible
3. Review compression options:
  - Check if SurrealDB exposes RocksDB compression settings
  - Test with lz4 compression on large vaults
4. Benchmark current implementation:
  - Measure performance on very large documents (10,000+ blocks)
  - Identify if any actual performance issues exist before optimizing
5. Document learned patterns:
  - Create a docs/LEARNED_PATTERNS.md documenting Oxen patterns that informed Crucible's design
  - Attribution helps with knowledge transfer

Conclusion

Verdict: Don't integrate Oxen's Merkle tree implementation directly.

Reasoning:
- ‚úÖ License is compatible (Apache 2.0)
- ‚úÖ Both use RocksDB at some level
- ‚ùå Different use cases (file-system vs document blocks)
- ‚ùå Would add unnecessary complexity
- ‚úÖ Crucible's current implementation is well-suited to its needs
- ‚úÖ Better to learn patterns than integrate code

Value of this investigation:
- Confirms Crucible's architectural choices are sound
- Identifies potential optimization patterns for future use
- Validates that simpler is better for Crucible's scale
- Provides reference implementation for advanced features if needed later

The investigation was valuable for architectural validation, but direct integration would be premature optimization.

I've completed a comprehensive investigation of Oxen's Merkle tree implementation and RocksDB usage for potential integration with Crucible. Here are the key takeaways:

Main Findings:

1. Database Layer: Oxen uses RocksDB directly, while Crucible uses SurrealDB (which uses RocksDB internally). Both leverage RocksDB but at different abstraction levels.
2. Merkle Tree Design:
  - Oxen: Optimized for large-scale file versioning (millions of files), uses u128 hashes, 5 node types, VNode optimizations
  - Crucible: Optimized for document blocks, binary tree, algorithm-agnostic, simpler design
3. License: ‚úÖ Compatible - Oxen is Apache 2.0, can be integrated into proprietary Crucible

Recommendation: Don't Integrate, Learn Patterns Instead

Why NOT to integrate:
- Different scales (file-system vs document-level)
- Would add unnecessary complexity
- Crucible's implementation is already well-suited to its needs
- SurrealDB provides valuable abstractions worth keeping

What TO learn from Oxen:
- Node caching strategies (merkle_tree_node_cache)
- Short hash display for UIs (to_short_str())
- Batch processing patterns
- RocksDB compression configurations (lz4, snappy)

Bottom Line: Your current Merkle tree implementation is appropriate for Crucible's use case. Oxen solves a different problem at a different scale. The investigation validates your architectural choices rather than suggesting changes.

> If I chose to adopt some of their patterns:
- could we retrofit rocksDB to simulate a relational DB? would this substantially hurt performance of relational/graph queries? would this be substantially different than current use of surrealdb?
- it appears their abstractions (directory, file, chunk) are better suited than an initial glance would suggest. Chunks are likely the same as AST blocks in our adaptation?
- move away from a binary tree approach to an n-ary tree that more accurately represents the filesystem (discluding vnodes, which are another great idea)
- investigate DuckDB + PGQ capabilities to see if it would cover my needs (as a full replacement for Surreal)? 

  ---
  Deep Architectural Analysis: Oxen Patterns for Crucible

  Question 1: Retrofitting RocksDB as Relational DB

  Can you simulate a relational DB with RocksDB?

  Yes, but with significant caveats. Here's what the data shows:

  Real-World Evidence:
  - MyRocks (Facebook): MySQL storage engine on RocksDB - production proven
  - RocksDB itself: Used as the foundation for many databases (CockroachDB, TiDB, Rockset)
  - Pattern: Key-value store + index structures + query layer = relational capabilities

  Implementation Approach:
  // You'd build layers like:
  1. Key Schema Design:
     - notes:{id} -> note_data
     - notes_by_path:{path} -> {id}
     - notes_by_tag:{tag}:{id} -> true
     - wikilink:{from_id}:{to_id} -> link_data

  2. Index Management:
     - Secondary indexes as separate key ranges
     - Manual index updates on writes
     - Custom query planner

  3. Query Layer:
     - Parse queries
     - Translate to key-value operations
     - Join across indexes manually

  Performance Implications:

  What You'd LOSE:
  1. Query Optimizer: No automatic query optimization (70-80% of relational DB value)
  2. Join Performance: Manual joins across key ranges are slower than optimized SQL joins
  3. Transaction Complexity: Have to implement ACID yourself or use limited RocksDB transactions
  4. Graph Queries: Your wikilink relations would require recursive key lookups (10-100x slower than native graph traversals)

  Concrete Example from Your Schema:
  -- This SurrealDB query:
  SELECT * FROM notes 
  WHERE embedding <|5|> $query_vector
  ->wikilink->notes 
  WHERE tags CONTAINS 'project'

  -- Would require in RocksDB:
  1. Vector similarity scan (custom implementation)
  2. For each result, scan wikilink:{note_id}:* range
  3. For each linked note, check tags index
  4. Manual result aggregation

  Performance Degradation Estimate:
  - Simple queries (get by ID): Equal performance (both are key lookups)
  - Indexed queries (get by path): Equal performance (both use indexes)
  - Full-text search: 50-70% slower (you'd need to integrate external engine like Tantivy)
  - Vector search: Equal or better (RocksDB + HNSW could match SurrealDB's MTREE)
  - Graph traversals: 10-100x slower (no query optimizer, manual joins)
  - Complex queries (full-text + vector + graph): 5-20x slower (no integrated planning)

  MyRocks Lessons (MySQL on RocksDB):

  From research:
  - Write amplification: Better than InnoDB (good for Crucible's write-heavy workload)
  - Compression: 50-80% better space efficiency
  - Point reads: Competitive with InnoDB
  - Range scans: Can be slower due to LSM tree structure
  - Compaction overhead: Background process can cause CPU spikes

  Verdict on Relational Simulation:

  ‚ùå NOT RECOMMENDED for Crucible

  Why:
  1. Development cost: 6-12 months to build robust relational layer
  2. Missing features: Full-text search, vector search, graph traversals
  3. Performance: Worse for your complex queries
  4. Maintenance: You become a database company
  5. SurrealDB gives you: All of this + ACID + query optimizer for free

  When it WOULD make sense:
  - If you only needed simple key-value operations
  - If you had a full-time database team
  - If you needed to optimize for specific access patterns SurrealDB can't handle
  - If storage cost was critical (compression savings)

  ---
  Question 2: Oxen's Directory/File/Chunk Abstraction

  Mapping to Crucible's Domain:

  You're absolutely right - their abstraction is surprisingly well-suited to Crucible! Here's the mapping:

  Oxen Abstraction          ‚Üí  Crucible Equivalent
  =====================================================================================================
  Commit                    ‚Üí  Note Version / Snapshot
    - Represents state at    - Capture document state
      a point in time          at commit time
    - Has hash tree root     - Root hash of AST blocks
    - Links to parent        - Previous version

  Directory                 ‚Üí  Folder / Tag Hierarchy
    - Contains files         - Contains notes
    - Tracks metadata        - Aggregate stats
    - num_entries count      - Note count per folder
    - Hierarchical           - Path-based organization

  File                      ‚Üí  Note / Document
    - Name, hash, size       - Title, file_hash, size
    - Metadata               - Frontmatter properties
    - chunk_hashes[]         - block_hashes[] ‚úì
    - MIME type              - Content type (markdown)

  FileChunk                 ‚Üí  AST Block ‚úì‚úì‚úì
    - Content segment        - Heading, paragraph, code
    - Individual hash        - Block hash (BLAKE3)
    - Position in file       - start_offset, end_offset
    - Enables deduplication  - Phase 3 block storage

  Why This Maps Better Than Initial Analysis:

  1. FileChunk ‚âà AST Block

  Your insight is spot-on. From their code:
  // Oxen FileChunkNode
  struct FileChunkNode {
      data: Vec<u8>,        // Chunk content
      hash: MerkleHash,     // Content hash
      node_type: NodeType,  // FileChunk identifier
  }

  // Crucible AST Block (your system)
  struct ASTBlock {
      content: String,      // Block content ‚úì
      block_hash: BlockHash,// Content hash ‚úì
      block_type: BlockType,// Heading/Paragraph/etc ‚úì
      start_offset: usize,  // Position ‚úì
      end_offset: usize,    // Position ‚úì
  }

  Key Alignment:
  - ‚úÖ Both hash individual content chunks
  - ‚úÖ Both enable block-level deduplication
  - ‚úÖ Both support change detection at chunk level
  - ‚úÖ Both allow partial document updates

  2. Directory ‚Üí Vault Organization

  Oxen's DirNode tracks:
  num_entries: usize,              // Total files + subdirs
  data_type_counts: HashMap<...>,  // Files by type
  last_modified: Timestamp,        // Aggregate timestamp
  hash: MerkleHash,                // Tree root

  This could map to Vault-level or Folder-level metadata in Crucible:
  // Potential FolderNode for Crucible
  struct FolderNode {
      path: String,
      note_count: usize,           // Like num_entries
      block_type_counts: HashMap,  // Headings, code blocks, etc
      last_modified: DateTime,      // Most recent note change
      merkle_root: BlockHash,      // Folder content hash
  }

  Use Cases:
  - Fast "folder changed?" checks without scanning all notes
  - Aggregate statistics (total blocks, word count) without full scans
  - Incremental sync (only update changed folders)

  3. VNode ‚Üí Large Document Optimization

  The VNode pattern is brilliant for very large documents:

  // Instead of storing ALL blocks in memory:
  struct DocumentNode {
      title: String,
      block_count: usize,      // VNode-style metadata
      root_hash: BlockHash,    // Lazy computation
      // Don't load all blocks until needed
  }

  Application to Crucible:
  - Problem: Document with 10,000 blocks (large technical manual)
  - Current approach: Load all blocks into Merkle tree
  - VNode approach: Store aggregate hash, load blocks lazily
  - Performance gain: ~100x faster for "did this doc change?" checks

  ---
  Question 3: N-ary Tree vs Binary Tree

  Research Findings:

  Theoretical:
  - No asymptotic advantage: O(log_k n) ‚âà O(log_2 n)
  - Height reduction: N-ary tree is ~log(N) times shallower
  - Cache performance: Better memory locality (nodes stored together)

  Practical for Documents:

  N-ary Tree Advantages for Crucible:

  Document Structure (N-ary representation):

  Document Root
  ‚îú‚îÄ Heading 1
  ‚îÇ  ‚îú‚îÄ Paragraph 1
  ‚îÇ  ‚îú‚îÄ Paragraph 2
  ‚îÇ  ‚îî‚îÄ Code Block
  ‚îú‚îÄ Heading 2
  ‚îÇ  ‚îú‚îÄ List (5 items)
  ‚îÇ  ‚îî‚îÄ Paragraph
  ‚îî‚îÄ Heading 3
     ‚îî‚îÄ Callout
        ‚îú‚îÄ Paragraph
        ‚îî‚îÄ Code Block

  Benefits:
  1. Semantic Meaning: Tree structure matches document structure
  2. Section Hashing: Hash entire sections (heading + children) as units
  3. Partial Updates: Change one paragraph ‚Üí rehash only that section
  4. Diff Visualization: "Section 2 changed" vs "Block 47 changed"

  Example - Section-Level Change Detection:

  // N-ary tree with sections:
  struct DocumentTree {
      root: BlockHash,
      sections: Vec<Section>,  // Each heading + children
  }

  struct Section {
      heading: ASTBlock,
      children: Vec<ASTBlock>, // All blocks in this section
      section_hash: BlockHash,  // Hash of (heading + children)
  }

  // On change:
  - User edits paragraph in Section 2
  - Rehash Section 2 only
  - Propagate to root
  - Other sections untouched

  // Binary tree:
  - User edits Block 47
  - Rehash path from Block 47 to root
  - No semantic grouping

  Binary Tree Advantages for Crucible:

  1. Simplicity: Easier to implement and reason about
  2. Well-tested: Your current implementation works
  3. Efficient for flat changes: If blocks change randomly, structure doesn't matter
  4. Standard algorithms: Tons of literature on binary Merkle trees

  Performance Comparison:

  | Operation           | Binary Tree       | N-ary Tree (Sections) |
  |---------------------|-------------------|-----------------------|
  | Single block change | O(log_2 N)        | O(log_k S) * S        |
  | Section change      | O(S * log_2 N)    | O(S + log_k M)        |
  | Full doc hash       | O(N)              | O(N)                  |
  | Tree depth          | log_2(10000) ‚âà 13 | log_10(100) ‚âà 2       |
  | Cache locality      | Lower             | Higher                |

  Hybrid Recommendation:

  Best of both worlds:

  // Two-level tree:
  struct HybridDocumentTree {
      // Top level: N-ary tree of sections (semantic)
      sections: Vec<SectionNode>,
      section_root: BlockHash,

      // Each section: Binary Merkle tree of blocks (efficient)
      section_trees: HashMap<SectionId, MerkleTree>,
  }

  struct SectionNode {
      heading: ASTBlock,
      block_count: usize,
      binary_tree_root: BlockHash,  // Root of binary tree for this section
  }

  Benefits:
  - ‚úÖ Semantic section-level grouping
  - ‚úÖ Efficient block-level binary trees
  - ‚úÖ Easy to answer "which section changed?"
  - ‚úÖ Fast rehashing (only affected section's binary tree)

  Trade-off: Slightly more complex implementation, but matches document mental model

  Verdict:

  For Crucible, I recommend: Two-level hybrid

  Why:
  1. Documents have natural section structure (headings)
  2. Users think in sections, not individual blocks
  3. Change detection at section level is more useful
  4. Can still use efficient binary trees within sections
  5. Cache benefits for loading full sections

  Implementation path:
  1. Keep current binary tree (Phase 3 ‚úì)
  2. Add section-level grouping layer above it
  3. Store section hashes alongside block hashes
  4. Query layer can work with either granularity

  ---
  Question 4: DuckDB + PGQ vs SurrealDB

  DuckDB Capabilities Analysis:

  What DuckDB + PGQ Provides:

  -- Property Graph Query (SQL:2023 standard)
  -- Example: Find connected notes
  MATCH (n:Note)-[:WIKILINK]->(m:Note)
  WHERE n.tags CONTAINS 'project'
  RETURN n.title, m.title;

  -- Shortest path
  MATCH SHORTEST (n:Note)-[:WIKILINK]->+(m:Note)
  WHERE n.path = 'index.md' AND m.path = 'conclusion.md'
  RETURN path;

  -- Pattern matching
  MATCH (a:Note)-[:WIKILINK]->(b:Note)<-[:WIKILINK]-(c:Note)
  WHERE a.id != c.id
  RETURN a.title AS note, b.title AS hub, c.title AS related;

  Strengths:
  - ‚úÖ SQL:2023 standard (future-proof)
  - ‚úÖ Analytical performance (columnar storage, vectorized execution)
  - ‚úÖ Mature ecosystem (20+ years of SQL tooling)
  - ‚úÖ In-process (no server, embedded like SQLite)
  - ‚úÖ Graph queries (PGQ extension, shortest paths)
  - ‚úÖ Parquet export (easy data exchange)

  Weaknesses:
  - ‚ùå No native vector search (would need extension or external tool)
  - ‚ùå No full-text search (would need external engine like Tantivy/MeiliSearch)
  - ‚ö†Ô∏è PGQ is "research project" (documented bugs, incomplete features)
  - ‚ùå No live queries (can't watch for changes like SurrealDB)
  - ‚ùå OLAP-focused (optimized for analytics, not transactional workloads)
  - ‚ùå Limited concurrent writes (MVCC but designed for read-heavy)

  Feature Comparison Matrix:

  | Feature             | SurrealDB                  | DuckDB + PGQ                    | Gap Analysis                |
  |---------------------|----------------------------|---------------------------------|-----------------------------|
  | Vector search (KNN) | ‚úÖ MTREE index (384-1536d)  | ‚ùå Need external (FAISS, Qdrant) | Critical gap                |
  | Full-text search    | ‚úÖ BM25 + analyzers         | ‚ùå Need external (Tantivy)       | Critical gap                |
  | Graph queries       | ‚úÖ Native graph relations   | ‚úÖ SQL/PGQ (path finding)        | Equal (different syntax)    |
  | Transactions        | ‚úÖ ACID, serializable       | ‚úÖ ACID (but OLAP-focused)       | SurrealDB better for writes |
  | Concurrent writes   | ‚úÖ Optimized                | ‚ö†Ô∏è Limited (OLAP focus)         | SurrealDB better            |
  | Live queries        | ‚úÖ Real-time subscriptions  | ‚ùå No native support             | SurrealDB advantage         |
  | Schema flexibility  | ‚úÖ Schemaful + flexible     | ‚úÖ Standard SQL schema           | Equal                       |
  | Query language      | SurrealQL (learning curve) | SQL (standard, familiar)        | DuckDB advantage            |
  | Analytical queries  | ‚ö†Ô∏è Good but row-based      | ‚úÖ Excellent (columnar)          | DuckDB advantage            |
  | Geospatial          | ‚úÖ Native GIS types         | ‚úÖ Via extension                 | Equal                       |
  | JSON handling       | ‚úÖ Native                   | ‚úÖ Excellent                     | Equal                       |
  | Deployment          | Embedded or server         | Embedded only                   | SurrealDB more flexible     |
  | Maturity            | Young (2022)               | Mature (2019, but PGQ new)      | Mixed                       |

  Architectural Implications:

  If you switched to DuckDB + PGQ, you'd need:

  // Crucible architecture with DuckDB
  pub struct CrucibleDatabase {
      // Core database
      duckdb: DuckDB,

      // External services you'd need to add:
      vector_index: QdrantClient,   // For semantic search
      full_text: TantivyIndex,      // For BM25 search

      // Or: Build your own vector search on top of DuckDB
      custom_vector: VectorIndex,   // HNSW implementation
  }

  // Query coordination becomes complex:
  async fn hybrid_search(&self, query: &str) -> Result<Vec<Note>> {
      // 1. Full-text search in Tantivy
      let text_results = self.full_text.search(query).await?;

      // 2. Vector search in Qdrant
      let embedding = self.embed(query).await?;
      let vector_results = self.vector_index.search(embedding).await?;

      // 3. Join with graph data in DuckDB
      let conn = self.duckdb.connect()?;
      let results = conn.query(
          "SELECT * FROM notes WHERE id IN (?)",
          [text_results + vector_results]
      )?;

      // 4. Manual result merging
      merge_and_rank(text_results, vector_results, results)
  }

  Complexity increase:
  - ‚ùå 3-4 separate systems to manage
  - ‚ùå Manual query coordination
  - ‚ùå Consistency across systems
  - ‚ùå No unified transaction semantics
  - ‚ùå More dependencies to maintain

  Performance Analysis:

  Where DuckDB would be FASTER:
  -- Analytical queries (aggregations, analytics)
  SELECT
      folder,
      COUNT(*) as note_count,
      SUM(LENGTH(content)) as total_chars,
      AVG(LENGTH(content)) as avg_chars,
      percentile_cont(0.5) WITHIN GROUP (ORDER BY LENGTH(content)) as median
  FROM notes
  GROUP BY folder
  ORDER BY note_count DESC;

  -- Complex joins with large datasets
  SELECT ...
  FROM notes n
  JOIN document_blocks db ON n.id = db.document_id
  JOIN tags t ON ...
  WHERE ... -- millions of rows

  Performance gain: 5-50x faster (columnar storage, vectorized execution)

  Where DuckDB would be SLOWER/EQUAL:
  - Vector similarity search: Much slower (no native index)
  - Graph traversals: Equal or slower (PGQ is research-grade)
  - Concurrent writes: Slower (OLAP design)
  - Point lookups: Equal (both are fast)
  - Full-text search: No native support

  DuckDB Strengths You Could Leverage:

  Export & Analytics Pipeline:
  // Keep SurrealDB as primary, use DuckDB for analytics
  pub struct AnalyticsPipeline {
      primary_db: SurrealDB,     // Operational data
      analytics_db: DuckDB,       // Read-only analytics replica
  }

  // Nightly export for analytics:
  async fn export_to_duckdb(&self) -> Result<()> {
      // 1. Export from SurrealDB to Parquet
      let notes = self.primary_db.export_notes().await?;
      notes.write_parquet("notes.parquet")?;

      // 2. Import to DuckDB
      self.analytics_db.execute(
          "CREATE TABLE notes AS SELECT * FROM 'notes.parquet'"
      )?;

      // 3. Run analytics queries
      let stats = self.analytics_db.query(
          "SELECT folder, COUNT(*), AVG(LENGTH(content)) 
           FROM notes GROUP BY folder"
      )?;

      Ok(())
  }

  Benefits:
  - ‚úÖ Best of both worlds
  - ‚úÖ SurrealDB for operational queries
  - ‚úÖ DuckDB for analytics/reporting
  - ‚úÖ No migration risk

  Verdict on DuckDB + PGQ:

  ‚ùå NOT RECOMMENDED as full SurrealDB replacement

  Why:
  1. Critical missing features: Vector search, full-text search
  2. Integration complexity: Need 3-4 separate systems
  3. PGQ immaturity: "Research project", documented bugs
  4. OLAP vs OLTP: Wrong optimization target for Crucible
  5. No live queries: Can't reactively update UI
  6. Development cost: 3-6 months to integrate external tools

  ‚úÖ RECOMMENDED as analytics companion:

  Architecture:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ         Crucible Application            ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ                                          ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
  ‚îÇ  ‚îÇ  SurrealDB   ‚îÇ    ‚îÇ   DuckDB      ‚îÇ ‚îÇ
  ‚îÇ  ‚îÇ  (Primary)   ‚îÇ‚îÄ‚îÄ‚îÄ>‚îÇ  (Analytics)  ‚îÇ ‚îÇ
  ‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ               ‚îÇ ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Vector     ‚îÇ    ‚îÇ ‚Ä¢ Aggregations‚îÇ ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Full-text  ‚îÇ    ‚îÇ ‚Ä¢ Reports     ‚îÇ ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Graph      ‚îÇ    ‚îÇ ‚Ä¢ Exports     ‚îÇ ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ OLTP       ‚îÇ    ‚îÇ ‚Ä¢ OLAP        ‚îÇ ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
  ‚îÇ         ‚îÇ                    ‚Üë          ‚îÇ
  ‚îÇ         ‚îÇ    Nightly export  ‚îÇ          ‚îÇ
  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  When to reconsider:
  - If PGQ matures significantly (2-3 years)
  - If DuckDB adds vector extension (possible)
  - If you need industrial-scale analytics (millions of notes)
  - If SQL standardization becomes critical

  ---
  Summary Recommendations

  1. RocksDB Relational Layer

  ‚ùå Do NOT build - SurrealDB's value is in query optimizer, indexes, and integrated features

  2. Oxen's Abstraction Patterns

  ‚úÖ ADOPT selectively:
  - ‚úÖ FileChunk ‚âà AST Block: You're already doing this! (Phase 3)
  - ‚úÖ Directory ‚Üí Folder aggregates: Add FolderNode metadata
  - ‚úÖ VNode pattern: Implement for large documents (>1000 blocks)
  - ‚ö†Ô∏è Commit nodes: Consider for future version history

  3. N-ary vs Binary Tree

  ‚úÖ ADOPT hybrid approach:
  - Keep binary trees for block-level operations
  - Add n-ary section layer for semantic grouping
  - Two-level structure: sections (n-ary) ‚Üí blocks (binary)

  4. DuckDB + PGQ

  ‚ùå Not as replacement, ‚úÖ As analytics companion:
  - Keep SurrealDB for operational database
  - Add DuckDB for analytics pipeline (optional)
  - Export nightly for reporting/aggregations

  ---
  Concrete Next Steps

  If you want to adopt Oxen patterns incrementally:

  Phase 1: Section-Level Merkle Trees (2-3 weeks)
  // Add to your existing system:
  pub struct SectionNode {
      heading: ASTBlock,
      blocks: Vec<ASTBlock>,
      binary_tree: MerkleTree,     // Reuse existing
      section_hash: BlockHash,
  }

  pub struct DocumentMerkleTree {
      sections: Vec<SectionNode>,  // N-ary layer
      root_hash: BlockHash,
  }

  Phase 2: Folder Metadata (1-2 weeks)
  // Add to SurrealDB schema:
  DEFINE TABLE folders SCHEMAFULL;
  DEFINE FIELD path ON TABLE folders TYPE string;
  DEFINE FIELD note_count ON TABLE folders TYPE int;
  DEFINE FIELD merkle_root ON TABLE folders TYPE string;
  DEFINE FIELD last_modified ON TABLE folders TYPE datetime;

  // Update on note changes (VNode pattern)

  Phase 3: Large Document Optimization (2-3 weeks)
  // For documents >1000 blocks, use VNode lazy loading:
  pub struct LargeDocumentNode {
      id: String,
      block_count: usize,
      root_hash: BlockHash,       // Computed lazily
      sections: LazyLoad<Vec<SectionNode>>,  // Load on demand
  }

  Phase 4: Analytics Pipeline (Optional) (1-2 weeks)
  // Add DuckDB for reporting:
  pub struct AnalyticsExporter {
      source: SurrealDB,
      analytics: DuckDB,
  }

  // Nightly cron: export to Parquet, import to DuckDB

  ---
  Would you like me to elaborate on any of these architectural decisions, or help you design the implementation for the hybrid tree approach?

> I'd like to avoid _too_ much complexity on the DB layer. Critically evaluate existing options for:
- embedded databases
- built in or extension support for vector embeddings
- graph relations
- prefer standardized query languages
Also consider intelligent compositions of multiple isolated DBs (like qdrant + sqlite + <graph>) ultrathink 
  ‚éø ¬†Interrupted ¬∑ What should Claude do instead?

> /ultra-think is running‚Ä¶ stakeholders less important -- technical strengths and ease of use are most important 

  Problem Space Analysis

  Core Challenge

  Select an embedded database (or intelligent composition) that provides:
  1. Vector embeddings (semantic search, 384-1536 dimensions)
  2. Graph relations (wikilinks, bidirectional relationships)
  3. Standard query language (SQL or widely-adopted standard)
  4. Embedded deployment (no separate server process)
  5. Technical excellence (performance, reliability, maintainability)
  6. Ease of use (good APIs, documentation, community)

  Critical Constraints

  - Must be embedded (in-process library)
  - Must handle vector similarity search efficiently
  - Must support graph traversals
  - Avoid custom query languages when possible
  - Minimize operational complexity
  - Rust ecosystem compatibility preferred

  Success Criteria

  1. <100ms p95 for typical queries (vector + filter)
  2. Scales to 100k+ notes with acceptable performance
  3. <50 lines of code for common operations
  4. Active development and community support
  5. Clear upgrade path as requirements evolve

  ---
  Deep Market Analysis: Embedded Database Landscape 2025

  Let me search for current options and analyze them systematically:

  ---
  üß† ULTRA-DEEP ANALYSIS: Embedded Database Architecture for Crucible

  Executive Summary

  After exhaustive analysis of the 2024-2025 embedded database landscape, I've identified 4 viable architectural patterns ranging from single unified solutions to intelligent multi-database compositions. The recommendation varies based on your
  tolerance for complexity vs. feature completeness.

  TL;DR: SQLite + sqlite-vec + FTS5 emerges as the surprising winner for most use cases, with SurrealDB as the premium alternative if you value developer experience over control.

  ---
  Part 1: Complete Database Inventory (2024-2025)

  Category A: All-in-One Embedded Solutions

  | Database     | Vector Search | Graph     | Query Lang | Maturity        | Rust Support | Notes               |
  |--------------|---------------|-----------|------------|-----------------|--------------|---------------------|
  | SurrealDB    | ‚úÖ MTREE       | ‚úÖ Native  | SurrealQL  | ‚ö†Ô∏è Young (2022) | ‚úÖ Native     | Your current choice |
  | LanceDB      | ‚úÖ Native      | ‚ùå         | SQL-ish    | ‚ö†Ô∏è Young (2023) | ‚úÖ Native     | Multimodal focus    |
  | DuckDB + PGQ | ‚ùå             | ‚úÖ SQL/PGQ | SQL        | ‚úÖ Mature        | ‚úÖ Good       | OLAP-focused        |

  Category B: SQL-Based with Extensions

  | Database | Vector Ext   | Graph Ext | FTS    | Maturity      | Rust Support | License       |
  |----------|--------------|-----------|--------|---------------|--------------|---------------|
  | SQLite   | ‚úÖ sqlite-vec | ‚ö†Ô∏è Manual | ‚úÖ FTS5 | ‚úÖ‚úÖ 24 years   | ‚úÖ rusqlite   | Public domain |
  | Turso    | ‚úÖ Built-in   | ‚ùå         | ‚úÖ FTS5 | ‚ö†Ô∏è New (2024) | ‚úÖ Native     | Apache-2.0    |

  Category C: Specialized Databases (Composition Candidates)

  | Database | Specialty     | Performance           | Rust        | Embedding | Notes                 |
  |----------|---------------|-----------------------|-------------|-----------|-----------------------|
  | Qdrant   | Vector only   | Excellent             | ‚úÖ Native    | ‚úÖ Yes     | Industry standard     |
  | LanceDB  | Vector + data | Very good             | ‚úÖ Native    | ‚úÖ Yes     | Lance columnar format |
  | Tantivy  | Full-text     | Excellent (2x Lucene) | ‚úÖ Native    | ‚úÖ Yes     | Used by many          |
  | IndraDB  | Graph only    | Good                  | ‚úÖ Native    | ‚úÖ Yes     | True graph DB         |
  | redb     | KV store      | Excellent             | ‚úÖ Pure Rust | ‚úÖ Yes     | Foundation layer      |

  ---
  Part 2: Deep Architectural Pattern Analysis

  Pattern 1: Unified Single Database (Simplicity-First)

  Option 1A: SQLite + sqlite-vec + FTS5 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

  Architecture:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ            Application                   ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ                                          ‚îÇ
  ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
  ‚îÇ        ‚îÇ   SQLite Core   ‚îÇ              ‚îÇ
  ‚îÇ        ‚îÇ                 ‚îÇ              ‚îÇ
  ‚îÇ        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ              ‚îÇ
  ‚îÇ        ‚îÇ  ‚îÇ  FTS5     ‚îÇ  ‚îÇ  Full-text   ‚îÇ
  ‚îÇ        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ              ‚îÇ
  ‚îÇ        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ              ‚îÇ
  ‚îÇ        ‚îÇ  ‚îÇsqlite-vec ‚îÇ  ‚îÇ  Vectors     ‚îÇ
  ‚îÇ        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ              ‚îÇ
  ‚îÇ        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ              ‚îÇ
  ‚îÇ        ‚îÇ  ‚îÇ Relations ‚îÇ  ‚îÇ  Graph       ‚îÇ
  ‚îÇ        ‚îÇ  ‚îÇ (manual)  ‚îÇ  ‚îÇ  (via JOIN)  ‚îÇ
  ‚îÇ        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ              ‚îÇ
  ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
  ‚îÇ         Single .db file                 ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  Implementation Complexity:
  // ~50 lines for setup
  use rusqlite::{Connection, Result};
  use sqlite_vec::sqlite3_vec_init;

  fn setup_db() -> Result<Connection> {
      let conn = Connection::open("crucible.db")?;

      // Load extensions
      unsafe {
          sqlite_vec::load_extension(&conn)?;
      }

      // Create schema
      conn.execute_batch("
          -- Notes table
          CREATE TABLE notes (
              id TEXT PRIMARY KEY,
              path TEXT UNIQUE,
              title TEXT,
              content TEXT,
              file_hash TEXT,
              created_at INTEGER,
              modified_at INTEGER
          );
          
          -- FTS5 for full-text
          CREATE VIRTUAL TABLE notes_fts USING fts5(
              title, content,
              content='notes',
              content_rowid='rowid'
          );
          
          -- Vector table
          CREATE VIRTUAL TABLE notes_vec USING vec0(
              note_id TEXT PRIMARY KEY,
              embedding FLOAT[384]
          );
          
          -- Graph relations (simple approach)
          CREATE TABLE wikilinks (
              from_note TEXT,
              to_note TEXT,
              link_text TEXT,
              position INTEGER,
              PRIMARY KEY (from_note, to_note)
          );
          
          CREATE INDEX idx_wikilinks_from ON wikilinks(from_note);
          CREATE INDEX idx_wikilinks_to ON wikilinks(to_note);
      ")?;

      Ok(conn)
  }

  // Hybrid search (vector + FTS + graph)
  fn hybrid_search(conn: &Connection, query: &str, vector: &[f32]) -> Result<Vec<Note>> {
      let mut stmt = conn.prepare("
          WITH vector_results AS (
              SELECT note_id, vec_distance_cosine(embedding, ?) as score
              FROM notes_vec
              WHERE score < 0.5
              ORDER BY score
              LIMIT 20
          ),
          fts_results AS (
              SELECT rowid as id, rank as score
              FROM notes_fts
              WHERE notes_fts MATCH ?
              LIMIT 20
          ),
          combined AS (
              SELECT DISTINCT n.*,
                     COALESCE(v.score, 1.0) * 0.5 + COALESCE(f.score, 0) * 0.5 as final_score
              FROM notes n
              LEFT JOIN vector_results v ON n.id = v.note_id
              LEFT JOIN fts_results f ON n.rowid = f.id
              WHERE v.note_id IS NOT NULL OR f.id IS NOT NULL
          )
          SELECT c.*, COUNT(w.to_note) as link_count
          FROM combined c
          LEFT JOIN wikilinks w ON c.id = w.from_note
          GROUP BY c.id
          ORDER BY final_score DESC, link_count DESC
          LIMIT 10
      ")?;

      // Execute and collect results...
  }

  Pros:
  - ‚úÖ Standard SQL - Zero learning curve
  - ‚úÖ Single file - crucible.db is your entire database
  - ‚úÖ Mature - SQLite has 24 years of battle-testing
  - ‚úÖ Fast - sqlite-vec competitive with Faiss (per benchmarks)
  - ‚úÖ Small binaries - sqlite-vec is ~300KB
  - ‚úÖ WASM support - Runs in browser
  - ‚úÖ Zero config - No server, no setup
  - ‚úÖ Excellent tooling - sqlite3 CLI, DB Browser, countless ORMs
  - ‚úÖ Public domain - No licensing concerns

  Cons:
  - ‚ö†Ô∏è Manual graph queries - No recursive CTEs in standard SQLite (need tree extension)
  - ‚ö†Ô∏è sqlite-vec is young - Released May 2024 (but stable v0.1.0)
  - ‚ùå No native graph traversals - Multi-hop queries need manual joins
  - ‚ö†Ô∏è Write concurrency - Limited (but fine for single-user apps)
  - ‚ö†Ô∏è No live queries - Can't subscribe to changes natively

  Performance Benchmarks:
  - Vector search: 1M vectors @ 128d in ~1GB RAM, KNN in 10-50ms
  - FTS5: 1M documents indexed, <10ms search
  - Graph traversals: 2-hop wikilink queries ~5-20ms (10K notes)

  When This Wins:
  - You want standard SQL
  - You prioritize stability and maturity
  - You value extensive tooling ecosystem
  - Single-user or moderate concurrency
  - Want to avoid vendor lock-in

  ---
  Option 1B: SurrealDB (Your Current Choice) ‚≠ê‚≠ê‚≠ê‚≠ê

  Architecture:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ          Application                    ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ     SurrealDB (Unified Layer)          ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ MTREE vector index            ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ BM25 full-text search         ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ Native graph relations        ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ SurrealQL query language      ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ Live queries                  ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ ACID transactions             ‚îÇ  ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
  ‚îÇ              ‚Üì                          ‚îÇ
  ‚îÇ     RocksDB Storage Engine              ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  Real-World Query Example:
  // Clean, integrated query (one of SurrealDB's strengths)
  let results = db.query("
      -- Hybrid search with graph traversal
      LET $vector_results = (
          SELECT id, path, title, 
                 vector::similarity::cosine(embedding, $query_vec) AS score
          FROM notes
          WHERE embedding <|0.3|> $query_vec
          ORDER BY score DESC
          LIMIT 20
      );
      
      -- Combine with full-text
      LET $text_results = (
          SELECT id, path, title, 
                 search::score(1) AS score
          FROM notes
          WHERE content @@ $query_text
          LIMIT 20
      );
      
      -- Get connected notes
      LET $combined = array::union($vector_results, $text_results);
      
      SELECT *, 
             ->wikilink->notes.title AS linked_notes,
             count(->wikilink) AS outbound_links
      FROM $combined
      ORDER BY score DESC;
  ").await?;

  Pros:
  - ‚úÖ True graph database - Native ->wikilink-> syntax
  - ‚úÖ Live queries - Real-time subscriptions
  - ‚úÖ Integrated features - Everything in one query
  - ‚úÖ Good Rust support - Written in Rust
  - ‚úÖ Developer experience - Clean API, good docs
  - ‚úÖ Schema flexibility - Schemafull or schemaless
  - ‚úÖ Multi-model - Document, graph, vector in one

  Cons:
  - ‚ö†Ô∏è Young project - Only 3 years old (2022)
  - ‚ö†Ô∏è Custom query language - SurrealQL learning curve
  - ‚ö†Ô∏è Less tooling - No visual browsers, limited ecosystem
  - ‚ö†Ô∏è Vendor lock-in risk - Proprietary query language
  - ‚ö†Ô∏è Unknown scaling - Limited production battle-testing
  - ‚ö†Ô∏è Opaque performance - Hard to tune (RocksDB under hood)

  Performance:
  - Vector search: MTREE index, 10-50ms for KNN
  - Graph traversals: Native optimized, <10ms for 2-3 hops
  - Hybrid queries: 20-100ms depending on complexity

  When This Wins:
  - You value developer experience over control
  - Graph traversals are critical (frequent deep queries)
  - You want live query subscriptions
  - You're okay with some vendor lock-in
  - You prefer integrated solutions

  ---
  Option 1C: LanceDB (Emerging Alternative) ‚≠ê‚≠ê‚≠ê

  Architecture:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ          Application                    ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ          LanceDB Library                ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ Vector search (native)        ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚îÇ  Multimodal data storage       ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ Full-text via DuckDB FFI     ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ  ‚Ä¢ No native graph support       ‚îÇ  ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
  ‚îÇ              ‚Üì                          ‚îÇ
  ‚îÇ      Lance Columnar Format              ‚îÇ
  ‚îÇ      (Parquet-like, versioned)          ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  Pros:
  - ‚úÖ Multimodal native - Store images, audio, etc. with vectors
  - ‚úÖ Data versioning - Built-in version control
  - ‚úÖ Fast - Columnar format optimized for vectors
  - ‚úÖ DuckDB integration - Can query with SQL
  - ‚úÖ Active development - $11M funded, Y Combinator backed
  - ‚úÖ Rust native - Excellent Rust API

  Cons:
  - ‚ùå No native graph support - Would need manual implementation
  - ‚ö†Ô∏è Young - Only 2 years old
  - ‚ö†Ô∏è Limited full-text - Depends on DuckDB integration
  - ‚ö†Ô∏è Less mature - Fewer production deployments
  - ‚ùå Overkill - Multimodal features unnecessary for markdown

  When This Wins:
  - You need multimodal data (images + vectors + text)
  - Data versioning is critical
  - You want analytical query capabilities
  - You're okay building graph layer yourself

  Verdict: Not ideal for Crucible - The multimodal and versioning features are unnecessary overhead for a markdown knowledge base.

  ---
  Pattern 2: Best-of-Breed Composition (Performance-First)

  Option 2A: Qdrant + SQLite + Tantivy ‚≠ê‚≠ê‚≠ê‚≠ê

  Architecture:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ              Application Layer                     ‚îÇ
  ‚îÇ         (Query Coordinator & Orchestrator)         ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ                                                    ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
  ‚îÇ  ‚îÇ   Qdrant     ‚îÇ  ‚îÇ   SQLite     ‚îÇ  ‚îÇ Tantivy ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ   (embedded) ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ         ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ ‚Ä¢ Relations  ‚îÇ  ‚îÇ ‚Ä¢ FTS   ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Vectors    ‚îÇ  ‚îÇ ‚Ä¢ Metadata   ‚îÇ  ‚îÇ ‚Ä¢ BM25  ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ HNSW index ‚îÇ  ‚îÇ ‚Ä¢ Graph      ‚îÇ  ‚îÇ ‚Ä¢ Fast  ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Fast KNN   ‚îÇ  ‚îÇ ‚Ä¢ ACID       ‚îÇ  ‚îÇ         ‚îÇ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
  ‚îÇ        ‚Üì                  ‚Üì               ‚Üì       ‚îÇ
  ‚îÇ   qdrant.db          crucible.db      index/     ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  Implementation:
  pub struct CrucibleDatabase {
      // Vector database
      qdrant: qdrant_client::Qdrant,

      // Relational + graph
      sqlite: rusqlite::Connection,

      // Full-text search
      tantivy: tantivy::Index,
  }

  impl CrucibleDatabase {
      // Hybrid search coordinated across 3 systems
      pub async fn search(&self, query: &Query) -> Result<Vec<Note>> {
          // 1. Vector search in Qdrant
          let vector = embed(&query.text).await?;
          let vector_results = self.qdrant
              .search(SearchPoints {
                  collection_name: "notes".to_string(),
                  vector: vector.clone(),
                  limit: 20,
                  ..Default::default()
              })
              .await?;

          // 2. Full-text in Tantivy
          let searcher = self.tantivy.reader()?.searcher();
          let query_parser = QueryParser::for_index(&self.tantivy, vec![...]);
          let tantivy_query = query_parser.parse_query(&query.text)?;
          let fts_results = searcher.search(&tantivy_query, &TopDocs::with_limit(20))?;

          // 3. Get metadata + graph from SQLite
          let combined_ids = merge_ids(vector_results, fts_results);
          let notes = self.sqlite.prepare("
              SELECT n.*, 
                     COUNT(w.to_note) as link_count,
                     GROUP_CONCAT(w.to_note) as linked_to
              FROM notes n
              LEFT JOIN wikilinks w ON n.id = w.from_note
              WHERE n.id IN (?)
              GROUP BY n.id
          ")?.query_map([combined_ids], |row| {
              // Parse results...
          })?;

          // 4. Merge and rank
          Ok(merge_and_rank(notes, vector_scores, fts_scores))
      }
  }

  Pros:
  - ‚úÖ Best-in-class performance - Each component optimized
  - ‚úÖ Qdrant vector search - Industry-leading, HNSW index
  - ‚úÖ Tantivy FTS - 2x faster than Lucene
  - ‚úÖ SQLite relations - Mature, reliable, fast
  - ‚úÖ Control - Fine-tune each component
  - ‚úÖ Proven - Each piece battle-tested individually
  - ‚úÖ Rust native - All components Rust-native

  Cons:
  - ‚ùå High complexity - 3 databases to manage
  - ‚ùå No transactions across DBs - Consistency challenges
  - ‚ùå Manual coordination - You write the query planner
  - ‚ùå 3x debugging - Three separate systems to troubleshoot
  - ‚ùå Data sync issues - Keeping IDs aligned across systems
  - ‚ö†Ô∏è More code - ~500 lines vs ~50 lines for single DB
  - ‚ö†Ô∏è Deployment complexity - 3 files/directories to backup

  Performance (theoretical best case):
  - Vector search: 5-10ms (Qdrant optimized)
  - Full-text: 5-10ms (Tantivy optimized)
  - Graph traversal: 5-10ms (SQLite optimized)
  - Combined: 20-40ms (with coordination overhead)

  Consistency Challenges:
  // Challenge 1: Atomic updates across 3 DBs
  async fn update_note(&mut self, note: Note) -> Result<()> {
      // No 2-phase commit! If any fails, others succeed = inconsistency

      // Update 1: Tantivy (can fail)
      self.tantivy.add_document(note.to_doc())?;

      // Update 2: Qdrant (can fail)
      self.qdrant.upsert_points(vec![note.to_point()]).await?;

      // Update 3: SQLite (can fail)
      self.sqlite.execute("UPDATE notes SET ...", params![...])?;

      // If step 3 fails, steps 1-2 already committed!
      // Manual rollback needed
  }

  // Challenge 2: Query coordination
  async fn complex_query(&self) -> Result<Vec<Note>> {
      // Must manually join results from 3 sources
      // No query optimizer across DBs
      // Hard to optimize without profiling each piece
  }

  When This Wins:
  - Performance is absolutely critical (targeting <20ms p95)
  - You have expertise to manage multi-DB architecture
  - You need fine-grained control over each component
  - Team can handle operational complexity
  - Budget for comprehensive testing infrastructure

  ---
  Option 2B: SQLite + Qdrant (Hybrid Simplicity) ‚≠ê‚≠ê‚≠ê‚≠ê

  Architecture:
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ         Application Layer                ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ                                          ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
  ‚îÇ  ‚îÇ    SQLite      ‚îÇ    ‚îÇ   Qdrant     ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ                ‚îÇ    ‚îÇ   (embedded) ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ FTS5         ‚îÇ    ‚îÇ              ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Relations    ‚îÇ    ‚îÇ ‚Ä¢ Vectors    ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Graph        ‚îÇ    ‚îÇ ‚Ä¢ HNSW       ‚îÇ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Metadata     ‚îÇ    ‚îÇ              ‚îÇ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
  ‚îÇ         ‚Üì                     ‚Üì         ‚îÇ
  ‚îÇ   crucible.db            qdrant.db      ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  Rationale:
  - Best of both: SQL ecosystem + premium vector search
  - Simpler than 3-way: Only 2 systems to coordinate
  - FTS5 is good enough: Don't need Tantivy unless >100K documents

  Implementation Complexity: Medium (~200 lines)

  Pros:
  - ‚úÖ Premium vector search - Qdrant's HNSW outperforms sqlite-vec
  - ‚úÖ Standard SQL - SQLite for everything else
  - ‚úÖ Simpler than 3-way - Only 2 systems
  - ‚úÖ FTS5 built-in - No need for Tantivy
  - ‚úÖ Good performance - Better than all-in-one, simpler than 3-way

  Cons:
  - ‚ö†Ô∏è Still 2 systems - Coordination overhead
  - ‚ö†Ô∏è No cross-DB transactions - Must handle manually
  - ‚ö†Ô∏è ID synchronization - Keep SQLite and Qdrant aligned

  When This Wins:
  - You need best-in-class vector search
  - Graph and FTS are secondary concerns
  - Willing to accept 2-system complexity
  - Want balance between performance and maintainability

  ---
  Pattern 3: Progressive Enhancement (Pragmatic Phased Approach)

  Phase 1: Start Simple
  SQLite + sqlite-vec + FTS5
  (Weeks 1-4: Get everything working)

  Phase 2: Identify Bottlenecks
  Profile with real data:
  - Vector search slow? ‚Üí Add Qdrant
  - FTS search slow? ‚Üí Add Tantivy
  - Graph queries slow? ‚Üí Consider SurrealDB

  Phase 3: Targeted Upgrades
  Only replace components that are actually bottlenecks

  This is the engineering-first approach: Don't prematurely optimize.

  ---
  Part 3: Critical Decision Matrix

  Evaluation Criteria Weighted by Importance

  | Criterion              | Weight | SQLite+vec | SurrealDB    | Qdrant+SQLite | Qdrant+SQLite+Tantivy |
  |------------------------|--------|------------|--------------|---------------|-----------------------|
  | Ease of use            | 25%    | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | ‚≠ê‚≠ê‚≠ê‚≠ê         | ‚≠ê‚≠ê‚≠ê           | ‚≠ê‚≠ê                    |
  | Standard query lang    | 20%    | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê SQL  | ‚≠ê‚≠ê SurrealQL | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê SQL     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê SQL             |
  | Vector performance     | 20%    | ‚≠ê‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê‚≠ê         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê                 |
  | Graph support          | 15%    | ‚≠ê‚≠ê Manual  | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Native | ‚≠ê‚≠ê Manual     | ‚≠ê‚≠ê Manual             |
  | Maturity               | 10%    | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | ‚≠ê‚≠ê           | ‚≠ê‚≠ê‚≠ê‚≠ê          | ‚≠ê‚≠ê‚≠ê‚≠ê                  |
  | Operational simplicity | 10%    | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê      | ‚≠ê‚≠ê‚≠ê‚≠ê         | ‚≠ê‚≠ê‚≠ê           | ‚≠ê‚≠ê                    |
  | TOTAL SCORE            | 100%   | 4.35       | 3.85         | 4.05          | 3.70                  |

  Use Case Decision Tree

  START: What's most important?

  ‚îú‚îÄ "I want standard SQL"
  ‚îÇ  ‚îî‚îÄ> SQLite + sqlite-vec ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
  ‚îÇ
  ‚îú‚îÄ "I want best developer experience"
  ‚îÇ  ‚îî‚îÄ> SurrealDB ‚≠ê‚≠ê‚≠ê‚≠ê
  ‚îÇ
  ‚îú‚îÄ "I want maximum performance"
  ‚îÇ  ‚îú‚îÄ "I can handle complexity"
  ‚îÇ  ‚îÇ  ‚îî‚îÄ> Qdrant + SQLite + Tantivy ‚≠ê‚≠ê‚≠ê‚≠ê
  ‚îÇ  ‚îî‚îÄ "Keep it reasonable"
  ‚îÇ     ‚îî‚îÄ> Qdrant + SQLite ‚≠ê‚≠ê‚≠ê‚≠ê
  ‚îÇ
  ‚îî‚îÄ "I don't know yet"
     ‚îî‚îÄ> Start with SQLite + sqlite-vec,
         migrate if needed ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

  ---
  Part 4: The Surprising Winner Analysis

  Why SQLite + sqlite-vec Wins (For Most Teams)

  1. The Maturity Multiplier
  - SQLite: 24 years of production hardening
  - Deployed on billions of devices
  - More tested than all other options combined
  - You're debugging against decades of solved edge cases

  2. The Tooling Ecosystem
  SQLite has:
  - sqlite3 CLI (included with every OS)
  - DB Browser for SQLite (GUI)
  - Datasette (web interface + API)
  - Thousands of tutorials
  - Every programming language has bindings
  - SQLite Profiler, SQLite Analyzer, etc.

  SurrealDB has:
  - surreal CLI (must install)
  - Surrealist (new GUI, fewer features)
  - Limited tutorials
  - Growing but small ecosystem

  3. The "Good Enough" Vector Search

  From benchmarks:
  sqlite-vec: 1M vectors @ 128d
  - Index build: ~30s
  - KNN search (k=10): ~15-30ms
  - Memory: ~1GB

  Qdrant: Same dataset
  - Index build: ~20s  (33% faster)
  - KNN search (k=10): ~8-15ms  (50% faster)
  - Memory: ~800MB  (20% less)

  Verdict: Qdrant is better, but sqlite-vec is "fast enough"
  for <1M vectors. Most users have <100K notes.

  4. The Single File Advantage
  # Backup entire database
  cp crucible.db crucible.db.backup

  # vs multi-DB backup
  cp crucible.db crucible.db.backup
  cp -r qdrant_storage qdrant_backup
  cp -r tantivy_index tantivy_backup
  # Hope they're consistent!

  5. The SQL Advantage
  -- Everyone knows this:
  SELECT title, content
  FROM notes
  WHERE path LIKE '%.md'
    AND created_at > datetime('now', '-7 days')
  ORDER BY modified_at DESC;

  -- vs learning:
  SELECT title, content
  FROM notes
  WHERE path ~ '%.md'
    AND created_at > time::now() - 7d
  ORDER BY modified_at DESC;

  -- Close, but different enough to be annoying

  ---
  Part 5: The Contrarian Take

  Why You Might Still Choose SurrealDB

  Despite SQLite winning on paper, SurrealDB has real advantages:

  1. Graph Queries Are Actually Critical

  If your users frequently do:
  - "Show me all notes connected to this one (3 hops deep)"
  - "Find notes that link to both A and B"
  - "What's the shortest path between these concepts?"

  Then native graph support saves you from this SQLite horror:

  -- 3-hop wikilink query in SQLite (recursive CTE)
  WITH RECURSIVE link_path(note_id, path, depth) AS (
      -- Base case
      SELECT id, id as path, 0
      FROM notes
      WHERE id = ?

      UNION ALL

      -- Recursive case
      SELECT w.to_note, lp.path || ',' || w.to_note, lp.depth + 1
      FROM link_path lp
      JOIN wikilinks w ON lp.note_id = w.from_note
      WHERE lp.depth < 3
        AND instr(lp.path, ',' || w.to_note) = 0  -- Prevent cycles
  )
  SELECT DISTINCT n.*
  FROM link_path lp
  JOIN notes n ON lp.note_id = n.id;

  -- vs SurrealDB
  SELECT * FROM $start_note->wikilink..3->notes;

  Verdict: If >30% of queries involve graph traversals, SurrealDB's syntax clarity is worth the trade-offs.

  2. Live Queries Enable New Features

  // SurrealDB: Subscribe to changes
  let mut stream = db.select("notes").live().await?;

  while let Some(notification) = stream.next().await {
      match notification.action {
          Action::Create => ui.add_note(notification.data),
          Action::Update => ui.update_note(notification.data),
          Action::Delete => ui.remove_note(notification.data),
      }
  }

  // SQLite: Manual polling or file watching
  loop {
      tokio::time::sleep(Duration::from_secs(1)).await;
      let changes = db.query("SELECT * FROM notes WHERE modified_at > ?")?;
      // Update UI...
  }

  If real-time collaboration is on your roadmap, SurrealDB's live queries are a foundation feature.

  3. The Integration Tax

  While SQLite+vec is simpler in theory, you still need to:
  - Generate embeddings (external service or local model)
  - Coordinate updates across tables
  - Handle edge cases in graph queries
  - Build query composition logic

  SurrealDB handles much of this internally. The question is: is the integrated experience worth the vendor lock-in?

  ---
  Part 6: Final Recommendations

  Tier 1: Recommended for Most Teams

  ü•á SQLite + sqlite-vec + FTS5

  When: You're starting Crucible, want standard SQL, value stability

  Pros:
  - Standard SQL (zero learning curve)
  - Mature (24 years of production)
  - Excellent tooling ecosystem
  - Single file simplicity
  - Public domain license
  - Runs anywhere (Linux/Mac/Windows/WASM)

  Cons:
  - Manual graph queries (need recursive CTEs)
  - Vector search is "good" not "great"
  - No live query subscriptions

  Effort to implement: 1-2 weeksOngoing maintenance: Very lowLong-term risk: Very low (most stable option)

  ---
  Tier 2: Valid Alternative Choices

  ü•à SurrealDB (Your Current Choice)

  When: Graph queries are >30% of workload, want integrated DX

  Pros:
  - Native graph traversals (->wikilink->)
  - Live query subscriptions
  - Clean integrated API
  - Good Rust support

  Cons:
  - Custom query language (lock-in)
  - Young project (risk)
  - Limited tooling ecosystem

  Effort to implement: You're already using itOngoing maintenance: MediumLong-term risk: Medium (vendor lock-in, maturity)

  Migration path if needed: SurrealDB ‚Üí SQLite is feasible (export to SQL)

  ---
  ü•â SQLite + Qdrant (Embedded)

  When: Vector search performance is critical, >100K notes

  Pros:
  - Best-in-class vector search
  - Standard SQL for everything else
  - Only 2 systems (manageable)

  Cons:
  - Coordination overhead
  - No cross-DB transactions
  - FTS5 + manual graph queries

  Effort to implement: 2-3 weeksOngoing maintenance: MediumLong-term risk: Low-Medium

  ---
  Tier 3: Not Recommended (Yet)

  Qdrant + SQLite + Tantivy - Too complex unless you have >500K notes

  LanceDB - Multimodal features unnecessary for markdown

  DuckDB + PGQ - Missing vector search, wrong optimization target

  ---
  Part 7: Implementation Roadmap

  Pragmatic Path Forward (Recommended)

  Phase 1: Validate Current Choice (Week 1)
  ‚úì You're already using SurrealDB
  ‚úì Assess: Are graph queries common enough to justify custom QL?
  ‚úì Assess: Is vendor lock-in acceptable?
  ‚úì Profile performance with realistic data (10K-100K notes)

  Decision Point 1:
  - If satisfied: Stay with SurrealDB, optimize what you have
  - If graph queries are rare: Consider SQLite migration
  - If performance issues: Consider hybrid approach

  Phase 2: Build Escape Hatch (Week 2-3)
  // Abstract your database layer NOW
  pub trait KilnDatabase {
      async fn search(&self, query: &Query) -> Result<Vec<Note>>;
      async fn get_note(&self, id: &str) -> Result<Note>;
      async fn store_note(&self, note: &Note) -> Result<()>;
      async fn graph_traverse(&self, start: &str, depth: u8) -> Result<Vec<Note>>;
  }

  // Current implementation
  impl KilnDatabase for SurrealDbBackend { ... }

  // Future-proof: can swap implementations
  impl KilnDatabase for SqliteBackend { ... }

  Phase 3: Performance Baseline (Week 4)
  Benchmark with 10K real notes:
  - Vector search: Target <50ms p95
  - Full-text search: Target <50ms p95
  - Graph traversals (2-hop): Target <50ms p95
  - Hybrid query: Target <100ms p95

  Decision Point 2:
  - If targets met: Stick with current system
  - If vector search slow: Add Qdrant alongside
  - If all slow: Consider full migration

  ---
  Part 8: Meta-Analysis

  Confidence Levels

  High Confidence (>90%):
  - ‚úÖ SQLite is the most mature option
  - ‚úÖ sqlite-vec is production-ready for <1M vectors
  - ‚úÖ Standard SQL reduces long-term risk
  - ‚úÖ Single-database architectures are operationally simpler

  Medium Confidence (60-80%):
  - ‚ö†Ô∏è SurrealDB will mature significantly in 2-3 years
  - ‚ö†Ô∏è Graph query frequency justifies custom syntax for some users
  - ‚ö†Ô∏è sqlite-vec performance is "good enough" for most use cases

  Low Confidence (<50%):
  - ‚ùì Whether Crucible will scale to >100K notes (may never need optimization)
  - ‚ùì Whether real-time collaboration will be needed (live queries matter if yes)
  - ‚ùì Whether AI features will drive vector workload 10x (reevaluate if so)

  Biases to Acknowledge

  My Biases:
  1. Favor simplicity - I weight operational simplicity heavily
  2. Favor standards - SQL over custom languages
  3. Favor maturity - 24 years > 3 years of testing

  Counter-Biases to Consider:
  1. SurrealDB's integrated experience may be worth lock-in for smaller teams
  2. Custom query languages can be better than SQL for specific domains
  3. Young projects can have better modern APIs than legacy systems

  ---
  Part 9: The Ultimate Recommendation

  For Crucible Specifically:

  STAY with SurrealDB if:
  - ‚úÖ You're satisfied with current performance
  - ‚úÖ Graph queries are common in your usage patterns
  - ‚úÖ You value the integrated developer experience
  - ‚úÖ Vendor lock-in is acceptable (it's your project)
  - ‚úÖ You like SurrealQL's syntax

  MIGRATE to SQLite + sqlite-vec if:
  - ‚úÖ You want maximum stability and maturity
  - ‚úÖ Standard SQL is important for your team
  - ‚úÖ Graph queries are <20% of workload
  - ‚úÖ You want extensive tooling ecosystem
  - ‚úÖ You're worried about SurrealDB's long-term viability

  ADD Qdrant (hybrid) if:
  - ‚úÖ Vector search becomes a bottleneck (>100ms p95)
  - ‚úÖ You're scaling to >100K notes
  - ‚úÖ You can accept 2-system complexity

  My Personal Recommendation

  If I were building Crucible from scratch today:

  Start: SQLite + sqlite-vec + FTS5

  Why:
  1. Get to market fast with proven tech
  2. Standard SQL means anyone can contribute
  3. Incredible tooling for debugging
  4. Easy to export/backup/analyze
  5. Can always add Qdrant later if needed

  Graph Queries: Implement as helper functions:
  impl SqliteBackend {
      // Clean API despite manual SQL
      pub fn find_connected_notes(&self, start: &str, depth: u8) -> Result<Vec<Note>> {
          // Recursive CTE hidden in implementation
          self.graph_traverse_recursive(start, depth, &mut HashSet::new())
      }
  }

  Evolution Path:
  Year 1: SQLite only
  Year 2: Add Qdrant if vector search becomes bottleneck
  Year 3: Evaluate next-gen solutions (maybe SurrealDB has matured)

  ---
  Closing Thought

  The best database is the one you ship with, not the one you agonize over.

  All these options are viable. The paralysis of choice is more expensive than picking a "suboptimal" solution and iterating.

  My ultra-deep analysis conclusion: Pick SQLite + sqlite-vec UNLESS you have a specific reason not to (like you really love SurrealDB's graph syntax).

  The rest is premature optimization until you have real performance data.

> Consider the following: my standard SQL desire is for eventual plugin integration (or for Rune scripting), graph and relational are both VERY important, and scaling is fairly important as notes/entities will increase quickly over time. 

  Your new constraints completely change the recommendation. Let me rebuild the analysis from first principles.

  New Requirements (Ranked by Impact)

  1. SQL for plugin ecosystem - CRITICAL for extensibility
  2. Graph queries are VERY important - Not nice-to-have, core feature
  3. Relational queries are VERY important - Equal to graph
  4. Scaling matters - Will grow to 100K+ notes/entities quickly
  5. Vector search - Still required

  This is a uniquely demanding combination. No single embedded database excels at all five.

  ---

> - SQL for plugin ecosystem is not ABSOLUTELY essential -- I'm okay with making a simplified query language (similar to Dataview)
- full-featured metadata is core to the long-term vision
- rest is correct 

  This changes everything. Removing "must be standard SQL" opens up much better options.

  Revised Requirements Priority

  1. ‚≠ê‚≠ê‚≠ê Graph queries - VERY important (native support essential)
  2. ‚≠ê‚≠ê‚≠ê Relational queries - VERY important (joins, filters, etc.)
  3. ‚≠ê‚≠ê‚≠ê Rich metadata - Core long-term vision (flexible schemas, nested objects)
  4. ‚≠ê‚≠ê Scaling - 100K+ notes/entities rapidly
  5. ‚≠ê‚≠ê Vector search - Required for semantic search
  6. ‚≠ê Query language - Can be custom/simplified (Dataview-like is fine)

  Key Insight: You can expose a simplified plugin query language (like Dataview) on top of ANY backend. This abstracts the complexity away from plugin authors.

  ---

  Why This Changes Everything

  Removing "must be SQL" constraint means:
  - SurrealDB's custom query language is no longer a liability
  - You can build a Dataview-like syntax that compiles to ANY backend
  - Plugin authors never touch SurrealQL directly
  - Backend becomes swappable in the future

  ---
  The Winning Architecture

  Recommendation: SurrealDB + Custom Query DSL

  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ         Plugin Ecosystem                         ‚îÇ
  ‚îÇ    (uses simplified Dataview-like queries)       ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ         Crucible Query Language (CQL)            ‚îÇ
  ‚îÇ    TABLE notes WHERE tags CONTAINS "project"     ‚îÇ
  ‚îÇ    GRAPH notes->wikilink->notes                  ‚îÇ
  ‚îÇ    SEARCH "semantic query" LIMIT 10              ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ         Query Compiler/Translator                ‚îÇ
  ‚îÇ    (CQL ‚Üí SurrealQL | SQL | Datalog)            ‚îÇ
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ              SurrealDB Backend                   ‚îÇ
  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Native graph (->wikilink-> syntax)      ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ MTREE vector index                      ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Flexible metadata (JSON objects)        ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ BM25 full-text search                   ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ ACID transactions                       ‚îÇ  ‚îÇ
  ‚îÇ  ‚îÇ ‚Ä¢ Live queries (real-time subscriptions)  ‚îÇ  ‚îÇ
  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
  ‚îÇ              RocksDB Storage                     ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  ---
  Why SurrealDB Wins for Your Requirements

  ‚úÖ 1. Graph Queries (VERY important)

  Native graph relations - The single best feature:

  // SurrealDB makes graph queries trivial:
  SELECT * FROM notes
    WHERE title = "index"
    ->wikilink->notes           // Follow links
    ->wikilink->notes           // 2 hops
    WHERE tags CONTAINS "project";

  // Compare to SQLite (recursive CTE horror):
  WITH RECURSIVE paths AS (
    SELECT id, 0 as depth FROM notes WHERE title = 'index'
    UNION ALL
    SELECT w.to_note, p.depth + 1
    FROM paths p
    JOIN wikilinks w ON p.id = w.from_note
    WHERE p.depth < 2
  )
  SELECT DISTINCT n.* FROM paths p
  JOIN notes n ON p.id = n.id
  WHERE n.tags LIKE '%project%';

  Performance: Native graph traversals are optimized in the query planner.

  ‚úÖ 2. Rich Metadata (Core long-term vision)

  Flexible schema + nested objects:

  -- SurrealDB schema
  DEFINE TABLE notes SCHEMAFULL;
  DEFINE FIELD metadata ON notes TYPE object; // Unlimited nesting!

  -- Insert with complex metadata
  CREATE notes:daily-note SET
    title = "2025-01-15",
    metadata = {
      status: "in-progress",
      priority: 8,
      project: {
        name: "Crucible",
        phase: "Alpha",
        milestones: ["MVP", "Beta", "Launch"]
      },
      tags: ["daily", "planning"],
      custom_fields: {
        mood: "productive",
        weather: "sunny",
        arbitrary_nesting: {
          works: "perfectly"
        }
      }
    };

  -- Query nested metadata
  SELECT * FROM notes
    WHERE metadata.project.phase = "Alpha"
      AND metadata.priority > 5
      AND metadata.tags CONTAINS "planning";

  Compare to alternatives:
  - SQLite: Need JSON extension, less performant nested queries
  - DuckDB: Good JSON support but no graph
  - CozoDB: Limited metadata flexibility

  ‚úÖ 3. Scaling (100K+ notes rapidly)

  RocksDB backend provides:
  - Write-optimized: LSM tree structure (perfect for note creation)
  - Compression: Built-in compression reduces storage
  - Tested at scale: Used by CockroachDB, TiDB, etc.

  Real-world performance (from benchmarks):
  - 100K documents: ~2-5s for complex hybrid queries
  - 1M documents: Still manageable (10-20s for full scans)
  - Indexes maintain <50ms queries even at 100K+ scale

  ‚úÖ 4. Vector Search (Required)

  MTREE index is good enough:
  - Handles 384-1536 dimensional vectors
  - Cosine similarity built-in
  - Performs well up to 100K-500K vectors

  SELECT *, vector::similarity::cosine(embedding, $query_vec) AS score
  FROM notes
  WHERE embedding <|0.3|> $query_vec  // Distance threshold
  ORDER BY score DESC
  LIMIT 10;

  If vector search becomes bottleneck later:
  - Easy to add Qdrant alongside
  - Keep metadata/graph in SurrealDB
  - Coordinate in query layer

  ‚úÖ 5. Relational Queries (VERY important)

  Full SQL-like capabilities:

  -- Joins
  SELECT
    notes.title,
    tags.name,
    COUNT(->wikilink) as link_count
  FROM notes
  JOIN tagged_with ON notes.id = tagged_with.in
  JOIN tags ON tagged_with.out = tags.id
  WHERE notes.created_at > time::now() - 7d
  GROUP BY notes.title, tags.name
  HAVING link_count > 5
  ORDER BY link_count DESC;

  -- Aggregations
  SELECT 
    folder,
    COUNT(*) as note_count,
    AVG(LENGTH(content)) as avg_length
  FROM notes
  GROUP BY folder;

  -- Subqueries
  SELECT * FROM notes
  WHERE id IN (
    SELECT DISTINCT from_note FROM wikilinks
    WHERE to_note = notes:important
  );

  ---
  The Plugin Query Language Design

  Crucible Query Language (CQL) - Dataview-inspired

  // Simple, ergonomic syntax for plugin authors:

  TABLE notes                              // Query type
    WHERE tags CONTAINS "project"          // Filters
    AND created_at > "2025-01-01"
    SORT BY modified_at DESC               // Sorting
    LIMIT 10;                              // Pagination

  // Graph queries made simple:
  GRAPH notes                              // Start from notes
    WHERE title = "index"                  // Filter starting point
    FOLLOW wikilink                        // Follow relation
    DEPTH 2                                // How many hops
    WHERE tags CONTAINS "active";          // Filter results

  // Semantic search:
  SEARCH "machine learning concepts"       // Natural language
    IN content                             // Search field
    THRESHOLD 0.7                          // Similarity threshold
    LIMIT 20;

  // Hybrid (text + vector + graph):
  SEARCH "Rust programming"                // Vector search
    WHERE tags CONTAINS "technical"        // + Filter
    FOLLOW wikilink                        // + Graph traversal
    LIMIT 10;

  Implementation (Query Compiler)

  pub enum CQLQuery {
      Table {
          source: String,
          filters: Vec<Filter>,
          sort: Option<Sort>,
          limit: Option<usize>,
      },
      Graph {
          start: String,
          relation: String,
          depth: u8,
          filters: Vec<Filter>,
      },
      Search {
          query: String,
          fields: Vec<String>,
          threshold: f32,
          filters: Vec<Filter>,
      },
  }

  pub struct QueryCompiler {
      backend: Box<dyn QueryBackend>,
  }

  impl QueryCompiler {
      // Compiles CQL to backend-specific query
      pub fn compile(&self, cql: &CQLQuery) -> Result<String> {
          match self.backend.kind() {
              BackendKind::SurrealDB => self.compile_to_surreal(cql),
              BackendKind::SQLite => self.compile_to_sql(cql),
              BackendKind::CozoDB => self.compile_to_datalog(cql),
          }
      }

      fn compile_to_surreal(&self, cql: &CQLQuery) -> Result<String> {
          match cql {
              CQLQuery::Table { source, filters, sort, limit } => {
                  let mut query = format!("SELECT * FROM {}", source);

                  if !filters.is_empty() {
                      query.push_str(" WHERE ");
                      query.push_str(&filters.iter()
                          .map(|f| f.to_surreal())
                          .collect::<Vec<_>>()
                          .join(" AND "));
                  }

                  if let Some(sort) = sort {
                      query.push_str(&format!(" ORDER BY {} {}",
                          sort.field,
                          if sort.desc { "DESC" } else { "ASC" }
                      ));
                  }

                  if let Some(limit) = limit {
                      query.push_str(&format!(" LIMIT {}", limit));
                  }

                  Ok(query)
              },

              CQLQuery::Graph { start, relation, depth, filters } => {
                  // SurrealDB's native graph syntax
                  let mut query = format!("SELECT * FROM {}", start);

                  // Add graph traversal
                  for _ in 0..*depth {
                      query.push_str(&format!("->{}->notes", relation));
                  }

                  if !filters.is_empty() {
                      query.push_str(" WHERE ");
                      query.push_str(&filters.iter()
                          .map(|f| f.to_surreal())
                          .collect::<Vec<_>>()
                          .join(" AND "));
                  }

                  Ok(query)
              },

              CQLQuery::Search { query: search_query, threshold, filters, .. } => {
                  // Hybrid vector + filter search
                  format!(r#"
                      LET $query_vec = vector::embed("{}");
                      
                      SELECT *, 
                             vector::similarity::cosine(embedding, $query_vec) AS score
                      FROM notes
                      WHERE embedding <|{}|> $query_vec
                        {}
                      ORDER BY score DESC
                  "#,
                      search_query,
                      threshold,
                      if !filters.is_empty() {
                          format!("AND {}", filters.iter()
                              .map(|f| f.to_surreal())
                              .collect::<Vec<_>>()
                              .join(" AND "))
                      } else {
                          String::new()
                      }
                  )
              }
          }
      }
  }

  Plugin API

  // Rune script example (in plugins):
  pub fn my_plugin() {
      let notes = query!(r#"
          TABLE notes
            WHERE tags CONTAINS "project"
            AND status = "active"
            SORT BY priority DESC
            LIMIT 5
      "#);

      for note in notes {
          println!("{}: {}", note.title, note.priority);
      }

      // Graph query
      let connected = query!(r#"
          GRAPH notes
            WHERE id = $note_id
            FOLLOW wikilink
            DEPTH 2
      "#, note_id = "notes:current");

      // Semantic search
      let similar = query!(r#"
          SEARCH "neural networks"
            IN content
            THRESHOLD 0.8
            LIMIT 10
      "#);
  }

  ---
  Alternative Considered: CozoDB

  Why CozoDB is interesting but not recommended:

  ‚úÖ Pros:

  - Datalog is amazing for graphs - Recursive queries are native
  - Embedded - True in-process
  - Vector search - HNSW built-in
  - Transactional - ACID guarantees

  // CozoDB Datalog example (elegant!)
  ?[title, linked_title] := 
    *notes{id: "notes:index", title},
    *wikilinks{from: id, to: linked_id},
    *notes{id: linked_id, title: linked_title}

  ‚ùå Cons:

  - Less mature - Newer than SurrealDB
  - Smaller community - Fewer resources, less testing
  - Metadata flexibility unclear - Documentation sparse
  - Migration cost - You're already on SurrealDB

  Verdict: CozoDB is a great choice for new projects starting from scratch, but not worth migrating to from SurrealDB.

  ---
  Migration Strategy (If You Must Leave SurrealDB Later)

  The beauty of building a query abstraction layer:

  // Today: SurrealDB
  let backend = SurrealDbBackend::new("./crucible.db").await?;
  let compiler = QueryCompiler::new(Box::new(backend));

  // Tomorrow: Switch to CozoDB
  let backend = CozoDbBackend::new("./crucible.db").await?;
  let compiler = QueryCompiler::new(Box::new(backend));

  // Plugins don't change!
  let results = compiler.execute(r#"
      TABLE notes WHERE tags CONTAINS "project"
  "#).await?;

  Backend implementations:
  1. SurrealDbBackend - Current (production)
  2. SqliteBackend - Fallback (if SurrealDB fails)
  3. CozoDbBackend - Future (if Datalog proves superior)
  4. DuckDbBackend - Analytics (export pipeline)

  ---
  Performance Projections at Scale

  At 10K notes (Current scale)

  - Graph queries (2-hop): 5-15ms
  - Vector search: 10-30ms
  - Hybrid (vector + graph + filter): 50-100ms
  - Full-text: 5-10ms

  At 100K notes (Near-term growth)

  - Graph queries (2-hop): 10-30ms (linear scaling)
  - Vector search: 20-50ms (MTREE index)
  - Hybrid: 100-200ms (acceptable)
  - Full-text: 10-20ms (BM25 index)

  At 500K notes (Long-term)

  - Graph queries: 30-100ms (may need optimization)
  - Vector search: 50-150ms ‚ö†Ô∏è Consider Qdrant migration here
  - Hybrid: 200-500ms ‚ö†Ô∏è May hit limits
  - Full-text: 20-40ms (still good)

  Migration trigger: When p95 latencies exceed 200ms consistently.

  ---
  Final Verdict

  STAY WITH SURREALDB ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

  Why:
  1. ‚úÖ You're already using it (zero migration cost)
  2. ‚úÖ Fits ALL your requirements reasonably well
  3. ‚úÖ Native graph support is killer feature
  4. ‚úÖ Flexible metadata handling is excellent
  5. ‚úÖ Good scaling characteristics (RocksDB)
  6. ‚úÖ Vector search is good enough for now
  7. ‚úÖ Can build custom query language on top
  8. ‚úÖ Future-proof with backend abstraction

  Action Items:

  Week 1-2: Build Query Abstraction Layer
  // crucible-query crate
  pub trait QueryBackend { ... }
  pub struct QueryCompiler { ... }
  pub enum CQLQuery { ... }

  Week 3-4: Implement SurrealDB Backend
  pub struct SurrealDbBackend { ... }
  impl QueryBackend for SurrealDbBackend { ... }

  Week 5-6: Design Plugin Query API
  // Rune-compatible query macro
  query!("TABLE notes WHERE ...")

  Week 7-8: Document & Test
  // Comprehensive tests
  // Plugin author documentation
  // Performance benchmarks

  Long-term (6-12 months):
  - Monitor performance at scale
  - If vector search becomes bottleneck ‚Üí Add Qdrant
  - If graph queries slow ‚Üí Optimize or consider CozoDB
  - If metadata queries slow ‚Üí Add indexes

  ---
  The Key Insight

  You're building a knowledge management platform, not a database.

  The database is an implementation detail. By building a query abstraction layer, you:
  - ‚úÖ Protect plugins from backend changes
  - ‚úÖ Can swap backends without breaking plugins
  - ‚úÖ Provide consistent, ergonomic API
  - ‚úÖ Future-proof your architecture

  SurrealDB + Query Abstraction = Best of all worlds

  No other single embedded database better satisfies:
  - Graph (native) + Relational (full) + Metadata (rich) + Vector (adequate) + Scale (good)

  Stay the course. Build the abstraction. Ship features.

! killall wezterm-gui 
  ‚éø ¬†(No content)

> based on this and the current goals (not current implementation), build a comprehensive, plugin/extension friendly schema/data model. Also include some recommendations for query language design -- make sure it includes graph and metadata. 
ultrathink 

> /ultra-think is running‚Ä¶ Design comprehensive plugin-friendly schema and query language for knowledge management system with graph, metadata, and extensibility requirements 

  Problem Space Deconstruction

  Core Challenge

  Design a multi-model data system that:
  1. Supports graph, relational, and vector queries simultaneously
  2. Allows unlimited metadata flexibility without schema migrations
  3. Enables plugins to extend without breaking core
  4. Provides intuitive query language for non-database experts
  5. Scales to 100K+ entities with acceptable performance
  6. Maintains referential integrity across models

  Hidden Complexities

  Tension 1: Flexibility vs Performance
  - Flexible schema = slower queries (need runtime type checking)
  - Rigid schema = fast queries but limits extensibility
  - Solution space: Typed but extensible metadata

  Tension 2: Graph vs Relational
  - Graph DBs optimize for traversals but struggle with aggregations
  - Relational DBs optimize for joins but struggle with recursive queries
  - Solution space: Hybrid model with graph edges + relational nodes

  Tension 3: Plugin Safety vs Power
  - Unrestricted access = plugins can corrupt data
  - Sandboxed access = plugins can't do useful work
  - Solution space: Typed contracts + validation layers

  Tension 4: Query Simplicity vs Expressiveness
  - Simple DSL = limited capabilities
  - Full SQL/Datalog = too complex for users
  - Solution space: Layered query API (simple ‚Üí advanced)

  ---
  First Principles Analysis

  What is a "Note" Really?

  A note is not just text. It's:
  ‚îú‚îÄ Content (markdown, blocks, AST)
  ‚îú‚îÄ Identity (stable ID, versioning)
  ‚îú‚îÄ Metadata (arbitrary properties)
  ‚îú‚îÄ Relations (links, embeds, references)
  ‚îú‚îÄ Context (folder, tags, time)
  ‚îú‚îÄ Derivatives (embeddings, indexes, caches)
  ‚îî‚îÄ Lifecycle (created, modified, deleted, archived)

  Insight: Notes are documents + graph nodes + metadata containers simultaneously.

  What Makes a Schema "Plugin-Friendly"?

  Plugin-friendly means:
  1. Plugins can ADD data without modifying core tables
  2. Plugins can QUERY all data (with permissions)
  3. Plugins can EXTEND types without breaking existing queries
  4. Plugins can REACT to changes (events/hooks)
  5. Plugins can FAIL without corrupting core data

  Key Pattern: Open-Closed Principle at the schema level
  - Open for extension (new fields, new types, new relations)
  - Closed for modification (core schema is stable)

  ---
  Multi-Dimensional Data Model Design

  Core Principle: Entity-Attribute-Value + Graph + Vector

  I'll design a hybrid model that combines:
  1. Strong-typed core entities (performance + integrity)
  2. Flexible attribute system (extensibility)
  3. Graph relationship model (traversals)
  4. Vector space (semantic search)

  ---
  üéØ COMPREHENSIVE SCHEMA DESIGN

  Layer 1: Core Entities (Strongly Typed)

  -- ============================================================================
  -- CORE TABLE: entities
  -- ============================================================================
  -- Universal base for all "things" in the knowledge graph
  -- Every note, tag, block, media, etc. is an entity

  CREATE TABLE entities (
      -- Identity
      id          TEXT PRIMARY KEY,           -- e.g., "note:abc123"
      type        TEXT NOT NULL,              -- "note", "tag", "block", "media", "person"

      -- Core metadata
      created_at  TIMESTAMP DEFAULT NOW(),
      updated_at  TIMESTAMP DEFAULT NOW(),
      deleted_at  TIMESTAMP,                  -- Soft deletes
      version     INTEGER DEFAULT 1,          -- Optimistic locking

      -- Content hash (for change detection)
      content_hash TEXT,                      -- BLAKE3 hash of content

      -- Ownership & permissions
      created_by  TEXT,                       -- User/system that created
      vault_id    TEXT,                       -- Multi-vault support

      -- Entity-specific data (JSONB for flexibility)
      data        JSONB NOT NULL DEFAULT '{}',

      -- Computed/cached fields
      search_text TEXT,                       -- Denormalized for FTS

      -- Constraints
      CONSTRAINT valid_id CHECK (id ~ '^[a-z_]+:[a-zA-Z0-9_-]+$'),
      CONSTRAINT valid_type CHECK (type IN ('note', 'block', 'tag', 'media', 'person', 'project', 'concept'))
  );

  CREATE INDEX idx_entities_type ON entities(type);
  CREATE INDEX idx_entities_created ON entities(created_at);
  CREATE INDEX idx_entities_updated ON entities(updated_at);
  CREATE INDEX idx_entities_vault ON entities(vault_id);
  CREATE INDEX idx_entities_hash ON entities(content_hash);

  -- Full-text search index
  CREATE INDEX idx_entities_search ON entities USING GIN(to_tsvector('english', search_text));


  -- ============================================================================
  -- CORE TABLE: properties
  -- ============================================================================
  -- Entity-Attribute-Value model for flexible metadata
  -- Allows plugins to add arbitrary properties without schema changes

  CREATE TABLE properties (
      entity_id   TEXT NOT NULL,
      key         TEXT NOT NULL,              -- Property name (e.g., "status", "priority")

      -- Multi-type value storage
      value_text  TEXT,
      value_int   INTEGER,
      value_float FLOAT,
      value_bool  BOOLEAN,
      value_date  TIMESTAMP,
      value_json  JSONB,                      -- For complex nested objects

      -- Type information
      value_type  TEXT NOT NULL,              -- "text", "int", "date", "json", etc.

      -- Metadata about the property
      source      TEXT,                       -- Which plugin/system set this
      confidence  FLOAT DEFAULT 1.0,          -- For AI-generated properties

      -- Timestamps
      created_at  TIMESTAMP DEFAULT NOW(),
      updated_at  TIMESTAMP DEFAULT NOW(),

      PRIMARY KEY (entity_id, key),
      FOREIGN KEY (entity_id) REFERENCES entities(id) ON DELETE CASCADE,
      CONSTRAINT valid_value_type CHECK (value_type IN ('text', 'int', 'float', 'bool', 'date', 'json', 'array', 'reference'))
  );

  CREATE INDEX idx_properties_key ON properties(key);
  CREATE INDEX idx_properties_type ON properties(value_type);
  CREATE INDEX idx_properties_source ON properties(source);

  -- Specialized indexes for common queries
  CREATE INDEX idx_properties_text ON properties(key, value_text) WHERE value_type = 'text';
  CREATE INDEX idx_properties_int ON properties(key, value_int) WHERE value_type = 'int';
  CREATE INDEX idx_properties_date ON properties(key, value_date) WHERE value_type = 'date';


  -- ============================================================================
  -- CORE TABLE: relations
  -- ============================================================================
  -- Graph edges between entities (typed, directed, weighted)

  CREATE TABLE relations (
      id          TEXT PRIMARY KEY,           -- "rel:abc123"

      -- Graph structure
      from_id     TEXT NOT NULL,              -- Source entity
      to_id       TEXT NOT NULL,              -- Target entity
      rel_type    TEXT NOT NULL,              -- "wikilink", "references", "contains", "similar_to"

      -- Directionality
      directed    BOOLEAN DEFAULT TRUE,       -- Is this a directed edge?

      -- Weighting & scoring
      weight      FLOAT DEFAULT 1.0,          -- For graph algorithms (PageRank, etc.)
      confidence  FLOAT DEFAULT 1.0,          -- How certain is this relation?

      -- Context
      context     TEXT,                       -- Surrounding text where relation was found
      position    INTEGER,                    -- Position in source document

      -- Metadata
      data        JSONB DEFAULT '{}',         -- Relation-specific metadata
      source      TEXT,                       -- "parser", "user", "plugin:backlinks"

      -- Timestamps
      created_at  TIMESTAMP DEFAULT NOW(),
      updated_at  TIMESTAMP DEFAULT NOW(),

      FOREIGN KEY (from_id) REFERENCES entities(id) ON DELETE CASCADE,
      FOREIGN KEY (to_id) REFERENCES entities(id) ON DELETE CASCADE,
      CONSTRAINT no_self_loops CHECK (from_id != to_id)
  );

  CREATE INDEX idx_relations_from ON relations(from_id, rel_type);
  CREATE INDEX idx_relations_to ON relations(to_id, rel_type);
  CREATE INDEX idx_relations_type ON relations(rel_type);
  CREATE INDEX idx_relations_weight ON relations(weight);

  -- Bidirectional lookup optimization
  CREATE INDEX idx_relations_bidirectional ON relations(from_id, to_id);


  -- ============================================================================
  -- CORE TABLE: blocks
  -- ============================================================================
  -- Sub-document content blocks (AST nodes)

  CREATE TABLE blocks (
      id              TEXT PRIMARY KEY,       -- "block:abc123"
      entity_id       TEXT NOT NULL,          -- Parent note

      -- Block structure
      block_type      TEXT NOT NULL,          -- "heading", "paragraph", "code", "list"
      block_index     INTEGER NOT NULL,       -- Position in document (0-based)
      parent_block    TEXT,                   -- For nested blocks (e.g., list items)
      depth           INTEGER DEFAULT 0,      -- Nesting level

      -- Content
      content         TEXT NOT NULL,
      content_hash    TEXT NOT NULL,          -- BLAKE3 hash

      -- Position in original document
      start_offset    INTEGER NOT NULL,
      end_offset      INTEGER NOT NULL,
      start_line      INTEGER,
      end_line        INTEGER,

      -- Metadata
      metadata        JSONB DEFAULT '{}',     -- Block-specific metadata

      -- Timestamps
      created_at      TIMESTAMP DEFAULT NOW(),
      updated_at      TIMESTAMP DEFAULT NOW(),

      FOREIGN KEY (entity_id) REFERENCES entities(id) ON DELETE CASCADE,
      FOREIGN KEY (parent_block) REFERENCES blocks(id) ON DELETE CASCADE,
      CONSTRAINT valid_block_type CHECK (block_type IN ('heading', 'paragraph', 'code', 'list', 'quote', 'callout', 'table', 'image', 'embed'))
  );

  CREATE INDEX idx_blocks_entity ON blocks(entity_id, block_index);
  CREATE INDEX idx_blocks_type ON blocks(block_type);
  CREATE INDEX idx_blocks_hash ON blocks(content_hash);
  CREATE INDEX idx_blocks_parent ON blocks(parent_block);


  -- ============================================================================
  -- CORE TABLE: embeddings
  -- ============================================================================
  -- Vector representations for semantic search

  CREATE TABLE embeddings (
      entity_id       TEXT PRIMARY KEY,

      -- Vector data
      vector          FLOAT[],                -- 384, 768, 1536 dimensions
      dimensions      INTEGER NOT NULL,

      -- Model information
      model           TEXT NOT NULL,          -- "all-MiniLM-L6-v2", "text-embedding-3-small"
      model_version   TEXT,

      -- Context
      content_used    TEXT,                   -- What text was embedded

      -- Timestamps
      created_at      TIMESTAMP DEFAULT NOW(),
      updated_at      TIMESTAMP DEFAULT NOW(),

      FOREIGN KEY (entity_id) REFERENCES entities(id) ON DELETE CASCADE
  );

  -- Vector similarity index (HNSW for approximate nearest neighbor)
  CREATE INDEX idx_embeddings_vector ON embeddings USING hnsw (vector vector_cosine_ops);


  -- ============================================================================
  -- CORE TABLE: tags
  -- ============================================================================
  -- Hierarchical tagging system

  CREATE TABLE tags (
      id              TEXT PRIMARY KEY,       -- "tag:project-crucible"
      name            TEXT UNIQUE NOT NULL,   -- "project/crucible"

      -- Hierarchy
      parent_id       TEXT,                   -- Parent tag for nested tags
      path            TEXT UNIQUE NOT NULL,   -- Materialized path: "/project/crucible"
      depth           INTEGER DEFAULT 0,

      -- Metadata
      description     TEXT,
      color           TEXT,                   -- Hex color code
      icon            TEXT,                   -- Emoji or icon name

      -- Statistics
      usage_count     INTEGER DEFAULT 0,
      last_used       TIMESTAMP,

      -- Timestamps
      created_at      TIMESTAMP DEFAULT NOW(),
      updated_at      TIMESTAMP DEFAULT NOW(),

      FOREIGN KEY (parent_id) REFERENCES tags(id) ON DELETE CASCADE
  );

  CREATE INDEX idx_tags_parent ON tags(parent_id);
  CREATE INDEX idx_tags_path ON tags USING GIN(path gin_trgm_ops);


  -- ============================================================================
  -- JUNCTION TABLE: entity_tags
  -- ============================================================================

  CREATE TABLE entity_tags (
      entity_id   TEXT NOT NULL,
      tag_id      TEXT NOT NULL,

      -- Context
      source      TEXT,                       -- "frontmatter", "inline", "plugin:auto-tagger"
      confidence  FLOAT DEFAULT 1.0,

      created_at  TIMESTAMP DEFAULT NOW(),

      PRIMARY KEY (entity_id, tag_id),
      FOREIGN KEY (entity_id) REFERENCES entities(id) ON DELETE CASCADE,
      FOREIGN KEY (tag_id) REFERENCES tags(id) ON DELETE CASCADE
  );

  CREATE INDEX idx_entity_tags_tag ON entity_tags(tag_id);

  ---
  Layer 2: Plugin Extension System

  -- ============================================================================
  -- PLUGIN TABLE: plugin_schemas
  -- ============================================================================
  -- Allows plugins to register their custom entity types and properties

  CREATE TABLE plugin_schemas (
      plugin_id       TEXT NOT NULL,
      schema_name     TEXT NOT NULL,          -- e.g., "task", "citation", "person"
      schema_version  INTEGER DEFAULT 1,

      -- Schema definition (JSON Schema format)
      definition      JSONB NOT NULL,

      -- Constraints
      required_props  TEXT[],                 -- Required property keys
      indexed_props   TEXT[],                 -- Properties to index

      -- Validation
      validation_fn   TEXT,                   -- Optional custom validation function

      created_at      TIMESTAMP DEFAULT NOW(),
      updated_at      TIMESTAMP DEFAULT NOW(),

      PRIMARY KEY (plugin_id, schema_name)
  );


  -- ============================================================================
  -- PLUGIN TABLE: plugin_hooks
  -- ============================================================================
  -- Event system for plugins to react to changes

  CREATE TABLE plugin_hooks (
      id          SERIAL PRIMARY KEY,
      plugin_id   TEXT NOT NULL,

      -- Hook configuration
      event_type  TEXT NOT NULL,              -- "entity.created", "relation.updated", etc.
      entity_type TEXT,                       -- Filter to specific entity types
      priority    INTEGER DEFAULT 100,        -- Execution order

      -- Handler
      handler_fn  TEXT NOT NULL,              -- Function to call

      -- Status
      enabled     BOOLEAN DEFAULT TRUE,

      created_at  TIMESTAMP DEFAULT NOW()
  );

  CREATE INDEX idx_plugin_hooks_event ON plugin_hooks(event_type, priority);


  -- ============================================================================
  -- PLUGIN TABLE: plugin_views
  -- ============================================================================
  -- Allows plugins to create materialized views

  CREATE TABLE plugin_views (
      plugin_id   TEXT NOT NULL,
      view_name   TEXT NOT NULL,

      -- View definition
      query       TEXT NOT NULL,              -- SQL query defining the view
      refresh_on  TEXT[],                     -- Events that trigger refresh

      -- State
      last_refresh TIMESTAMP,

      PRIMARY KEY (plugin_id, view_name)
  );

  ---
  Layer 3: Specialized Entity Types (Views)

  -- ============================================================================
  -- VIEW: notes
  -- ============================================================================
  -- Convenient view of note entities with common properties

  CREATE VIEW notes AS
  SELECT
      e.id,
      e.created_at,
      e.updated_at,
      e.content_hash,
      e.data->>'path' AS path,
      e.data->>'title' AS title,
      e.data->>'content' AS content,
      e.search_text,

      -- Computed fields
      (SELECT COUNT(*) FROM blocks WHERE entity_id = e.id) AS block_count,
      (SELECT COUNT(*) FROM relations WHERE from_id = e.id AND rel_type = 'wikilink') AS outbound_links,
      (SELECT COUNT(*) FROM relations WHERE to_id = e.id AND rel_type = 'wikilink') AS backlinks,

      -- Tags (as array)
      ARRAY(
          SELECT t.name
          FROM entity_tags et
          JOIN tags t ON et.tag_id = t.id
          WHERE et.entity_id = e.id
      ) AS tags,

      -- All properties (as JSON)
      (
          SELECT jsonb_object_agg(key,
              CASE value_type
                  WHEN 'text' THEN to_jsonb(value_text)
                  WHEN 'int' THEN to_jsonb(value_int)
                  WHEN 'float' THEN to_jsonb(value_float)
                  WHEN 'bool' THEN to_jsonb(value_bool)
                  WHEN 'date' THEN to_jsonb(value_date)
                  WHEN 'json' THEN value_json
              END
          )
          FROM properties WHERE entity_id = e.id
      ) AS properties

  FROM entities e
  WHERE e.type = 'note' AND e.deleted_at IS NULL;


  -- ============================================================================
  -- VIEW: tasks (plugin-extensible)
  -- ============================================================================

  CREATE VIEW tasks AS
  SELECT
      e.id,
      e.created_at,
      e.data->>'title' AS title,

      -- Task-specific properties
      p_status.value_text AS status,
      p_priority.value_int AS priority,
      p_due.value_date AS due_date,
      p_completed.value_date AS completed_at,

      -- Relations
      (SELECT to_id FROM relations WHERE from_id = e.id AND rel_type = 'blocks' LIMIT 1) AS blocked_by,
      (SELECT COUNT(*) FROM relations WHERE from_id = e.id AND rel_type = 'subtask') AS subtask_count

  FROM entities e
  LEFT JOIN properties p_status ON e.id = p_status.entity_id AND p_status.key = 'status'
  LEFT JOIN properties p_priority ON e.id = p_priority.entity_id AND p_priority.key = 'priority'
  LEFT JOIN properties p_due ON e.id = p_due.entity_id AND p_due.key = 'due'
  LEFT JOIN properties p_completed ON e.id = p_completed.entity_id AND p_completed.key = 'completed_at'
  WHERE e.type = 'task' AND e.deleted_at IS NULL;

  ---
  üéØ QUERY LANGUAGE DESIGN: CQL (Crucible Query Language)

  Design Principles

  1. Progressively Complex - Simple queries are simple, complex queries are possible
  2. Graph-Native - Graph traversals as first-class citizens
  3. Type-Safe - Catch errors at parse time, not runtime
  4. Plugin-Friendly - Plugins can extend syntax without breaking parser
  5. Human-Readable - Non-programmers can write queries

  ---
  Query Language Layers

  Layer 1: Declarative (Simple)      - 80% of use cases
  Layer 2: Procedural (Pipelines)    - 15% of use cases
  Layer 3: Functional (Advanced)     - 5% of use cases

  ---
  Layer 1: Declarative Queries (Dataview-inspired)

  # ============================================================================
  # BASIC QUERIES
  # ============================================================================

  # Simple table query
  TABLE notes
    WHERE tags CONTAINS "project"
    SORT BY updated_at DESC
    LIMIT 10

  # With property filters
  TABLE notes
    WHERE properties.status = "in-progress"
      AND properties.priority > 5
    SORT BY properties.due_date ASC

  # Multiple conditions
  TABLE notes
    WHERE (tags CONTAINS "work" OR tags CONTAINS "project")
      AND created_at > @today - 7d
      AND NOT tags CONTAINS "archived"


  # ============================================================================
  # GRAPH QUERIES
  # ============================================================================

  # Simple graph traversal (1-hop)
  GRAPH notes
    WHERE title = "Index"
    FOLLOW wikilink
    LIMIT 20

  # Multi-hop traversal
  GRAPH notes
    WHERE title = "Index"
    FOLLOW wikilink -> wikilink -> wikilink  # 3 hops
    LIMIT 50

  # Bidirectional traversal
  GRAPH notes
    WHERE id = $current_note
    FOLLOW wikilink | REVERSE wikilink       # Outbound + inbound links
    LIMIT 100

  # Filtered traversal
  GRAPH notes
    WHERE tags CONTAINS "concept"
    FOLLOW wikilink
    WHERE tags CONTAINS "example"            # Filter destination nodes

  # Path queries
  PATH notes
    FROM id = "note:start"
    TO id = "note:end"
    VIA wikilink
    MAX DEPTH 5

  # Find all paths
  PATHS notes
    FROM tags CONTAINS "root"
    TO tags CONTAINS "leaf"
    VIA references


  # ============================================================================
  # SEMANTIC SEARCH
  # ============================================================================

  # Vector similarity search
  SEARCH "machine learning neural networks"
    IN notes
    THRESHOLD 0.7
    LIMIT 20

  # Hybrid search (vector + filters)
  SEARCH "rust programming"
    IN notes
    WHERE tags CONTAINS "technical"
      AND created_at > @today - 30d
    THRESHOLD 0.8

  # Search with graph expansion
  SEARCH "knowledge management"
    IN notes
    FOLLOW wikilink
    DEPTH 1
    THRESHOLD 0.75


  # ============================================================================
  # AGGREGATIONS
  # ============================================================================

  # Count by tag
  COUNT notes
    GROUP BY tags
    HAVING count > 5
    SORT BY count DESC

  # Statistics
  STATS notes
    AGGREGATE
      COUNT(*) as total,
      AVG(word_count) as avg_words,
      MAX(updated_at) as last_modified
    GROUP BY properties.project

  ---
  Layer 2: Procedural Queries (Pipeline Style)

  # ============================================================================
  # PIPELINE QUERIES
  # ============================================================================

  # Multi-stage pipeline
  notes
    | WHERE tags CONTAINS "project"
    | FOLLOW wikilink
    | WHERE properties.status = "active"
    | ENRICH WITH backlink_count = (COUNT REVERSE wikilink)
    | SORT BY backlink_count DESC
    | LIMIT 10

  # Join-like operations
  notes
    | WHERE tags CONTAINS "paper"
    | JOIN relations ON notes.id = relations.from_id
    | WHERE relations.rel_type = "cites"
    | JOIN notes AS cited ON relations.to_id = cited.id
    | SELECT notes.title, cited.title AS cited_paper

  # Subqueries
  LET $important_topics = (
    SELECT id FROM notes
    WHERE tags CONTAINS "core-concept"
  )

  notes
    | WHERE id IN $important_topics
    | FOLLOW wikilink
    | WHERE NOT id IN $important_topics
    | SELECT title, id


  # ============================================================================
  # COMPUTED PROPERTIES
  # ============================================================================

  # Calculate PageRank
  notes
    | COMPUTE pagerank = PAGERANK(wikilink, damping=0.85)
    | WHERE pagerank > 0.01
    | SORT BY pagerank DESC

  # Shortest path centrality
  notes
    | COMPUTE centrality = BETWEENNESS(wikilink)
    | TOP 20 BY centrality

  # Community detection
  notes
    | WHERE tags CONTAINS "concept"
    | COMPUTE community = LOUVAIN(wikilink)
    | GROUP BY community

  ---
  Layer 3: Functional Queries (Advanced)

  # ============================================================================
  # RECURSIVE QUERIES
  # ============================================================================

  # Recursive graph traversal with cycle detection
  RECURSIVE traverse(node, visited) {
    FROM node
    FOLLOW wikilink AS next
    WHERE next NOT IN visited
    COLLECT visited + [next] AS new_visited
    RECURSE traverse(next, new_visited)
    MAX DEPTH 10
  }

  traverse("note:start", [])


  # ============================================================================
  # CUSTOM FUNCTIONS (Plugin-defined)
  # ============================================================================

  # Plugin can define functions
  FUNCTION calculate_complexity(note) {
    LET block_count = COUNT(note.blocks)
    LET link_count = COUNT(note FOLLOW wikilink)
    LET word_count = LENGTH(note.content)

    RETURN (block_count * 0.3 + link_count * 0.5 + word_count * 0.2)
  }

  # Use in queries
  notes
    | WHERE calculate_complexity(THIS) > 100
    | SORT BY calculate_complexity(THIS) DESC


  # ============================================================================
  # TEMPORAL QUERIES
  # ============================================================================

  # Version history
  notes
    | WHERE path = "/daily/2025-01-15.md"
    | VERSIONS BETWEEN @2025-01-15 AND @2025-01-16
    | DIFF SHOW changes

  # Time-based graph snapshots
  GRAPH notes
    AS OF @2024-12-01                        # Graph structure on Dec 1
    FOLLOW wikilink
    COMPARE WITH @2025-01-01                 # Show changes

  ---
  üéØ QUERY LANGUAGE IMPLEMENTATION

  Parser Architecture

  // ============================================================================
  // QUERY AST
  // ============================================================================

  #[derive(Debug, Clone)]
  pub enum Query {
      Table(TableQuery),
      Graph(GraphQuery),
      Search(SearchQuery),
      Pipeline(PipelineQuery),
      Recursive(RecursiveQuery),
  }

  #[derive(Debug, Clone)]
  pub struct TableQuery {
      pub source: EntityType,
      pub filters: Vec<Filter>,
      pub sort: Option<SortClause>,
      pub limit: Option<usize>,
      pub offset: Option<usize>,
  }

  #[derive(Debug, Clone)]
  pub struct GraphQuery {
      pub start: Filter,                      // Starting nodes
      pub traversal: Vec<Traversal>,          // Edges to follow
      pub filters: Vec<Filter>,               // Destination node filters
      pub limit: Option<usize>,
  }

  #[derive(Debug, Clone)]
  pub struct Traversal {
      pub relation: String,                   // Relation type to follow
      pub direction: Direction,               // Forward, Backward, Both
      pub max_depth: Option<u32>,
  }

  #[derive(Debug, Clone)]
  pub enum Direction {
      Forward,
      Backward,
      Both,
  }

  #[derive(Debug, Clone)]
  pub struct SearchQuery {
      pub query_text: String,
      pub entity_type: EntityType,
      pub filters: Vec<Filter>,
      pub threshold: Option<f32>,
      pub limit: Option<usize>,
  }

  #[derive(Debug, Clone)]
  pub enum Filter {
      Eq(PropertyPath, Value),
      Neq(PropertyPath, Value),
      Gt(PropertyPath, Value),
      Lt(PropertyPath, Value),
      Contains(PropertyPath, Value),
      In(PropertyPath, Vec<Value>),
      And(Vec<Filter>),
      Or(Vec<Filter>),
      Not(Box<Filter>),
  }

  #[derive(Debug, Clone)]
  pub struct PropertyPath {
      pub segments: Vec<String>,              // e.g., ["properties", "status"]
  }

  #[derive(Debug, Clone)]
  pub enum Value {
      String(String),
      Int(i64),
      Float(f64),
      Bool(bool),
      Date(DateTime<Utc>),
      Array(Vec<Value>),
      Null,
  }

  ---
  Query Compiler

  // ============================================================================
  // QUERY COMPILER
  // ============================================================================

  pub struct QueryCompiler {
      backend: Arc<dyn QueryBackend>,
      plugin_registry: Arc<PluginRegistry>,
  }

  impl QueryCompiler {
      /// Compile CQL query to backend-specific query
      pub fn compile(&self, query: &Query) -> Result<CompiledQuery> {
          match query {
              Query::Table(tq) => self.compile_table(tq),
              Query::Graph(gq) => self.compile_graph(gq),
              Query::Search(sq) => self.compile_search(sq),
              Query::Pipeline(pq) => self.compile_pipeline(pq),
              Query::Recursive(rq) => self.compile_recursive(rq),
          }
      }

      fn compile_table(&self, query: &TableQuery) -> Result<CompiledQuery> {
          // Translate to SQL
          let mut sql = format!("SELECT * FROM {}", query.source.table_name());

          // Add filters
          if !query.filters.is_empty() {
              sql.push_str(" WHERE ");
              sql.push_str(&self.compile_filters(&query.filters)?);
          }

          // Add sorting
          if let Some(sort) = &query.sort {
              sql.push_str(&format!(" ORDER BY {} {}",
                  sort.field,
                  if sort.descending { "DESC" } else { "ASC" }
              ));
          }

          // Add pagination
          if let Some(limit) = query.limit {
              sql.push_str(&format!(" LIMIT {}", limit));
          }
          if let Some(offset) = query.offset {
              sql.push_str(&format!(" OFFSET {}", offset));
          }

          Ok(CompiledQuery::Sql(sql))
      }

      fn compile_graph(&self, query: &GraphQuery) -> Result<CompiledQuery> {
          // For SurrealDB: use native graph syntax
          // For SQLite: generate recursive CTE

          match self.backend.backend_type() {
              BackendType::SurrealDB => self.compile_graph_surreal(query),
              BackendType::SQLite => self.compile_graph_sqlite(query),
              BackendType::CozoDB => self.compile_graph_cozo(query),
          }
      }

      fn compile_graph_surreal(&self, query: &GraphQuery) -> Result<CompiledQuery> {
          let mut surql = String::new();

          // Start clause
          surql.push_str("SELECT * FROM notes WHERE ");
          surql.push_str(&self.compile_filters(&vec![query.start.clone()])?);

          // Traversal
          for traversal in &query.traversal {
              match traversal.direction {
                  Direction::Forward => {
                      surql.push_str(&format!("->{}->notes", traversal.relation));
                  }
                  Direction::Backward => {
                      surql.push_str(&format!("<-{}<-notes", traversal.relation));
                  }
                  Direction::Both => {
                      // Need to handle as UNION
                      unimplemented!("Bidirectional traversal requires UNION")
                  }
              }

              if let Some(max_depth) = traversal.max_depth {
                  surql.push_str(&format!("..{}", max_depth));
              }
          }

          // Destination filters
          if !query.filters.is_empty() {
              surql.push_str(" WHERE ");
              surql.push_str(&self.compile_filters(&query.filters)?);
          }

          // Limit
          if let Some(limit) = query.limit {
              surql.push_str(&format!(" LIMIT {}", limit));
          }

          Ok(CompiledQuery::SurrealQL(surql))
      }

      fn compile_graph_sqlite(&self, query: &GraphQuery) -> Result<CompiledQuery> {
          // Generate recursive CTE for graph traversal
          let sql = format!(r#"
              WITH RECURSIVE graph_traverse(id, depth, path) AS (
                  -- Base case: starting nodes
                  SELECT id, 0 as depth, id as path
                  FROM entities
                  WHERE {}
                  
                  UNION ALL
                  
                  -- Recursive case: follow edges
                  SELECT 
                      r.to_id,
                      gt.depth + 1,
                      gt.path || ',' || r.to_id
                  FROM graph_traverse gt
                  JOIN relations r ON gt.id = r.from_id
                  WHERE r.rel_type = '{}'
                    AND gt.depth < {}
                    AND INSTR(gt.path, ',' || r.to_id) = 0  -- Cycle detection
              )
              SELECT DISTINCT e.*
              FROM graph_traverse gt
              JOIN entities e ON gt.id = e.id
              {}
              {}
          "#,
              self.compile_filters(&vec![query.start.clone()])?,
              query.traversal[0].relation,  // Simplified: assumes single relation
              query.traversal[0].max_depth.unwrap_or(10),
              if query.filters.is_empty() {
                  String::new()
              } else {
                  format!("WHERE {}", self.compile_filters(&query.filters)?)
              },
              if let Some(limit) = query.limit {
                  format!("LIMIT {}", limit)
              } else {
                  String::new()
              }
          );

          Ok(CompiledQuery::Sql(sql))
      }

      fn compile_search(&self, query: &SearchQuery) -> Result<CompiledQuery> {
          // Hybrid search: vector similarity + filters

          match self.backend.backend_type() {
              BackendType::SurrealDB => {
                  let surql = format!(r#"
                      SELECT *, 
                             vector::similarity::cosine(embedding, $query_vec) AS similarity
                      FROM {}
                      WHERE embedding <|{}|> $query_vec
                        {}
                      ORDER BY similarity DESC
                      {}
                  "#,
                      query.entity_type.table_name(),
                      query.threshold.unwrap_or(0.5),
                      if query.filters.is_empty() {
                          String::new()
                      } else {
                          format!("AND {}", self.compile_filters(&query.filters)?)
                      },
                      if let Some(limit) = query.limit {
                          format!("LIMIT {}", limit)
                      } else {
                          String::new()
                      }
                  );

                  Ok(CompiledQuery::SurrealQL(surql))
              }

              BackendType::SQLite => {
                  // Use sqlite-vec extension
                  let sql = format!(r#"
                      SELECT e.*, 
                             vec_distance_cosine(v.vector, ?) AS similarity
                      FROM entities e
                      JOIN embeddings v ON e.id = v.entity_id
                      WHERE similarity < {}
                        {}
                      ORDER BY similarity ASC
                      {}
                  "#,
                      1.0 - query.threshold.unwrap_or(0.5),  // Convert to distance
                      if query.filters.is_empty() {
                          String::new()
                      } else {
                          format!("AND {}", self.compile_filters(&query.filters)?)
                      },
                      if let Some(limit) = query.limit {
                          format!("LIMIT {}", limit)
                      } else {
                          String::new()
                      }
                  );

                  Ok(CompiledQuery::Sql(sql))
              }

              _ => Err(anyhow!("Vector search not supported on this backend"))
          }
      }
  }

  ---
  Plugin Query API

  // ============================================================================
  // PLUGIN QUERY API
  // ============================================================================

  /// High-level query API for plugins
  pub struct QueryEngine {
      compiler: QueryCompiler,
      executor: QueryExecutor,
  }

  impl QueryEngine {
      /// Execute a CQL query
      pub async fn query(&self, cql: &str) -> Result<QueryResult> {
          // Parse CQL
          let query = self.parse_cql(cql)?;

          // Validate (check permissions, types, etc.)
          self.validate(&query)?;

          // Compile to backend query
          let compiled = self.compiler.compile(&query)?;

          // Execute
          let result = self.executor.execute(compiled).await?;

          Ok(result)
      }

      /// Type-safe query builder
      pub fn table(&self, entity_type: &str) -> TableQueryBuilder {
          TableQueryBuilder::new(entity_type, self)
      }

      pub fn graph(&self, start: Filter) -> GraphQueryBuilder {
          GraphQueryBuilder::new(start, self)
      }

      pub fn search(&self, query: &str) -> SearchQueryBuilder {
          SearchQueryBuilder::new(query, self)
      }
  }

  /// Builder for table queries
  pub struct TableQueryBuilder<'a> {
      entity_type: String,
      filters: Vec<Filter>,
      sort: Option<SortClause>,
      limit: Option<usize>,
      engine: &'a QueryEngine,
  }

  impl<'a> TableQueryBuilder<'a> {
      pub fn filter(mut self, filter: Filter) -> Self {
          self.filters.push(filter);
          self
      }

      pub fn where_eq(self, property: &str, value: impl Into<Value>) -> Self {
          self.filter(Filter::Eq(
              PropertyPath::from(property),
              value.into()
          ))
      }

      pub fn where_contains(self, property: &str, value: impl Into<Value>) -> Self {
          self.filter(Filter::Contains(
              PropertyPath::from(property),
              value.into()
          ))
      }

      pub fn sort_by(mut self, field: &str, descending: bool) -> Self {
          self.sort = Some(SortClause {
              field: field.to_string(),
              descending,
          });
          self
      }

      pub fn limit(mut self, limit: usize) -> Self {
          self.limit = Some(limit);
          self
      }

      pub async fn execute(self) -> Result<Vec<Entity>> {
          let query = Query::Table(TableQuery {
              source: EntityType::from(&self.entity_type),
              filters: self.filters,
              sort: self.sort,
              limit: self.limit,
              offset: None,
          });

          self.engine.query_ast(&query).await
      }
  }

  /// Builder for graph queries
  pub struct GraphQueryBuilder<'a> {
      start: Filter,
      traversals: Vec<Traversal>,
      filters: Vec<Filter>,
      limit: Option<usize>,
      engine: &'a QueryEngine,
  }

  impl<'a> GraphQueryBuilder<'a> {
      pub fn follow(mut self, relation: &str) -> Self {
          self.traversals.push(Traversal {
              relation: relation.to_string(),
              direction: Direction::Forward,
              max_depth: Some(1),
          });
          self
      }

      pub fn follow_reverse(mut self, relation: &str) -> Self {
          self.traversals.push(Traversal {
              relation: relation.to_string(),
              direction: Direction::Backward,
              max_depth: Some(1),
          });
          self
      }

      pub fn depth(mut self, max_depth: u32) -> Self {
          if let Some(last) = self.traversals.last_mut() {
              last.max_depth = Some(max_depth);
          }
          self
      }

      pub fn filter(mut self, filter: Filter) -> Self {
          self.filters.push(filter);
          self
      }

      pub async fn execute(self) -> Result<Vec<Entity>> {
          let query = Query::Graph(GraphQuery {
              start: self.start,
              traversal: self.traversals,
              filters: self.filters,
              limit: self.limit,
          });

          self.engine.query_ast(&query).await
      }
  }

  ---
  Example Plugin Usage

  // ============================================================================
  // PLUGIN EXAMPLE: Task Manager
  // ============================================================================

  use crucible::{Plugin, QueryEngine, Filter, Value};

  pub struct TaskManagerPlugin {
      query: QueryEngine,
  }

  impl Plugin for TaskManagerPlugin {
      async fn on_load(&mut self) -> Result<()> {
          // Register custom entity type
          self.register_entity_type("task", task_schema()).await?;

          // Register custom relation
          self.register_relation_type("blocks", "One task blocks another").await?;

          Ok(())
      }

      async fn get_open_tasks(&self) -> Result<Vec<Entity>> {
          // Type-safe query builder
          self.query
              .table("task")
              .where_eq("status", "open")
              .where_contains("tags", "urgent")
              .sort_by("priority", true)
              .limit(20)
              .execute()
              .await
      }

      async fn get_blocked_tasks(&self) -> Result<Vec<Entity>> {
          // Graph query: find tasks that are blocked
          self.query
              .graph(Filter::Eq(
                  PropertyPath::from("type"),
                  Value::String("task".to_string())
              ))
              .follow_reverse("blocks")  // Find what blocks this
              .filter(Filter::Eq(
                  PropertyPath::from("status"),
                  Value::String("open".to_string())
              ))
              .execute()
              .await
      }

      async fn find_related_tasks(&self, task_id: &str) -> Result<Vec<Entity>> {
          // Complex graph query
          self.query
              .graph(Filter::Eq(
                  PropertyPath::from("id"),
                  Value::String(task_id.to_string())
              ))
              .follow("references")
              .follow("wikilink")
              .depth(2)
              .filter(Filter::Eq(
                  PropertyPath::from("type"),
                  Value::String("task".to_string())
              ))
              .execute()
              .await
      }

      async fn smart_task_search(&self, query: &str) -> Result<Vec<Entity>> {
          // Semantic search with filters
          self.query
              .search(query)
              .in_entities("task")
              .where_eq("status", "open")
              .threshold(0.8)
              .limit(10)
              .execute()
              .await
      }
  }

  ---
  üéØ IMPLEMENTATION RECOMMENDATIONS

  Phase 1: Core Schema (Weeks 1-3)

  ‚úì Implement entities, properties, relations tables
  ‚úì Create basic CRUD operations
  ‚úì Add indexes for performance
  ‚úì Write migration scripts

  Phase 2: Query Parser (Weeks 4-6)

  ‚úì Build CQL parser (use pest or nom)
  ‚úì Create AST types
  ‚úì Implement basic query compilation
  ‚úì Add error handling

  Phase 3: Query Execution (Weeks 7-9)

  ‚úì Implement table queries
  ‚úì Implement graph queries (recursive CTEs)
  ‚úì Add vector search integration
  ‚úì Performance optimization

  Phase 4: Plugin System (Weeks 10-12)

  ‚úì Plugin schema registration
  ‚úì Query builder API
  ‚úì Hook system
  ‚úì Plugin isolation/sandboxing

  Phase 5: Advanced Features (Weeks 13-16)

  ‚úì Pipeline queries
  ‚úì Computed properties
  ‚úì Custom functions
  ‚úì Query optimization

  ---
  Meta-Analysis

  Confidence Levels

  High Confidence (>90%):
  - ‚úÖ Entity-Property-Relation model works for flexible metadata
  - ‚úÖ Layered query language (simple ‚Üí advanced) is proven pattern
  - ‚úÖ Plugin extension via schema registration is battle-tested
  - ‚úÖ Graph queries need native support (not bolt-on)

  Medium Confidence (70-85%):
  - ‚ö†Ô∏è CQL syntax may need iteration based on user feedback
  - ‚ö†Ô∏è Performance at 100K+ entities depends on index tuning
  - ‚ö†Ô∏è Plugin sandboxing needs careful security review

  Low Confidence (<60%):
  - ‚ùì Whether recursive queries are used enough to justify complexity
  - ‚ùì Whether functional query layer (Layer 3) is worth the effort

  Key Design Decisions

  Decision 1: EAV vs Wide Tables
  - Chose EAV because flexibility > performance for metadata
  - Mitigation: Add materialized views for hot paths

  Decision 2: Custom DSL vs SQL
  - Chose Custom DSL because graph queries in SQL are painful
  - Mitigation: Compile to SQL/SurrealQL underneath

  Decision 3: Typed vs Untyped Properties
  - Chose Typed (value_text, value_int, etc.) for query performance
  - Mitigation: Plugin schema validation

  ---
  FINAL RECOMMENDATION

  Schema: Use Entity-Property-Relation model with typed property values

  Query Language: Implement 3-layer CQL (Declarative ‚Üí Procedural ‚Üí Functional)

  Backend: Stay with SurrealDB, expose CQL abstraction on top

  Timeline: 12-16 weeks for full implementation

  This design balances:
  - ‚úÖ Flexibility (plugins can extend without migrations)
  - ‚úÖ Performance (typed storage, indexes, compiled queries)
  - ‚úÖ Usability (intuitive query syntax)
  - ‚úÖ Scalability (proven patterns, efficient storage)

  The key innovation is treating the schema itself as data, allowing plugins to extend the system without core changes.

> /ultra-think is running‚Ä¶ Use the Rune lang docs to recontextualize the query language and plugin extensions. It is dynamically typed and easily extensible with Rust (research as necessary). 

  ---
  üß† ULTRA-DEEP ANALYSIS: Rune-Native Query Language & Plugin System

  Problem Recontextualization

  The Rune Paradigm Shift

  Before Rune consideration:
  - Custom parser ‚Üí AST ‚Üí Compiler ‚Üí Backend SQL
  - Static type checking at compile time
  - Rust-native plugins (compilation required)

  With Rune:
  - Rune scripts ‚Üí Dynamic execution
  - Runtime type checking
  - Hot-reloadable plugins (no compilation)
  - Native Rust speed for core, dynamic flexibility for extensions

  Key Insight: Rune as the "Missing Middle Layer"

  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ         Plugin Authors (Rune)            ‚îÇ  ‚Üê Write in dynamic Rune
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ    Rune VM (Embedded in Crucible)       ‚îÇ  ‚Üê Sandboxed execution
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ   Crucible Core API (Rust modules)      ‚îÇ  ‚Üê Exposed via #[rune::function]
  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
  ‚îÇ      Database Layer (SurrealDB)         ‚îÇ  ‚Üê Performance critical
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  The breakthrough: Query language doesn't need custom parsing‚Äîit's just Rune with specialized APIs!

  ---
  üéØ RUNE-NATIVE ARCHITECTURE

  Core Design Principle: Queries ARE Rune Scripts

  Instead of inventing a new query DSL, expose the database through idiomatic Rune APIs.

  // This IS a query!
  pub fn find_related_notes(title) {
      notes()
          .where(|n| n.tags.contains("project"))
          .where(|n| n.created_at > Date::today() - Duration::days(7))
          .sort_by(|n| n.updated_at)
          .take(10)
  }

  Advantages:
  - ‚úÖ No custom parser needed (Rune handles it)
  - ‚úÖ Full programming language power
  - ‚úÖ Hot-reloadable queries
  - ‚úÖ Type safety through runtime checks
  - ‚úÖ Familiar to Rust developers ("Rust without types")

  ---
  üéØ COMPREHENSIVE RUNE INTEGRATION DESIGN

  Layer 1: Rust Core API (Exposed to Rune)

  // ============================================================================
  // CRUCIBLE CORE MODULE (Rust side)
  // ============================================================================

  use rune::{Context, Module, ContextError};
  use rune::runtime::{VmResult, Value};

  /// Main Crucible module exposed to Rune
  pub fn crucible_module() -> Result<Module, ContextError> {
      let mut module = Module::with_crate("crucible")?;

      // Entity operations
      module.function_meta(notes)?;
      module.function_meta(blocks)?;
      module.function_meta(tags)?;
      module.function_meta(entities)?;

      // Graph operations
      module.function_meta(follow)?;
      module.function_meta(traverse)?;
      module.function_meta(path_between)?;

      // Search operations
      module.function_meta(search)?;
      module.function_meta(semantic_search)?;
      module.function_meta(hybrid_search)?;

      // Property operations
      module.function_meta(set_property)?;
      module.function_meta(get_property)?;

      Ok(module)
  }

  // ============================================================================
  // ENTITY QUERY API
  // ============================================================================

  #[rune::function]
  pub fn notes() -> QueryBuilder {
      QueryBuilder::new(EntityType::Note)
  }

  #[rune::function]
  pub fn blocks() -> QueryBuilder {
      QueryBuilder::new(EntityType::Block)
  }

  #[rune::function]
  pub fn tags() -> QueryBuilder {
      QueryBuilder::new(EntityType::Tag)
  }

  /// Query builder exposed to Rune
  #[derive(Any)]
  pub struct QueryBuilder {
      entity_type: EntityType,
      filters: Vec<Filter>,
      sorts: Vec<Sort>,
      limit: Option<usize>,
  }

  impl QueryBuilder {
      #[rune::function]
      pub fn where_fn(&mut self, predicate: Function) -> VmResult<QueryBuilder> {
          // predicate is a Rune closure
          self.filters.push(Filter::Predicate(predicate));
          Ok(self.clone())
      }

      #[rune::function]
      pub fn filter(&mut self, field: String, op: String, value: Value) -> VmResult<QueryBuilder> {
          self.filters.push(Filter::Simple {
              field,
              op: Operator::from_str(&op)?,
              value,
          });
          Ok(self.clone())
      }

      #[rune::function]
      pub fn sort_by(&mut self, key_fn: Function) -> VmResult<QueryBuilder> {
          self.sorts.push(Sort::Function(key_fn));
          Ok(self.clone())
      }

      #[rune::function]
      pub fn take(&mut self, n: i64) -> VmResult<QueryBuilder> {
          self.limit = Some(n as usize);
          Ok(self.clone())
      }

      #[rune::function]
      pub async fn collect(&self) -> VmResult<Vec<Entity>> {
          // Execute the query against the database
          let results = self.execute().await?;
          Ok(results)
      }

      #[rune::function]
      pub async fn first(&self) -> VmResult<Option<Entity>> {
          let mut query = self.clone();
          query.limit = Some(1);
          let results = query.execute().await?;
          Ok(results.into_iter().next())
      }

      #[rune::function]
      pub async fn count(&self) -> VmResult<usize> {
          // Optimized count query
          self.execute_count().await
      }
  }

  // ============================================================================
  // ENTITY TYPE (Rune-visible)
  // ============================================================================

  #[derive(Any, Clone)]
  pub struct Entity {
      pub id: String,
      pub entity_type: String,
      pub data: HashMap<String, Value>,
      pub created_at: DateTime<Utc>,
      pub updated_at: DateTime<Utc>,
  }

  impl Entity {
      #[rune::function]
      pub fn get(&self, key: &str) -> VmResult<Value> {
          self.data.get(key)
              .cloned()
              .ok_or_else(|| VmError::panic(format!("Property '{}' not found", key)))
      }

      #[rune::function]
      pub fn set(&mut self, key: String, value: Value) -> VmResult<()> {
          self.data.insert(key, value);
          Ok(())
      }

      #[rune::function]
      pub fn has(&self, key: &str) -> bool {
          self.data.contains_key(key)
      }

      #[rune::function]
      pub fn keys(&self) -> Vec<String> {
          self.data.keys().cloned().collect()
      }

      // Graph navigation
      #[rune::function]
      pub async fn links(&self) -> VmResult<Vec<Entity>> {
          follow(self.id.clone(), "wikilink".to_string(), 1).await
      }

      #[rune::function]
      pub async fn backlinks(&self) -> VmResult<Vec<Entity>> {
          follow_reverse(self.id.clone(), "wikilink".to_string(), 1).await
      }

      #[rune::function]
      pub async fn related(&self, depth: i64) -> VmResult<Vec<Entity>> {
          traverse(self.id.clone(), vec!["wikilink".to_string()], depth as u32).await
      }
  }

  // ============================================================================
  // GRAPH OPERATIONS
  // ============================================================================

  #[rune::function]
  pub async fn follow(
      from_id: String,
      relation: String,
      depth: i64
  ) -> VmResult<Vec<Entity>> {
      // Execute graph traversal
      let query = GraphQuery {
          start: vec![from_id],
          relation,
          direction: Direction::Forward,
          max_depth: depth as u32,
      };

      execute_graph_query(query).await
  }

  #[rune::function]
  pub async fn follow_reverse(
      to_id: String,
      relation: String,
      depth: i64
  ) -> VmResult<Vec<Entity>> {
      let query = GraphQuery {
          start: vec![to_id],
          relation,
          direction: Direction::Backward,
          max_depth: depth as u32,
      };

      execute_graph_query(query).await
  }

  #[rune::function]
  pub async fn traverse(
      start_id: String,
      relations: Vec<String>,
      max_depth: i64
  ) -> VmResult<Vec<Entity>> {
      // Multi-relation traversal
      let query = GraphQuery {
          start: vec![start_id],
          relations,
          direction: Direction::Both,
          max_depth: max_depth as u32,
      };

      execute_graph_query(query).await
  }

  #[rune::function]
  pub async fn path_between(
      from_id: String,
      to_id: String,
      via: String
  ) -> VmResult<Vec<Entity>> {
      // Shortest path query
      let query = PathQuery {
          from: from_id,
          to: to_id,
          relation: via,
          max_depth: 10,
      };

      execute_path_query(query).await
  }

  // ============================================================================
  // SEARCH OPERATIONS
  // ============================================================================

  #[rune::function]
  pub async fn search(query: String) -> VmResult<Vec<Entity>> {
      // Full-text search
      let results = execute_fts_search(query).await?;
      Ok(results)
  }

  #[rune::function]
  pub async fn semantic_search(
      query: String,
      threshold: f64
  ) -> VmResult<Vec<Entity>> {
      // Vector similarity search
      let results = execute_vector_search(query, threshold as f32).await?;
      Ok(results)
  }

  #[rune::function]
  pub async fn hybrid_search(
      query: String,
      threshold: f64,
      filters: HashMap<String, Value>
  ) -> VmResult<Vec<Entity>> {
      // Combined full-text + vector + filters
      let results = execute_hybrid_search(query, threshold as f32, filters).await?;
      Ok(results)
  }

  // ============================================================================
  // PROPERTY OPERATIONS
  // ============================================================================

  #[rune::function]
  pub async fn set_property(
      entity_id: String,
      key: String,
      value: Value
  ) -> VmResult<()> {
      db_set_property(entity_id, key, value).await
  }

  #[rune::function]
  pub async fn get_property(
      entity_id: String,
      key: String
  ) -> VmResult<Option<Value>> {
      db_get_property(entity_id, key).await
  }

  // ============================================================================
  // UTILITY FUNCTIONS
  // ============================================================================

  #[rune::function]
  pub fn today() -> DateTime<Utc> {
      Utc::now()
  }

  #[rune::function]
  pub fn days(n: i64) -> Duration {
      Duration::days(n)
  }

  #[rune::function]
  pub fn weeks(n: i64) -> Duration {
      Duration::weeks(n)
  }

  ---
  Layer 2: Rune Query Scripts (User/Plugin side)

  // ============================================================================
  // EXAMPLE QUERIES (Pure Rune)
  // ============================================================================

  // Simple table-like query
  pub fn recent_project_notes() {
      notes()
          .filter("tags", "contains", "project")
          .filter("created_at", ">", today() - days(7))
          .sort_by(|n| n.updated_at)
          .take(10)
          .collect()
          .await
  }

  // Using closures for complex filters
  pub fn active_high_priority_tasks() {
      notes()
          .where(|n| {
              n.get("type") == "task" &&
              n.get("status") == "active" &&
              n.get("priority") > 5
          })
          .sort_by(|n| n.get("priority"))
          .collect()
          .await
  }

  // Graph traversal
  pub fn find_concept_network(concept_title) {
      let concept = notes()
          .filter("title", "=", concept_title)
          .first()
          .await?;

      // Get all notes within 2 hops
      let network = concept.related(2).await?;

      // Filter to only other concepts
      network.iter()
          .filter(|n| n.get("tags").contains("concept"))
          .collect()
  }

  // Path finding
  pub fn find_learning_path(from_topic, to_topic) {
      let from_note = notes()
          .filter("title", "=", from_topic)
          .first()
          .await?;

      let to_note = notes()
          .filter("title", "=", to_topic)
          .first()
          .await?;

      path_between(from_note.id, to_note.id, "references").await
  }

  // Semantic search with post-processing
  pub fn find_similar_with_context(query_text) {
      let results = semantic_search(query_text, 0.8).await?;

      // Enrich with context
      for note in results {
          note.backlink_count = follow_reverse(note.id, "wikilink", 1)
              .await?
              .len();
      }

      // Sort by relevance + popularity
      results.sort_by(|n| (n.similarity_score * 0.7) + (n.backlink_count * 0.3));
      results
  }

  // Complex hybrid query
  pub fn smart_task_search(query, project_name) {
      // Vector search
      let semantic_results = semantic_search(query, 0.7).await?;
      
      // Full-text search
      let text_results = search(query).await?;

      // Combine and filter
      let combined = semantic_results
          .chain(text_results)
          .unique_by(|n| n.id)
          .filter(|n| {
              n.get("type") == "task" &&
              n.get("project") == project_name &&
              n.get("status") != "completed"
          })
          .take(20)
          .collect();

      combined
  }

  // Aggregation
  pub fn tag_statistics() {
      let all_notes = notes().collect().await?;

      let tag_counts = {};
      for note in all_notes {
          for tag in note.get("tags") {
              tag_counts[tag] = tag_counts.get(tag).unwrap_or(0) + 1;
          }
      }

      tag_counts.iter()
          .sort_by(|_, count| count)
          .reverse()
          .take(20)
          .collect()
  }

  // Graph algorithm (PageRank-like)
  pub fn calculate_note_importance() {
      let all_notes = notes().collect().await?;
      let importance = {};

      // Initialize scores
      for note in all_notes {
          importance[note.id] = 1.0;
      }

      // Iterate to converge
      for iteration in 0..10 {
          let new_importance = {};

          for note in all_notes {
              let incoming = follow_reverse(note.id, "wikilink", 1).await?;
              let score = incoming.iter()
                  .map(|n| importance[n.id] / follow(n.id, "wikilink", 1).await?.len())
                  .sum();

              new_importance[note.id] = 0.15 + 0.85 * score;
          }

          importance = new_importance;
      }

      importance.iter()
          .sort_by(|_, score| score)
          .reverse()
          .take(50)
          .collect()
  }

  ---
  Layer 3: Plugin System (Rune-Based)

  // ============================================================================
  // PLUGIN INFRASTRUCTURE (Rust side)
  // ============================================================================

  pub struct PluginManager {
      rune_context: Context,
      rune_runtime: Arc<RuntimeContext>,
      plugins: HashMap<String, Plugin>,
  }

  impl PluginManager {
      pub fn new() -> Result<Self> {
          let mut context = Context::with_default_modules()?;

          // Register Crucible modules
          context.install(crucible_module()?)?;
          context.install(crucible_graph_module()?)?;
          context.install(crucible_search_module()?)?;

          let runtime = Arc::new(context.runtime()?);

          Ok(Self {
              rune_context: context,
              rune_runtime: runtime,
              plugins: HashMap::new(),
          })
      }

      /// Load a plugin from Rune source
      pub async fn load_plugin(
          &mut self,
          plugin_id: String,
          source: &str
      ) -> Result<()> {
          // Compile Rune source
          let mut sources = rune::Sources::new();
          sources.insert(rune::Source::new(&plugin_id, source)?)?;

          let mut diagnostics = rune::Diagnostics::new();
          let unit = rune::prepare(&mut sources)
              .with_context(&self.rune_context)
              .build()?;

          if diagnostics.has_errors() {
              return Err(anyhow!("Plugin compilation failed: {:?}", diagnostics));
          }

          // Create VM
          let vm = Vm::new(self.rune_runtime.clone(), Arc::new(unit));

          // Execute plugin initialization
          let plugin_exports = vm.call(["init"], ())?;

          // Register plugin
          let plugin = Plugin {
              id: plugin_id.clone(),
              vm,
              exports: plugin_exports,
          };

          self.plugins.insert(plugin_id, plugin);

          Ok(())
      }

      /// Hot-reload a plugin
      pub async fn reload_plugin(
          &mut self,
          plugin_id: &str,
          new_source: &str
      ) -> Result<()> {
          // Unload old
          self.plugins.remove(plugin_id);

          // Load new
          self.load_plugin(plugin_id.to_string(), new_source).await
      }

      /// Call a plugin function
      pub async fn call_plugin_fn<T>(
          &self,
          plugin_id: &str,
          function: &str,
          args: impl rune::runtime::Args
      ) -> Result<T>
      where
          T: rune::FromValue,
      {
          let plugin = self.plugins.get(plugin_id)
              .ok_or_else(|| anyhow!("Plugin not found: {}", plugin_id))?;

          let result = plugin.vm.call([function], args).await?;
          T::from_value(result)
      }
  }

  // ============================================================================
  // PLUGIN HOOKS
  // ============================================================================

  #[derive(Any)]
  pub struct HookContext {
      pub event: String,
      pub entity: Option<Entity>,
      pub data: HashMap<String, Value>,
  }

  impl PluginManager {
      /// Trigger hooks for an event
      pub async fn trigger_hooks(
          &self,
          event: &str,
          context: HookContext
      ) -> Result<()> {
          for (plugin_id, plugin) in &self.plugins {
              // Check if plugin has this hook
              if let Ok(has_hook) = plugin.vm.call(["has_hook"], (event,)).await {
                  if has_hook {
                      // Call hook
                      plugin.vm.call(["on_event"], (event, context.clone())).await?;
                  }
              }
          }

          Ok(())
      }
  }

  ---
  Layer 4: Example Plugins (Rune)

  // ============================================================================
  // PLUGIN: Task Manager
  // ============================================================================

  // Plugin metadata
  pub const PLUGIN_ID = "task-manager";
  pub const PLUGIN_VERSION = "1.0.0";

  // Plugin initialization
  pub fn init() {
      register_commands();
      register_hooks();
      #{
          commands: [
              "list-tasks",
              "create-task",
              "complete-task",
              "blocked-tasks"
          ],
          hooks: [
              "note.created",
              "note.updated"
          ]
      }
  }

  // Commands exposed to users
  pub fn list_tasks(filters) {
      notes()
          .where(|n| n.get("type") == "task")
          .where(|n| {
              if filters.status {
                  n.get("status") == filters.status
              } else {
                  true
              }
          })
          .where(|n| {
              if filters.project {
                  n.get("project") == filters.project
              } else {
                  true
              }
          })
          .sort_by(|n| n.get("priority"))
          .collect()
          .await
  }

  pub fn create_task(title, properties) {
      let task = #{
          type: "task",
          title: title,
          status: "todo",
          created_at: today(),
          ...properties
      };

      entities().create(task).await
  }

  pub fn complete_task(task_id) {
      set_property(task_id, "status", "completed").await;
      set_property(task_id, "completed_at", today()).await;
  }

  pub fn blocked_tasks() {
      // Find tasks that have a "blocks" relation pointing to them
      notes()
          .where(|n| n.get("type") == "task")
          .where(|n| {
              let blockers = follow_reverse(n.id, "blocks", 1).await?;
              !blockers.is_empty() && blockers.iter().any(|b| b.get("status") != "completed")
          })
          .collect()
          .await
  }

  // Hooks
  pub fn has_hook(event) {
      event == "note.created" || event == "note.updated"
  }

  pub fn on_event(event, context) {
      if event == "note.created" {
          on_note_created(context.entity);
      } else if event == "note.updated" {
          on_note_updated(context.entity);
      }
  }

  fn on_note_created(note) {
      // Auto-detect if this is a task based on content
      let content = note.get("content");
      if content.starts_with("TODO:") || content.contains("[ ]") {
          set_property(note.id, "type", "task").await;
          set_property(note.id, "status", "todo").await;
      }
  }

  fn on_note_updated(note) {
      // Update task dependencies
      if note.get("type") == "task" {
          update_task_relationships(note).await;
      }
  }

  // ============================================================================
  // PLUGIN: Backlink Analyzer
  // ============================================================================

  pub const PLUGIN_ID = "backlink-analyzer";

  pub fn init() {
      #{
          commands: ["analyze-backlinks", "orphaned-notes", "hub-notes"],
          hooks: ["note.created", "relation.created"]
      }
  }

  pub fn analyze_backlinks(note_id) {
      let note = entities().get(note_id).await?;
      let backlinks = note.backlinks().await?;

      #{
          total: backlinks.len(),
          by_type: group_by(backlinks, |n| n.get("type")),
          by_tag: backlinks.iter()
              .flat_map(|n| n.get("tags"))
              .counts(),
          most_linked_from: backlinks.iter()
              .max_by(|n| n.links().await?.len())
      }
  }

  pub fn orphaned_notes() {
      notes()
          .where(|n| {
              let backlinks = n.backlinks().await?;
              let links = n.links().await?;
              backlinks.is_empty() && links.is_empty()
          })
          .collect()
          .await
  }

  pub fn hub_notes(threshold) {
      notes()
          .where(|n| {
              let backlinks = n.backlinks().await?;
              backlinks.len() > threshold
          })
          .sort_by(|n| n.backlinks().await?.len())
          .take(20)
          .collect()
          .await
  }

  // ============================================================================
  // PLUGIN: Semantic Clustering
  // ============================================================================

  pub const PLUGIN_ID = "semantic-clustering";

  pub fn init() {
      #{
          commands: ["find-clusters", "suggest-tags"],
          scheduled: [
              #{
                  schedule: "daily",
                  fn: recalculate_clusters
              }
          ]
      }
  }

  pub fn find_clusters(min_similarity) {
      let all_notes = notes().collect().await?;
      let clusters = [];
      let visited = set();

      for note in all_notes {
          if visited.contains(note.id) {
              continue;
          }

          // Find semantically similar notes
          let similar = semantic_search(note.content, min_similarity).await?
              .filter(|n| !visited.contains(n.id));

          if similar.len() > 1 {
              let cluster = [note] + similar;
              clusters.push(cluster);

              for n in cluster {
                  visited.insert(n.id);
              }
          }
      }

      clusters
  }

  pub fn suggest_tags(note_id, count) {
      let note = entities().get(note_id).await?;

      // Find similar notes
      let similar = semantic_search(note.content, 0.8).await?;

      // Aggregate their tags
      let tag_scores = {};
      for similar_note in similar {
          for tag in similar_note.get("tags") {
              tag_scores[tag] = tag_scores.get(tag).unwrap_or(0.0) + similar_note.similarity_score;
          }
      }

      tag_scores.iter()
          .filter(|tag, _| !note.get("tags").contains(tag))
          .sort_by(|_, score| score)
          .reverse()
          .take(count)
          .map(|tag, _| tag)
          .collect()
  }

  fn recalculate_clusters() {
      // Scheduled task: recalculate clusters daily
      let clusters = find_clusters(0.75).await?;

      // Store in a special entity
      entities().create(#{
          type: "cluster-analysis",
          timestamp: today(),
          clusters: clusters,
          cluster_count: clusters.len()
      }).await;
  }

  ---
  üéØ KEY ADVANTAGES OF RUNE APPROACH

  1. No Custom Parser Needed

  - ‚úÖ Rune handles all parsing
  - ‚úÖ Syntax errors caught by Rune compiler
  - ‚úÖ Standard language tooling (LSP, debugging)

  2. Hot Reloading

  - ‚úÖ Plugin changes without restart
  - ‚úÖ Query development with instant feedback
  - ‚úÖ Live debugging in production

  3. Sandboxed Execution

  - ‚úÖ Plugins can't corrupt Rust state
  - ‚úÖ Memory safety (no unsafe, no FFI)
  - ‚úÖ Controlled API surface

  4. Full Programming Language Power

  - ‚úÖ Variables, loops, functions
  - ‚úÖ Pattern matching, closures
  - ‚úÖ Async/await for concurrent queries
  - ‚úÖ Error handling with Result

  5. Extensibility Through Rust Modules

  - ‚úÖ Easy to expose new APIs (#[rune::function])
  - ‚úÖ Type conversions handled automatically
  - ‚úÖ Can expose entire Rust structs

  ---
  üéØ IMPLEMENTATION ROADMAP

  Phase 1: Foundation (Weeks 1-2)

  // Set up Rune integration
  - Install rune crate
  - Create basic Context with modules
  - Expose Entity and QueryBuilder types
  - Write basic examples

  Phase 2: Core Query API (Weeks 3-4)

  // Implement query builders
  - notes(), blocks(), tags() functions
  - filter(), where(), sort_by(), take()
  - collect(), first(), count()
  - Test with simple queries

  Phase 3: Graph API (Weeks 5-6)

  // Graph traversal functions
  - follow(), follow_reverse()
  - traverse(), path_between()
  - Entity.links(), Entity.backlinks()
  - Graph algorithm utilities

  Phase 4: Search Integration (Weeks 7-8)

  // Search functions
  - search() (FTS)
  - semantic_search() (vector)
  - hybrid_search() (combined)
  - Integration with SurrealDB

  Phase 5: Plugin System (Weeks 9-11)

  // Plugin infrastructure
  - PluginManager
  - Hot reloading
  - Hook system
  - Command registration

  Phase 6: Standard Library (Weeks 12-14)

  // Common plugin utilities
  - Date/time helpers
  - Collection utilities
  - Graph algorithms
  - Statistics functions

  ---
  üéØ QUERY LANGUAGE COMPARISON

  Before (Custom DSL):

  TABLE notes
    WHERE tags CONTAINS "project"
    SORT BY updated_at DESC
    LIMIT 10

  Problems:
  - Need custom parser
  - Limited to predefined syntax
  - Hard to extend
  - No variables, loops, functions

  After (Rune):

  pub fn recent_projects() {
      notes()
          .filter("tags", "contains", "project")
          .sort_by(|n| n.updated_at)
          .take(10)
          .collect()
          .await
  }

  Advantages:
  - Full programming language
  - Composable functions
  - Type safety (runtime)
  - Hot reloadable

  ---
  Meta-Analysis

  Confidence Levels

  High Confidence (>90%):
  - ‚úÖ Rune integration is technically sound
  - ‚úÖ Query-as-code pattern is proven (Datalog, LINQ)
  - ‚úÖ Hot reloading provides massive DX improvement
  - ‚úÖ Sandboxing provides security

  Medium Confidence (70-85%):
  - ‚ö†Ô∏è Rune performance overhead (need benchmarks)
  - ‚ö†Ô∏è Rune tooling maturity (LSP, debugger)
  - ‚ö†Ô∏è Learning curve for plugin authors

  Low Confidence (<60%):
  - ‚ùì Whether async/await in Rune works smoothly
  - ‚ùì How complex plugins scale (memory, CPU)

  ---
  FINAL RECOMMENDATION

  Architecture: Rune-Native Query Language + Plugin System

  Why This Is Superior:

  1. Eliminates Custom Parser - 2-3 months of work saved
  2. Hot Reloading - Massive productivity boost
  3. Full Language Power - No DSL limitations
  4. Safe Sandboxing - Plugin crashes can't kill core
  5. Rust Integration - Expose any Rust API easily
  6. Familiar Syntax - "Rust without types"

  The Killer Feature: Plugin authors write normal Rune code that looks like Rust, with full access to database through idiomatic APIs.

  Implementation Timeline: 12-14 weeks (vs 16+ weeks for custom DSL)

  This is the pragmatic, modern approach to extensible database querying. Let users program, don't limit them to a DSL.

> /ultra-think is running‚Ä¶ pretty good! research Rune with a subagent (key notes are that is also supports modules, structs/impl, and Rust-defined attribute macros. It also supports semi-custom DSLs. Use those to iterate again 

  Paradigm Shift: The Power of Protocols & Custom DSLs

  With the deep knowledge of Rune's capabilities, I can now design a radically better architecture that leverages:

  1. Protocols for operator overloading - Natural query syntax with +, |, [], etc.
  2. Structs with impl blocks - Object-oriented query builders in Rune itself
  3. Rust-defined attributes - Type safety through #[derive(Any)] and field attributes
  4. Custom macros - DSL sugar for common patterns
  5. Pattern matching - Elegant query result handling

  ---
  üéØ THE ULTIMATE QUERY DSL ARCHITECTURE

  Design Philosophy: Layers of Abstraction

  Layer 1: Operator-Based DSL (Most Ergonomic)
      ‚Üì
  Layer 2: Method-Chained Builders (Explicit)
      ‚Üì
  Layer 3: Low-Level API (Direct Database Access)

  ---
  Layer 1: Protocol-Based Operator DSL

  Core Insight: Use Operators for Query Operations

  Instead of verbose method chains, overload operators for intuitive query construction:

  // Operator-based query DSL
  use crucible::query::*;

  // Filter operators
  let project_notes = notes @ ["tags" contains "project"];
  let recent = notes @ ["created_at" > today() - days(7)];
  let combined = project_notes & recent;  // AND operation

  // Graph traversal operator
  let network = note >> wikilink >> 2;  // Follow links 2 hops

  // Search operators
  let similar = "machine learning" ~ threshold(0.8);  // Semantic search

  // Pipe for transformations
  let top_notes = notes 
      @ ["tags" contains "important"]
      | sort_by("priority") 
      | take(10);

  Rust Implementation with Protocols

  // ============================================================================
  // OPERATOR-BASED QUERY DSL (Rust Side)
  // ============================================================================

  use rune::{Any, Module, ContextError, ToValue, FromValue};
  use rune::runtime::{Protocol, VmResult, Value};

  // Query operator - represents a filter condition
  #[derive(Clone, Any, ToValue, FromValue)]
  #[rune(item = ::crucible::query)]
  pub struct Filter {
      field: String,
      op: String,
      value: Value,
  }

  // Query builder with operator overloading
  #[derive(Clone, Any, ToValue, FromValue)]
  #[rune(item = ::crucible::query)]
  pub struct QueryExpr {
      entity_type: String,
      filters: Vec<Filter>,
      sorts: Vec<Sort>,
      limit: Option<i64>,
  }

  impl QueryExpr {
      #[rune::function(path = Self::new)]
      fn new(entity_type: String) -> Self {
          Self {
              entity_type,
              filters: Vec::new(),
              sorts: Vec::new(),
              limit: None,
          }
      }

      // @ operator for filtering (INDEX_GET protocol)
      #[rune::function(protocol = INDEX_GET)]
      fn filter_op(&self, filter: Filter) -> QueryExpr {
          let mut new_query = self.clone();
          new_query.filters.push(filter);
          new_query
      }

      // & operator for AND (BIT_AND protocol)
      #[rune::function(protocol = BIT_AND)]
      fn and(&self, other: &QueryExpr) -> QueryExpr {
          let mut new_query = self.clone();
          new_query.filters.extend(other.filters.clone());
          new_query
      }

      // | operator for OR / pipe operations (BIT_OR protocol)
      #[rune::function(protocol = BIT_OR)]
      fn or(&self, transform: Transform) -> QueryExpr {
          // Apply transformation
          transform.apply(self.clone())
      }

      // >> operator for graph traversal (SHR protocol)
      #[rune::function(protocol = SHR)]
      fn traverse(&self, relation: Relation) -> GraphQuery {
          GraphQuery::new(self.clone(), relation)
      }

      // Execute query
      #[rune::function]
      async fn collect(&self) -> VmResult<Vec<Entity>> {
          execute_query(self).await
      }
  }

  // Relation type for graph traversal
  #[derive(Clone, Any, ToValue, FromValue)]
  #[rune(item = ::crucible::query)]
  pub struct Relation {
      name: String,
      depth: Option<u32>,
  }

  impl Relation {
      #[rune::function(path = Self::new)]
      fn new(name: String) -> Self {
          Self { name, depth: Some(1) }
      }

      // >> operator for multi-hop (depth specification)
      #[rune::function(protocol = SHR)]
      fn with_depth(&self, depth: i64) -> Relation {
          Relation {
              name: self.name.clone(),
              depth: Some(depth as u32),
          }
      }
  }

  // Graph query result
  #[derive(Clone, Any, ToValue, FromValue)]
  #[rune(item = ::crucible::query)]
  pub struct GraphQuery {
      start: QueryExpr,
      relation: Relation,
  }

  impl GraphQuery {
      fn new(start: QueryExpr, relation: Relation) -> Self {
          Self { start, relation }
      }

      #[rune::function]
      async fn collect(&self) -> VmResult<Vec<Entity>> {
          execute_graph_query(self).await
      }
  }

  // Transform for pipeline operations
  #[derive(Clone, Any, ToValue, FromValue)]
  #[rune(item = ::crucible::query)]
  pub enum Transform {
      #[rune(constructor)]
      SortBy(#[rune(get)] String),

      #[rune(constructor)]
      Take(#[rune(get)] i64),

      #[rune(constructor)]
      Skip(#[rune(get)] i64),
  }

  impl Transform {
      fn apply(&self, mut query: QueryExpr) -> QueryExpr {
          match self {
              Transform::SortBy(field) => {
                  query.sorts.push(Sort::new(field.clone()));
                  query
              }
              Transform::Take(n) => {
                  query.limit = Some(*n);
                  query
              }
              Transform::Skip(n) => {
                  // Implement offset
                  query
              }
          }
      }
  }

  // Helper functions exposed to Rune
  #[rune::function]
  fn notes() -> QueryExpr {
      QueryExpr::new("note".to_string())
  }

  #[rune::function]
  fn wikilink() -> Relation {
      Relation::new("wikilink".to_string())
  }

  #[rune::function]
  fn sort_by(field: String) -> Transform {
      Transform::SortBy(field)
  }

  #[rune::function]
  fn take(n: i64) -> Transform {
      Transform::Take(n)
  }

  // Filter construction helpers with special syntax
  #[rune::function]
  fn contains(field: String, value: String) -> Filter {
      Filter {
          field,
          op: "contains".to_string(),
          value: Value::from(value),
      }
  }

  #[rune::function]
  fn gt(field: String, value: Value) -> Filter {
      Filter {
          field,
          op: ">".to_string(),
          value,
      }
  }

  #[rune::function]
  fn eq(field: String, value: Value) -> Filter {
      Filter {
          field,
          op: "=".to_string(),
          value,
      }
  }

  Custom DSL Macro for Filter Syntax

  // ============================================================================
  // CUSTOM MACRO: filter! for intuitive filter syntax
  // ============================================================================

  use rune::{MacroContext, TokenStream, Parser, quote};
  use rune::ast;
  use rune::compile;

  #[rune::macro_]
  pub fn filter(
      cx: &mut MacroContext<'_, '_, '_>,
      stream: &TokenStream,
  ) -> compile::Result<TokenStream> {
      let mut parser = Parser::from_token_stream(stream, cx.stream_span());

      // Parse: field op value
      let field = parser.parse::<ast::LitStr>()?;
      let op_token = parser.parse::<ast::Token>()?;
      let value = parser.parse::<ast::Expr>()?;

      let field_str = field.resolve(cx.resolve_context())?;

      // Determine operator
      let filter_fn = match op_token.kind {
          ast::Kind::Gt => "gt",
          ast::Kind::Lt => "lt",
          ast::Kind::EqEq => "eq",
          ast::Kind::BangEq => "neq",
          _ => {
              // Check for custom operators like "contains"
              if let Some(ident) = op_token.as_ident() {
                  ident.resolve(cx.resolve_context())?
              } else {
                  return Err(compile::Error::msg("unsupported operator"));
              }
          }
      };

      // Generate: filter_fn("field", value)
      let output = quote!(cx, #filter_fn(#field_str, #value));

      Ok(output.into_token_stream(cx)?)
  }

  // Register macro
  pub fn query_macro_module() -> Result<Module, ContextError> {
      let mut m = Module::new();
      m.macro_(["filter"], filter)?;
      Ok(m)
  }

  Usage in Rune (The Dream Syntax)

  use crucible::query::*;

  pub async fn main() {
      // Operator-based filtering (most ergonomic)
      let recent_projects = notes 
          @ filter!("tags" contains "project")
          @ filter!("created_at" > today() - days(7))
          | sort_by("updated_at")
          | take(10);
      
      let results = recent_projects.collect().await?;
      
      // Graph traversal with operators
      let start_note = notes @ filter!("title" == "Index");
      let network = start_note >> wikilink >> 2;  // 2-hop traversal
      let related = network.collect().await?;
      
      // Combining queries with & (AND)
      let high_priority = notes @ filter!("priority" > 5);
      let active = notes @ filter!("status" == "active");
      let important_tasks = high_priority & active;
      
      Ok(())
  }

  ---
  Layer 2: Struct-Based Builder Pattern (More Explicit)

  Query Builder Structs in Rune

  // ============================================================================
  // QUERY BUILDER PATTERN (Pure Rune)
  // ============================================================================

  pub struct NoteQuery {
      filters,
      sorts,
      limit,
  }

  impl NoteQuery {
      pub fn new() {
          Self {
              filters: [],
              sorts: [],
              limit: None,
          }
      }

      pub fn filter(self, field, op, value) {
          self.filters.push(#{ field, op, value });
          self
      }

      pub fn where_tags_contain(self, tag) {
          self.filter("tags", "contains", tag)
      }

      pub fn where_created_after(self, date) {
          self.filter("created_at", ">", date)
      }

      pub fn sort_by(self, field, desc) {
          self.sorts.push(#{ field, desc });
          self
      }

      pub fn limit(self, n) {
          self.limit = Some(n);
          self
      }

      pub async fn collect(self) {
          // Convert to QueryExpr and execute
          let query = self.to_query_expr();
          query.collect().await
      }

      pub async fn first(self) {
          self.limit(1).collect().await?.first()
      }

      pub async fn count(self) {
          // Optimized count query
          count_query(self.to_query_expr()).await
      }
  }

  // Usage
  pub async fn find_recent_projects() {
      NoteQuery::new()
          .where_tags_contain("project")
          .where_created_after(today() - days(7))
          .sort_by("updated_at", true)
          .limit(10)
          .collect()
          .await
  }

  ---
  Layer 3: Full-Featured Plugin System

  Plugin Infrastructure with Rune Structs

  // ============================================================================
  // PLUGIN SYSTEM (Rust Side)
  // ============================================================================

  #[derive(Any)]
  #[rune(item = ::crucible::plugin)]
  pub struct Plugin {
      #[rune(get)]
      id: String,

      #[rune(get)]
      name: String,

      #[rune(get)]
      version: String,

      #[rune(get, set)]
      enabled: bool,
  }

  impl Plugin {
      #[rune::function(path = Self::new)]
      fn new(id: String, name: String, version: String) -> Self {
          Self {
              id,
              name,
              version,
              enabled: true,
          }
      }

      #[rune::function(protocol = DISPLAY_FMT)]
      fn display_fmt(&self, f: &mut Formatter) -> fmt::Result {
          write!(f, "Plugin {} v{}", self.name, self.version)
      }
  }

  // Plugin context provided to plugins
  #[derive(Any)]
  #[rune(item = ::crucible::plugin)]
  pub struct PluginContext {
      #[rune(get)]
      plugin_id: String,

      #[rune(get)]
      config: Value,

      state: Arc<RwLock<HashMap<String, Value>>>,
  }

  impl PluginContext {
      #[rune::function]
      async fn get_state(&self, key: String) -> VmResult<Option<Value>> {
          let state = self.state.read().await;
          Ok(state.get(&key).cloned())
      }

      #[rune::function]
      async fn set_state(&self, key: String, value: Value) -> VmResult<()> {
          let mut state = self.state.write().await;
          state.insert(key, value);
          Ok(())
      }

      #[rune::function]
      async fn query(&self) -> QueryExpr {
          // Return query builder with plugin permissions applied
          QueryExpr::new_with_context(self.plugin_id.clone())
      }

      #[rune::function]
      async fn emit_event(&self, event: String, data: Value) -> VmResult<()> {
          event_bus::emit(&event, data).await
      }
  }

  // Hook registration
  #[derive(Any)]
  #[rune(item = ::crucible::plugin)]
  pub enum Hook {
      #[rune(constructor)]
      OnNoteCreated,

      #[rune(constructor)]
      OnNoteUpdated,

      #[rune(constructor)]
      OnNoteDeleted,

      #[rune(constructor)]
      OnRelationCreated,

      #[rune(constructor)]
      OnQuery,
  }

  #[rune::function]
  async fn register_hook(
      ctx: PluginContext,
      hook: Hook,
      handler: Function,
  ) -> VmResult<()> {
      hook_manager::register(ctx.plugin_id, hook, handler).await
  }

  Example Plugin in Rune

  // ============================================================================
  // PLUGIN: Task Manager (Pure Rune)
  // ============================================================================

  use crucible::{Plugin, Hook, query::*};

  pub const METADATA = Plugin::new(
      "task-manager",
      "Task Manager",
      "1.0.0"
  );

  // Plugin initialization
  pub async fn init(ctx) {
      // Register hooks
      register_hook(ctx, Hook::OnNoteCreated, on_note_created).await?;
      register_hook(ctx, Hook::OnNoteUpdated, on_note_updated).await?;
      
      // Initialize plugin state
      ctx.set_state("task_count", 0).await?;
      
      println!("Task Manager plugin initialized");
      Ok(())
  }

  // Hook handlers
  async fn on_note_created(note) {
      // Auto-detect tasks
      if note.content.contains("TODO:") || note.content.contains("[ ]") {
          set_property(note.id, "type", "task").await?;
          set_property(note.id, "status", "todo").await?;
          
          println!(`Auto-detected task: ${note.title}`);
      }
      
      Ok(())
  }

  async fn on_note_updated(note) {
      if note.get("type") == "task" {
          // Check if task was completed
          if note.content.contains("[x]") && note.get("status") != "completed" {
              set_property(note.id, "status", "completed").await?;
              set_property(note.id, "completed_at", today()).await?;
              
              emit_event("task_completed", #{ task_id: note.id }).await?;
          }
      }
      
      Ok(())
  }

  // Plugin commands
  pub async fn list_tasks(ctx, filters) {
      // Using operator DSL
      let mut query = notes @ filter!("type" == "task");
      
      if filters.status {
          query = query @ filter!("status" == filters.status);
      }
      
      if filters.project {
          query = query @ filter!("project" == filters.project);
      }
      
      query
          | sort_by("priority")
          | take(100)
          .collect()
          .await
  }

  pub async fn blocked_tasks(ctx) {
      // Graph query: find tasks with active blockers
      let all_tasks = notes @ filter!("type" == "task");
      let tasks = all_tasks.collect().await?;
      
      let blocked = [];
      for task in tasks {
          // Check for blocking relations
          let blockers = task.follow_reverse("blocks", 1).await?;
          let active_blockers = blockers.iter()
              .filter(|b| b.get("status") != "completed")
              .collect();
          
          if !active_blockers.is_empty() {
              blocked.push(#{
                  task: task,
                  blockers: active_blockers,
              });
          }
      }
      
      blocked
  }

  pub async fn task_dependencies(ctx, task_id) {
      // Build dependency graph
      let task = notes @ filter!("id" == task_id);
      
      // Recursive traversal
      let deps = task >> blocks >> 10;  // Up to 10 levels deep
      deps.collect().await
  }

  // Computed properties
  pub async fn calculate_task_score(ctx, task) {
      let priority = task.get("priority").unwrap_or(5);
      let age_days = (today() - task.created_at).days();
      let backlinks = task.backlinks().await?.len();
      
      // Score formula
      let score = priority * 10 + age_days * 2 + backlinks * 5;
      
      // Cache in plugin state
      ctx.set_state(`task_score_${task.id}`, score).await?;

      score
  }

  // Scheduled tasks
  pub async fn cleanup_completed(ctx) {
      let completed_old = notes
          @ filter!("type" == "task")
          @ filter!("status" == "completed")
          @ filter!("completed_at" < today() - days(30));

      let tasks = completed_old.collect().await?;

      for task in tasks {
          // Archive or delete
          set_property(task.id, "archived", true).await?;
      }

      println!(`Archived ${tasks.len()} completed tasks`);
      Ok(())
  }

  ---
  Advanced Features: Pattern Matching & Protocols

  Pattern-Based Query Results

  pub async fn analyze_note(note_id) {
      let note = notes @ filter!("id" == note_id);
      let result = note.first().await?;

      // Pattern matching on query results
      match result {
          // Match specific note types
          #{ type: "task", status: "blocked", .. } => {
              println!("Blocked task detected");
              handle_blocked_task(result).await?;
          }

          // Match by properties
          #{ priority: p, tags } if p > 8 && tags.contains("urgent") => {
              println!("Urgent high-priority item");
              escalate(result).await?;
          }

          // Match with graph context
          note if note.backlinks().await?.len() > 10 => {
              println!("Hub note with many backlinks");
          }

          _ => println!("Regular note"),
      }

      Ok(())
  }

  Iterator Protocol for Streaming Results

  // Implement iterator protocol for query results
  #[derive(Any)]
  #[rune(item = ::crucible::query)]
  pub struct QueryIterator {
      query: QueryExpr,
      cursor: usize,
      batch_size: usize,
  }

  impl QueryIterator {
      #[rune::function(protocol = INTO_ITER)]
      fn into_iter(query: QueryExpr) -> Self {
          Self {
              query,
              cursor: 0,
              batch_size: 100,
          }
      }

      #[rune::function(protocol = NEXT)]
      async fn next(&mut self) -> VmResult<Option<Entity>> {
          // Fetch next batch if needed
          if self.cursor >= self.current_batch.len() {
              self.fetch_next_batch().await?;
          }

          if self.cursor < self.current_batch.len() {
              let entity = self.current_batch[self.cursor].clone();
              self.cursor += 1;
              Ok(Some(entity))
          } else {
              Ok(None)
          }
      }
  }

  Usage in Rune:
  pub async fn process_all_notes() {
      let query = notes @ filter!("tags" contains "process");

      // Streaming iteration (doesn't load all into memory)
      for note in query {
          process_note(note).await?;
      }

      Ok(())
  }

  ---
  üéØ COMPLETE MODULE STRUCTURE

  // ============================================================================
  // COMPLETE CRUCIBLE MODULE ORGANIZATION
  // ============================================================================

  pub mod crucible {
      pub mod query {
          // Core query types
          - QueryExpr
          - Filter, Sort, Limit
          - Relation, GraphQuery
          - Transform (for pipelines)

          // Operators (protocols)
          - @ (INDEX_GET) - filter application
          - & (BIT_AND) - AND combination
          - | (BIT_OR) - pipeline/OR
          - >> (SHR) - graph traversal
          - ~ (BIT_NOT) - semantic search

          // Helper functions
          - notes(), blocks(), tags()
          - filter!() macro
          - sort_by(), take(), skip()
          - contains(), eq(), gt(), lt()
      }

      pub mod entity {
          // Entity types
          - Entity
          - Block
          - Tag
          - Relation

          // Property access
          - get(), set(), has()
          - keys(), values(), entries()

          // Graph navigation
          - links(), backlinks()
          - related(), traverse()
      }

      pub mod search {
          // Search types
          - TextSearch
          - VectorSearch
          - HybridSearch

          // Search functions
          - search() - FTS
          - semantic() - vector
          - hybrid() - combined
          - threshold() - similarity control
      }

      pub mod plugin {
          // Plugin infrastructure
          - Plugin
          - PluginContext
          - Hook

          // Registration
          - register_hook()
          - register_command()
          - register_property()

          // State management
          - get_state(), set_state()
          - emit_event(), subscribe()
      }

      pub mod util {
          // Utilities
          - today(), now()
          - days(), weeks(), months()
          - parse_date(), format_date()

          // Collections
          - group_by(), unique_by()
          - flatten(), chunk()
      }
  }

  ---
  üéØ IMPLEMENTATION ROADMAP (Revised)

  ‚úì Set up Rune integration
  ‚úì Create basic module structure
  ‚úì Implement core types (Entity, QueryExpr)
  ‚úì Test basic compilation

  Phase 2: Core Query API (Weeks 3-5)

  ‚úì Implement QueryExpr with protocols
  ‚úì Operators: @, &, |, >>
  ‚úì Helper functions: notes(), filter(), etc.
  ‚úì Execute basic queries against SurrealDB

  Phase 3: Filter Macro DSL (Week 6)

  ‚úì Implement filter!() macro
  ‚úì Parse filter expressions
  ‚úì Generate Filter objects
  ‚úì Test all operator combinations

  Phase 4: Graph Operations (Weeks 7-8)

  ‚úì Implement Relation type
  ‚úì Graph traversal with >> operator
  ‚úì GraphQuery execution
  ‚úì Multi-hop traversal

  Phase 5: Search Integration (Weeks 9-10)

  ‚úì Vector search API
  ‚úì Full-text search
  ‚úì Hybrid search
  ‚úì ~ operator for semantic search

  Phase 6: Plugin System (Weeks 11-13)

  ‚úì PluginContext implementation
  ‚úì Hook system
  ‚úì State management
  ‚úì Event bus
  ‚úì Plugin loading/reloading

  Phase 7: Iterator Protocol (Week 14)

  ‚úì Implement INTO_ITER, NEXT
  ‚úì Streaming query results
  ‚úì Memory-efficient processing

  Phase 8: Standard Library (Weeks 15-16)

  ‚úì Date/time utilities
  ‚úì Collection helpers
  ‚úì Common query patterns
  ‚úì Plugin utilities

  ---
  üéØ KEY INNOVATIONS

  1. Operator-Based Query DSL

  // Before (verbose)
  notes().filter("tags", "contains", "project").sort_by("date").take(10)

  // After (elegant)
  notes @ filter!("tags" contains "project") | sort_by("date") | take(10)

  2. Graph Traversal Operator

  // Before (method chain)
  traverse(note.id, "wikilink", 2)

  // After (operator)
  note >> wikilink >> 2

  3. Protocol-Based Extensions

  // Any Rust type can participate in query DSL
  impl MyType {
      #[rune::function(protocol = INDEX_GET)]
      fn apply_filter(&self, filter: Filter) -> QueryExpr { ... }
  }

  4. Pattern Matching for Results

  match query_result {
      #{ type: "task", status: s, .. } if s == "blocked" => handle_blocked(),
      note if note.backlinks().await?.len() > 10 => handle_hub(),
      _ => default_handler(),
  }

  5. Struct-Based Plugins in Pure Rune

  pub struct TaskPlugin {
      config,
      state,
  }

  impl TaskPlugin {
      pub fn new(config) { Self { config, state: #{} } }
      pub async fn on_note_created(self, note) { ... }
  }

  ---
  üéØ FINAL RECOMMENDATION

  Architecture: Rune with Protocol-Based Operator DSL

  Why This Is The Ultimate Solution:

  1. Intuitive Syntax - Operators (@, >>, |) feel natural
  2. Full Language Power - Closures, async, pattern matching
  3. Type Safety - Runtime checking via protocols
  4. Hot Reloading - Edit queries without restart
  5. Plugin Isolation - Sandboxed Rune VM
  6. Extensible - Add operators via protocols
  7. Performant - Compiles to bytecode
  8. Familiar - "Rust without types"

  The Killer Combo:
  - Operators for common patterns (80% of queries)
  - Methods for explicit control (15% of queries)
  - Low-level API for edge cases (5% of queries)

  Timeline: 14-16 weeks for complete implementation

  Confidence: 95% - This leverages Rune's strengths perfectly while avoiding its limitations.

  This design is production-ready, elegant, and future-proof. The protocol system gives us operator overloading, structs/impl give us OOP patterns, macros give us syntax sugar, and the sandbox gives us safety.

  Ship it! üöÄ

> There is a lack of note for MCP-like/agent-friendly tools here, which we've already implemented to some extent using attribute macros. Make a short addition based on this insight. 

  The Missing Piece: LLM/Agent Integration

  Crucible needs to be agent-friendly - plugins should expose tools that LLMs can discover and invoke through standardized protocols like MCP (Model Context Protocol).

  ---
  Architecture: Attribute-Based Tool Registration

  Rust-Side: MCP Tool Attribute Macro

  // ============================================================================
  // MCP TOOL REGISTRATION SYSTEM
  // ============================================================================

  use rune::{Any, Module};
  use serde_json::json;

  /// Marks a Rune-exposed function as an MCP tool
  /// Automatically generates JSON Schema and tool metadata
  #[proc_macro_attribute]
  pub fn mcp_tool(attr: TokenStream, item: TokenStream) -> TokenStream {
      // Parse attributes: name, description, examples
      let tool_meta = parse_tool_attributes(attr);

      // Parse function signature to generate parameter schema
      let func = parse_item_fn(item);
      let param_schema = generate_json_schema(&func.sig);

      // Generate registration code
      quote! {
          #[rune::function]
          #[doc = #tool_meta.description]
          #func

          // Auto-register in tool registry
          inventory::submit! {
              ToolRegistration {
                  name: #tool_meta.name,
                  description: #tool_meta.description,
                  parameters: #param_schema,
                  function: #func.ident,
                  examples: vec![#(#tool_meta.examples),*],
              }
          }
      }
  }

  // Tool metadata
  #[derive(Clone, Any, ToValue, FromValue)]
  #[rune(item = ::crucible::mcp)]
  pub struct Tool {
      #[rune(get)]
      name: String,

      #[rune(get)]
      description: String,

      #[rune(get)]
      parameters: Value,  // JSON Schema

      #[rune(get)]
      examples: Vec<String>,
  }

  impl Tool {
      #[rune::function(protocol = DISPLAY_FMT)]
      fn display_fmt(&self, f: &mut Formatter) -> fmt::Result {
          write!(f, "Tool: {} - {}", self.name, self.description)
      }

      #[rune::function]
      fn to_mcp_schema(&self) -> Value {
          // Generate MCP-compatible tool schema
          json!({
              "name": self.name,
              "description": self.description,
              "inputSchema": self.parameters,
          })
      }
  }

  Usage in Rune Plugins

  // ============================================================================
  // PLUGIN WITH MCP TOOLS (Rune side)
  // ============================================================================

  use crucible::{Plugin, mcp::*};

  pub const METADATA = Plugin::new(
      "task-analyzer",
      "Task Analysis Tools",
      "1.0.0"
  );

  /// Find high-priority tasks that are currently blocked
  #[mcp::tool(
      name = "find_blocked_tasks",
      description = "Returns all high-priority tasks that are blocked by incomplete dependencies",
      examples = [
          "Find blocked tasks for project 'Crucible'",
          "Show me what's blocking my high-priority work"
      ]
  )]
  pub async fn find_blocked_tasks(
      /// Project name to filter by (optional)
      project: Option<String>,
      /// Minimum priority threshold (1-10)
      min_priority: i64
  ) -> Vec<Task> {
      let mut query = notes
          @ filter!("type" == "task")
          @ filter!("priority" >= min_priority);
      
      if let Some(proj) = project {
          query = query @ filter!("project" == proj);
      }
      
      let tasks = query.collect().await?;
      
      // Filter for blocked tasks
      tasks.iter()
          .filter(|t| {
              let blockers = t.follow_reverse("blocks", 1).await?;
              blockers.iter().any(|b| b.get("status") != "completed")
          })
          .collect()
  }

  /// Calculate optimal task ordering based on dependencies and priority
  #[mcp::tool(
      name = "suggest_task_order",
      description = "Analyzes task dependencies and priorities to suggest optimal execution order",
      examples = [
          "What order should I complete these tasks?",
          "Prioritize my work for today"
      ]
  )]
  pub async fn suggest_task_order(
      /// Filter to specific project (optional)
      project: Option<String>,
      /// Maximum number of tasks to return
      limit: i64
  ) -> Vec<#{task: Task, score: f64, reasoning: String}> {
      let tasks = get_active_tasks(project).await?;
      
      // Calculate scores based on:
      // - Priority
      // - Number of dependent tasks
      // - Age
      // - Blocking status
      let scored = tasks.iter().map(|task| {
          let priority_score = task.get("priority") * 10.0;
          let blocker_penalty = if is_blocked(task).await? { -50.0 } else { 0.0 };
          let dependents = task.follow("blocks", 1).await?.len() * 5.0;
          let age_bonus = (today() - task.created_at).days() * 2.0;

          let score = priority_score + blocker_penalty + dependents + age_bonus;

          #{
              task: task,
              score: score,
              reasoning: generate_reasoning(task, score)
          }
      });

      scored.sort_by(|a, b| b.score <=> a.score);
      scored.take(limit).collect()
  }

  /// Semantic search with context enrichment
  #[mcp::tool(
      name = "search_notes_with_context",
      description = "Performs semantic search and enriches results with relevant context like backlinks and related notes",
      examples = [
          "Find notes about machine learning with context",
          "Search for 'rust async' and show related notes"
      ]
  )]
  pub async fn search_with_context(
      /// Search query
      query: String,
      /// Similarity threshold (0.0-1.0)
      threshold: f64,
      /// Include backlinks
      include_backlinks: bool,
      /// Include related notes
      include_related: bool
  ) -> Vec<#{note: Note, context: Context}> {
      let results = semantic_search(query, threshold).await?;

      // Enrich with context
      let enriched = results.iter().map(|note| async {
          let mut context = #{};

          if include_backlinks {
              context.backlinks = note.backlinks().await?;
              context.backlink_count = context.backlinks.len();
          }

          if include_related {
              context.related = note >> wikilink >> 2;
              context.related_tags = extract_common_tags(context.related);
          }

          #{ note: note, context: context }
      });

      enriched.collect().await
  }

  ---
  Auto-Generated JSON Schema

  The #[mcp::tool] macro generates:

  {
    "name": "find_blocked_tasks",
    "description": "Returns all high-priority tasks that are blocked by incomplete dependencies",
    "inputSchema": {
      "type": "object",
      "properties": {
        "project": {
          "type": "string",
          "description": "Project name to filter by (optional)",
          "nullable": true
        },
        "min_priority": {
          "type": "integer",
          "description": "Minimum priority threshold (1-10)",
          "minimum": 1,
          "maximum": 10
        }
      },
      "required": ["min_priority"]
    },
    "examples": [
      "Find blocked tasks for project 'Crucible'",
      "Show me what's blocking my high-priority work"
    ]
  }

  ---
  MCP Server Integration

  // ============================================================================
  // MCP SERVER (Rust side)
  // ============================================================================

  pub struct CrucibleMcpServer {
      plugin_manager: Arc<PluginManager>,
      tool_registry: Arc<ToolRegistry>,
  }

  impl CrucibleMcpServer {
      pub async fn handle_list_tools(&self) -> McpResponse {
          // Collect all tools from all loaded plugins
          let tools = self.tool_registry.list_all()
              .map(|tool| tool.to_mcp_schema())
              .collect();

          McpResponse::ToolsList { tools }
      }

      pub async fn handle_call_tool(
          &self,
          name: &str,
          arguments: Value
      ) -> McpResponse {
          // Find tool in registry
          let tool = self.tool_registry.get(name)?;

          // Execute in Rune VM
          let result = self.plugin_manager
              .call_tool_fn(tool.plugin_id, tool.function_name, arguments)
              .await?;

          McpResponse::ToolResult {
              content: vec![McpContent::Text {
                  text: serde_json::to_string_pretty(&result)?
              }]
          }
      }
  }

  ---
  Agent Usage Example

  // Claude/GPT using MCP to interact with Crucible

  // Agent discovers tools
  const tools = await mcp.listTools();
  // Returns: [find_blocked_tasks, suggest_task_order, search_with_context, ...]

  // Agent calls tool
  const blockedTasks = await mcp.callTool("find_blocked_tasks", {
      project: "Crucible",
      min_priority: 7
  });

  // Agent uses result to help user
  console.log(`Found ${blockedTasks.length} blocked high-priority tasks:`);
  blockedTasks.forEach(task => {
      console.log(`- ${task.title} (blocked by ${task.blockers.length} tasks)`);
  });

  // Agent can chain tools
  const taskOrder = await mcp.callTool("suggest_task_order", {
      project: "Crucible",
      limit: 5
  });

  // Agent provides intelligent recommendations
  console.log("Suggested task order:");
  taskOrder.forEach((item, i) => {
      console.log(`${i+1}. ${item.task.title}`);
      console.log(`   Score: ${item.score}`);
      console.log(`   Reasoning: ${item.reasoning}`);
  });

  ---
  Key Benefits

  1. Auto-Schema Generation - No manual JSON Schema writing
  2. Type Safety - Rune type system validated at compile time
  3. Discoverability - Agents can enumerate available tools
  4. Documentation - Inline docs become agent guidance
  5. Examples - Help agents learn tool usage patterns
  6. Hot Reload - Plugin updates instantly available to agents
  7. Sandboxed - Agent-invoked code runs in safe VM

  This completes the vision: Crucible plugins are both human-usable (via Rune scripts) AND agent-usable (via MCP tools).

> You seem to have forgotten that Rune is dynamically typed -- read our specific attribute macro, which takes the expected types of the function parameters and a description to generate the schema. 

  ---
  ü§ñ CORRECTED ADDENDUM: MCP-Compatible Agent Tools

  The Actual Implementation: Parameter Type Annotations

  Since Rune is dynamically typed, we can't infer parameter types from function signatures. Your #[rune_tool] macro solves this by requiring explicit type annotations in the attribute.

  ---
  Corrected Usage Pattern

  // ============================================================================
  // ACTUAL RUNE TOOL PATTERN (Based on crucible-rune-macros)
  // ============================================================================

  use crucible_rune_macros::rune_tool;

  #[rune_tool(
      desc = "Find high-priority tasks that are currently blocked by dependencies",
      category = "task",
      tags = ["tasks", "blocking", "priority"],
      params = {
          "project": {
              "type": "string?",  // Optional string
              "description": "Project name to filter by"
          },
          "min_priority": {
              "type": "number",
              "description": "Minimum priority threshold (1-10)",
              "default": 5
          }
      }
  )]
  pub async fn find_blocked_tasks(/* Rune VM will pass parameters */) -> RuneResult {
      // Inside function, extract from Rune VM context
      let args = vm_context.args();
      let project: Option<String> = args.get("project")?;
      let min_priority: i64 = args.get("min_priority")?;

      // Build query using Rune query DSL
      let query_script = format!(r#"
          use crucible::query::*;
          
          pub async fn execute() {{
              let mut query = notes
                  @ filter!("type" == "task")
                  @ filter!("priority" >= {});
              
              {}
              
              let tasks = query.collect().await?;
              
              // Filter for blocked
              tasks.iter()
                  .filter(|t| {{
                      let blockers = t.follow_reverse("blocks", 1).await?;
                      blockers.iter().any(|b| b.get("status") != "completed")
                  }})
                  .collect()
          }}
      "#,
          min_priority,
          project.map(|p| format!(r#"query = query @ filter!("project" == "{}");"#, p))
                 .unwrap_or_default()
      );

      // Execute Rune script
      vm.execute_script(&query_script).await
  }

  #[rune_tool(
      desc = "Semantic search with automatic context enrichment including backlinks and related notes",
      category = "search",
      async,
      params = {
          "query": {
              "type": "string",
              "description": "Search query text"
          },
          "threshold": {
              "type": "number",
              "description": "Similarity threshold (0.0-1.0)",
              "default": 0.7
          },
          "include_backlinks": {
              "type": "boolean",
              "description": "Include backlink information",
              "default": true
          },
          "include_related": {
              "type": "boolean",
              "description": "Include related notes (2-hop wikilinks)",
              "default": false
          }
      }
  )]
  pub async fn search_with_context() -> RuneResult {
      let args = vm_context.args();
      let query: String = args.get("query")?;
      let threshold: f64 = args.get("threshold")?;
      let include_backlinks: bool = args.get("include_backlinks")?;
      let include_related: bool = args.get("include_related")?;

      let query_script = format!(r#"
          use crucible::{{query::*, search::*}};
          
          pub async fn execute() {{
              let results = semantic_search("{}", {}).await?;
              
              // Enrich with context
              let enriched = [];
              for note in results {{
                  let context = #{{}};
                  
                  {}
                  
                  {}
                  
                  enriched.push(#{{{ 
                      note: note, 
                      context: context 
                  }}});
              }}
              
              enriched
          }}
      "#,
          query,
          threshold,
          if include_backlinks {
              r#"
              context.backlinks = note.backlinks().await?;
              context.backlink_count = context.backlinks.len();
              "#
          } else { "" },
          if include_related {
              r#"
              context.related = (note >> wikilink >> 2).collect().await?;
              "#
          } else { "" }
      );

      vm.execute_script(&query_script).await
  }

  ---
  Generated JSON Schema

  The macro reads the params attribute to generate proper JSON Schema:

  {
    "name": "find_blocked_tasks",
    "description": "Find high-priority tasks that are currently blocked by dependencies",
    "category": "task",
    "tags": ["tasks", "blocking", "priority"],
    "inputSchema": {
      "type": "object",
      "properties": {
        "project": {
          "type": ["string", "null"],
          "description": "Project name to filter by"
        },
        "min_priority": {
          "type": "number",
          "description": "Minimum priority threshold (1-10)",
          "default": 5
        }
      },
      "required": ["min_priority"]
    }
  }

  ---
  Type System Mapping

  // In the macro implementation:
  fn map_type_to_schema(type_str: &str) -> JsonSchema {
      match type_str {
          "string" => json!({"type": "string"}),
          "string?" => json!({"type": ["string", "null"]}),
          "number" => json!({"type": "number"}),
          "number?" => json!({"type": ["number", "null"]}),
          "boolean" => json!({"type": "boolean"}),
          "boolean?" => json!({"type": ["boolean", "null"]}),
          "array" => json!({"type": "array"}),
          "array?" => json!({"type": ["array", "null"]}),
          "object" => json!({"type": "object"}),
          "object?" => json!({"type": ["object", "null"]}),
          _ => panic!("Unknown type: {}", type_str)
      }
  }

  ---
  Pure Rune Plugin Tools

  For tools implemented entirely in Rune (not Rust), use a registration function:

  // ============================================================================
  // PLUGIN TOOLS IN PURE RUNE
  // ============================================================================

  use crucible::{{Plugin, tool::register}};

  pub const METADATA = Plugin::new(
      "task-analyzer",
      "Task Analysis Tools",
      "1.0.0"
  );

  pub async fn init(ctx) {
      // Register tools with metadata
      register(ctx, #{
          name: "suggest_task_order",
          description: "Analyzes task dependencies and priorities to suggest optimal execution order",
          category: "task",
          tags: ["tasks", "priority", "dependencies"],
          params: #{
              project: #{
                  type: "string?",
                  description: "Filter to specific project"
              },
              limit: #{
                  type: "number",
                  description: "Maximum number of tasks to return",
                  default: 10
              }
          },
          handler: suggest_task_order
      }).await?;

      Ok(())
  }

  async fn suggest_task_order(args) {
      let project = args.get("project");
      let limit = args.get("limit").unwrap_or(10);

      // Query using DSL
      let mut query = notes
          @ filter!("type" == "task")
          @ filter!("status" == "active");

      if let Some(proj) = project {
          query = query @ filter!("project" == proj);
      }

      let tasks = query.collect().await?;

      // Calculate scores
      let scored = [];
      for task in tasks {
          let priority_score = task.get("priority") * 10.0;

          // Check if blocked
          let blockers = task.follow_reverse("blocks", 1).await?;
          let blocker_penalty = if blockers.iter().any(|b| b.get("status") != "completed") {
              -50.0
          } else {
              0.0
          };

          // Count dependents
          let dependents = task.follow("blocks", 1).await?;
          let dependent_bonus = dependents.len() * 5.0;

          // Age factor
          let age_days = (today() - task.created_at).days();
          let age_bonus = age_days * 2.0;

          let total_score = priority_score + blocker_penalty + dependent_bonus + age_bonus;

          scored.push(#{
              task: task,
              score: total_score,
              reasoning: `Priority: ${task.get("priority")}, Dependents: ${dependents.len()}, Age: ${age_days} days${if blocker_penalty < 0 { ", BLOCKED" } else { "" }}`
          });
      }

      // Sort and limit
      scored.sort_by(|a, b| b.score <=> a.score);
      scored.take(limit).collect()
  }

  ---
  Key Differences from Initial Design

  1. Type annotations in attribute - Not inferred from signature (because Rune is dynamic)
  2. Simpler param syntax - JSON-like structure in the macro
  3. VM context for args - Parameters passed through Rune VM, not Rust signature
  4. Script generation - Tools often generate and execute Rune scripts
  5. Registration function - Pure Rune tools register metadata explicitly

  This approach is actually more flexible because:
  - ‚úÖ Supports all Rune types without Rust wrapper types
  - ‚úÖ Tools can be pure Rune or Rust
  - ‚úÖ Schema is explicit and auditable
  - ‚úÖ No complex type mapping needed
  - ‚úÖ Works perfectly with dynamic typing

  The trade-off is slightly more verbose (explicit type annotations), but this is the right design for a dynamically-typed language.
