# Rust Inference Engines & Model Formats Research

## Summary: Best Options for AMD APU (Vulkan/ROCm)

**Highest Impact/Work Ratio:**
1. **FastEmbed (already integrated)** - ONNX Runtime, CPU-only but works now
2. **Candle** - SafeTensors/GGUF, CUDA/Metal/CPU, Vulkan experimental
3. **ort (ONNX Runtime)** - Direct ONNX, CUDA/ROCm/DirectML, but requires GPU build
4. **Burn (current work)** - Custom, Vulkan via wgpu, full control but needs implementation

---

## 1. FastEmbed (Already Integrated)

**Status:** ‚úÖ Already in codebase, working

**Model Formats:**
- ONNX models (pre-converted, auto-downloaded from HuggingFace)
- Limited to ~18 pre-configured models (BGE, Nomic, MiniLM, E5, etc.)

**GPU Backends:**
- ‚ùå **CPU-only** - Uses ONNX Runtime CPU build
- ‚ö†Ô∏è Could use ONNX Runtime GPU build, but requires separate `ort` integration

**Pros:**
- ‚úÖ Already working, zero integration work
- ‚úÖ Production-ready, battle-tested
- ‚úÖ Auto-downloads models, handles caching
- ‚úÖ Good performance on CPU (5k-14k sentences/sec)
- ‚úÖ Simple API, async-friendly

**Cons:**
- ‚ùå No GPU acceleration (CPU-only)
- ‚ùå Limited model selection (only pre-converted ONNX models)
- ‚ùå Can't use arbitrary SafeTensors/GGUF models
- ‚ùå ONNX Runtime GPU would require separate `ort` crate integration

**Work Required:** 0 (already done)

**Impact/Work:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (infinite - already works)

---

## 2. Candle (HuggingFace)

**Status:** üîÑ Not integrated, actively maintained

**Model Formats:**
- ‚úÖ **SafeTensors** (native, excellent support)
- ‚úÖ **GGUF** (via `candle-core`, good support)
- ‚úÖ **PyTorch .safetensors** (via HuggingFace Hub)
- ‚úÖ **ONNX** (limited, via conversion)

**GPU Backends:**
- ‚úÖ **CUDA** (stable, well-supported)
- ‚úÖ **Metal** (Apple Silicon, stable)
- ‚úÖ **CPU** (ndarray backend, stable)
- ‚ùå **Vulkan** (NOT implemented - only experimental fork by niklasha, stalled)
- ‚ùå **ROCm** (NOT officially supported - only POC by vberthet, has issues)

**Real-World Status (from GitHub Issues):**
- **Vulkan (Issue #1810)**: Open since March 2024, no official support. One contributor (niklasha) started work in private fork for OpenBSD/Radeon but stalled. No shader caching, no production-ready implementation.
- **ROCm (Issue #346)**: Open issue, POC exists but:
  - Only works on specific GPU architectures (gfx1030/RDNA2 tested)
  - Has issues with APUs (HIP selects embedded GPU incorrectly)
  - Examples hang on some GPUs (gfx1102/RX7600 reported)
  - Requires hardcoded GPU arch in build.rs
  - Not production-ready

**Pros:**
- ‚úÖ Native SafeTensors support (perfect for HuggingFace models)
- ‚úÖ GGUF support built-in
- ‚úÖ Actively maintained by HuggingFace
- ‚úÖ Good documentation and examples
- ‚úÖ Embedding models work well (BGE, Nomic, etc.)
- ‚úÖ Can load arbitrary models from HuggingFace Hub
- ‚úÖ CUDA and Metal work great

**Cons:**
- ‚ùå **No Vulkan support** (only stalled experimental fork)
- ‚ùå **No ROCm support** (only broken POC, especially problematic for APUs)
- ‚ö†Ô∏è Requires implementing transformer forward pass (but examples exist)
- ‚ö†Ô∏è More complex than FastEmbed (but more flexible)
- ‚ùå **Not viable for AMD APU/Strix Halo** - no Vulkan or ROCm support

**Work Required:** Medium (2-3 days) - but **only for CUDA/Metal/CPU**
- Add `candle-core`, `candle-nn`, `candle-transformers`
- Implement embedding provider wrapper
- Load SafeTensors models, run forward pass
- Test with actual models

**Impact/Work:** ‚≠ê‚≠ê (low impact for AMD APU - no GPU support)

**AMD APU Compatibility:** ‚ùå **Not viable** - No Vulkan or ROCm support. CPU-only on AMD APU.

---

## 3. ort (ONNX Runtime Rust Bindings)

**Status:** üîÑ Not integrated, actively maintained

**Model Formats:**
- ‚úÖ **ONNX** (native, excellent support)
- ‚ö†Ô∏è **SafeTensors** (via conversion to ONNX)
- ‚ùå **GGUF** (not supported)

**GPU Backends:**
- ‚úÖ **CUDA** (stable, requires CUDA build)
- ‚úÖ **ROCm** (experimental, requires ROCm build)
- ‚úÖ **DirectML** (Windows, stable)
- ‚úÖ **TensorRT** (NVIDIA, optional)
- ‚úÖ **CPU** (default, stable)

**Pros:**
- ‚úÖ Production-grade ONNX Runtime (Microsoft-backed)
- ‚úÖ ROCm support exists (experimental but available)
- ‚úÖ Excellent performance optimizations
- ‚úÖ Can use any ONNX model (not limited to pre-converted)
- ‚úÖ Direct GPU acceleration

**Cons:**
- ‚ö†Ô∏è Requires building ONNX Runtime with GPU support (complex)
- ‚ö†Ô∏è ROCm build is experimental (may have issues)
- ‚ùå No GGUF support
- ‚ö†Ô∏è Need to convert SafeTensors ‚Üí ONNX (extra step)
- ‚ö†Ô∏è More complex setup than FastEmbed

**Work Required:** Medium-High (3-5 days)
- Add `ort` crate
- Build ONNX Runtime with ROCm support (or use pre-built)
- Implement embedding provider
- Handle model conversion if needed

**Impact/Work:** ‚≠ê‚≠ê‚≠ê (good impact, higher work)

**AMD APU Compatibility:** ‚ö†Ô∏è ROCm experimental, may not work on APU

---

## 4. Tract (ONNX/TensorFlow Runtime)

**Status:** üîÑ Not integrated, actively maintained

**Model Formats:**
- ‚úÖ **ONNX** (native, excellent support)
- ‚úÖ **TensorFlow** (native)
- ‚úÖ **TensorFlow Lite** (native)
- ‚ùå **SafeTensors** (not directly, need conversion)
- ‚ùå **GGUF** (not supported)

**GPU Backends:**
- ‚ùå **CPU-only** (no GPU acceleration)
- ‚ùå No CUDA, ROCm, Vulkan support

**Pros:**
- ‚úÖ Pure Rust implementation (no C++ dependencies)
- ‚úÖ Small binary size
- ‚úÖ Good ONNX support
- ‚úÖ Simple API

**Cons:**
- ‚ùå **No GPU acceleration** (CPU-only)
- ‚ùå No GGUF support
- ‚ùå Less optimized than ONNX Runtime
- ‚ùå Not ideal for GPU workloads

**Work Required:** Low (1-2 days)
- Add `tract` crate
- Implement embedding provider
- Load ONNX models

**Impact/Work:** ‚≠ê‚≠ê (low impact - CPU-only, no GPU benefit)

**AMD APU Compatibility:** ‚ùå CPU-only, no GPU acceleration

---

## 5. llama.cpp + Rust Bindings (embellama, llama_cpp, etc.)

**Status:** üîÑ Not integrated, community-maintained

**Model Formats:**
- ‚úÖ **GGUF** (native, excellent support)
- ‚ùå **SafeTensors** (not supported - would need conversion)
- ‚ùå **ONNX** (not supported)

**GPU Backends:**
- ‚úÖ **CUDA** (via llama.cpp, stable)
- ‚ö†Ô∏è **Vulkan** (via llama.cpp, **EXPERIMENTAL** - merged in 2024)
- ‚ö†Ô∏è **ROCm** (via llama.cpp, **EXPERIMENTAL** - exists but limited)
- ‚úÖ **CPU** (via llama.cpp, stable)

**Rust Bindings Available:**
- `embellama` (0.8.0) - **Specifically for embeddings** using llama-cpp
- `llama_cpp` (0.3.2) - High-level bindings
- `llama-cpp-4` (0.1.94) - Lower-level bindings
- `rs-llama-cpp` (0.1.67) - Automated bindings

**Pros:**
- ‚úÖ **Vulkan support exists** (experimental but merged into llama.cpp)
- ‚úÖ **ROCm support exists** (experimental)
- ‚úÖ Excellent GGUF support (native format)
- ‚úÖ Can use any GGUF embedding model
- ‚úÖ `embellama` crate specifically designed for embeddings
- ‚úÖ Mature C++ codebase (llama.cpp)
- ‚úÖ Good performance on CPU, CUDA works well

**Cons:**
- ‚ö†Ô∏è **Vulkan/ROCm are EXPERIMENTAL** (may have issues)
- ‚ùå No SafeTensors support (need to convert models)
- ‚ö†Ô∏è Primarily designed for LLMs, embedding support is secondary
- ‚ö†Ô∏è llama.cpp C++ dependency (FFI complexity)
- ‚ö†Ô∏è Experimental GPU backends may not work on all hardware
- ‚ö†Ô∏è Need to verify embedding model compatibility with Vulkan/ROCm

**Work Required:** Medium (2-3 days)
- Add `embellama` or `llama_cpp` crate
- Implement embedding provider wrapper
- Test with GGUF embedding models
- Verify Vulkan/ROCm actually works for embeddings (not just LLMs)

**Impact/Work:** ‚≠ê‚≠ê‚≠ê‚≠ê (potentially high impact if Vulkan/ROCm work)

**AMD APU Compatibility:** ‚ö†Ô∏è **POTENTIALLY VIABLE** - Vulkan/ROCm support exists but experimental. Need to test.

---

## 6. Burn (Current Work)

**Status:** üîÑ Partially integrated, in progress

**Model Formats:**
- ‚úÖ **SafeTensors** (via `safetensors` crate, manual loading)
- ‚ö†Ô∏è **GGUF** (basic parsing, full inference not implemented)
- ‚ùå **ONNX** (not supported)

**GPU Backends:**
- ‚úÖ **Vulkan** (via wgpu, stable)
- ‚úÖ **CUDA** (experimental)
- ‚ö†Ô∏è **ROCm** (experimental)
- ‚úÖ **CPU** (ndarray backend, stable)

**Pros:**
- ‚úÖ **Vulkan support** (perfect for AMD APU via wgpu)
- ‚úÖ Full control over inference pipeline
- ‚úÖ Pure Rust, no C++ dependencies
- ‚úÖ Can implement custom optimizations
- ‚úÖ SafeTensors loading already implemented

**Cons:**
- ‚ùå **High implementation work** (need to build transformer forward pass)
- ‚ùå GGUF inference not implemented (only discovery)
- ‚ö†Ô∏è ROCm support experimental
- ‚ö†Ô∏è Less mature than other options
- ‚ö†Ô∏è Need to implement attention, layer norm, etc.

**Work Required:** High (1-2 weeks)
- Implement full BERT/transformer forward pass
- Implement GGUF tensor reading and inference
- Test with actual models
- Optimize for performance

**Impact/Work:** ‚≠ê‚≠ê‚≠ê (high impact, but very high work)

**AMD APU Compatibility:** ‚úÖ Vulkan via wgpu should work

---

## 7. Direct llama.cpp Bindings

**Status:** üîÑ Not integrated, community-maintained

**Model Formats:**
- ‚úÖ **GGUF** (native, excellent support)
- ‚ùå **SafeTensors** (not supported)

**GPU Backends:**
- ‚úÖ **CUDA** (via llama.cpp)
- ‚ö†Ô∏è **ROCm** (via llama.cpp HIP backend, experimental)
- ‚ö†Ô∏è **Vulkan** (via llama.cpp, experimental)
- ‚úÖ **CPU** (stable)

**Pros:**
- ‚úÖ Excellent GGUF support
- ‚úÖ ROCm support exists (experimental)
- ‚úÖ Vulkan support exists (experimental)
- ‚úÖ Very mature C++ codebase

**Cons:**
- ‚ùå C++ dependency (not pure Rust)
- ‚ö†Ô∏è ROCm/Vulkan backends are experimental
- ‚ùå No SafeTensors support
- ‚ö†Ô∏è Primarily for LLMs, embedding models less common
- ‚ö†Ô∏è FFI complexity

**Work Required:** Medium-High (3-4 days)
- Add llama.cpp bindings crate
- Build llama.cpp with ROCm/Vulkan
- Implement embedding provider
- Handle FFI complexity

**Impact/Work:** ‚≠ê‚≠ê‚≠ê (good for GGUF, but experimental GPU backends)

**AMD APU Compatibility:** ‚ö†Ô∏è ROCm/Vulkan experimental, may not work

---

## Recommendations for AMD APU (128GB unified RAM)

### Short-term (Immediate):
1. **Keep FastEmbed** - Already works, CPU-only but functional
2. ~~**Add Candle with Vulkan**~~ - **NOT VIABLE** - No Vulkan support exists (only stalled experimental fork)

### Medium-term (If Candle Vulkan works):
1. **Complete Burn implementation** - Full control, Vulkan native
   - Implement transformer forward pass
   - Add GGUF inference
   - High work, but best long-term solution

### Long-term (If needed):
1. **ort with ROCm** - If ROCm support improves for APUs
2. **llama.cpp with Vulkan** - If GGUF becomes primary format

---

## Model Format Support Matrix

| Engine | SafeTensors | GGUF | ONNX | Notes |
|--------|-------------|------|------|-------|
| **FastEmbed** | ‚ùå | ‚ùå | ‚úÖ | Pre-converted models only |
| **Candle** | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | Native SafeTensors, good GGUF |
| **ort** | ‚ö†Ô∏è | ‚ùå | ‚úÖ | Need conversion for SafeTensors |
| **Tract** | ‚ùå | ‚ùå | ‚úÖ | CPU-only |
| **llama-rs** | ‚ùå | ‚úÖ | ‚ùå | GGUF native |
| **Burn** | ‚úÖ | ‚ö†Ô∏è | ‚ùå | SafeTensors native, GGUF partial |
| **llama.cpp** | ‚ùå | ‚úÖ | ‚ùå | GGUF native |

---

## GPU Backend Support Matrix (AMD APU)

| Engine | Vulkan | ROCm | CUDA | CPU | Notes |
|--------|--------|------|------|-----|-------|
| **FastEmbed** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | CPU-only |
| **Candle** | ‚ùå | ‚ùå | ‚úÖ | ‚úÖ | No Vulkan/ROCm support |
| **ort** | ‚ùå | ‚ö†Ô∏è | ‚úÖ | ‚úÖ | ROCm experimental |
| **Tract** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | CPU-only |
| **llama.cpp** | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚úÖ | ‚úÖ | Vulkan/ROCm experimental |
| **Burn** | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚úÖ | Vulkan stable via wgpu |
| **llama.cpp** | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚úÖ | ‚úÖ | Vulkan/ROCm experimental |

---

## Final Recommendation

**For AMD APU with Vulkan/ROCm requirements:**

1. **Immediate:** Keep FastEmbed (CPU-only, but works now)
2. **Next step (TEST FIRST):** Try **llama.cpp + embellama** with Vulkan/ROCm
   - Vulkan/ROCm support exists (experimental but merged)
   - `embellama` crate specifically for embeddings
   - GGUF format (need to convert SafeTensors if needed)
   - Medium work, potentially high impact if it works
   - **TEST THIS FIRST** - might actually work!

3. **If llama.cpp doesn't work:** Complete **Burn implementation**
   - Full control, Vulkan native via wgpu
   - Should work on AMD APU (wgpu supports Vulkan)
   - More work, but guaranteed to work
   - SafeTensors support already implemented

**Avoid:**
- ‚ùå **Candle** - No Vulkan/ROCm support (only stalled experimental forks)
- ‚ùå ort with ROCm (experimental, may not work on APU)
- ‚ùå Tract (CPU-only)
