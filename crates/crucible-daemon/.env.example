# Ollama configuration for integration tests
# Copy this file to .env and configure for your environment

# Ollama server endpoint
# Local Ollama server (default)
OLLAMA_ENDPOINT=http://localhost:11434

# Remote Ollama server (uncomment if using remote server)
# OLLAMA_ENDPOINT=https://llama.krohnos.io

# Ollama embedding model
# Default model for local testing
OLLAMA_MODEL=nomic-embed-text-v1.5-q8_0

# Alternative models (uncomment to use)
# OLLAMA_MODEL=nomic-embed-text
# OLLAMA_MODEL=mxbai-embed-large

# Optional: Request timeout in seconds
# OLLAMA_TIMEOUT=30

# Optional: Enable debug logging for embedding requests
# OLLAMA_DEBUG=true