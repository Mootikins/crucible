# crucible-llm

LLM and AI integration library for Crucible knowledge management system.

## Features

- **Embeddings**: Text embeddings for semantic search with multiple providers
- **Multi-Provider**: Support for Ollama, OpenAI, and custom endpoints
- **Type-Safe**: Compile-time safety with trait-based design
- **Production-Ready**: Health checks, model discovery, dimension validation
- **Rich Metadata**: Full response context (model, dimensions, token counts)
- **Async**: Built on tokio for high performance

## Quick Start

```rust
use crucible_llm::embeddings::{EmbeddingConfig, EmbeddingProvider, OllamaProvider};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Configure provider
    let config = EmbeddingConfig::ollama(
        Some("https://llama.krohnos.io".to_string()),
        Some("nomic-embed-text-v1.5-q8_0".to_string()),
    );

    // Create provider
    let provider = OllamaProvider::new(config)?;

    // Health check
    provider.health_check().await?;

    // Generate embedding
    let response = provider.embed("Hello, world!").await?;

    println!("Model: {}", response.model);
    println!("Dimensions: {}", response.dimensions);
    println!("Tokens: {:?}", response.tokens);
    println!("Embedding: {:?}", &response.embedding[..5]); // First 5 values

    Ok(())
}
```

## Providers

### Ollama

Local LLM server with no API key required. Supports model discovery via `/api/tags`.

```rust
let config = EmbeddingConfig::ollama(
    Some("http://localhost:11434".to_string()),
    Some("nomic-embed-text".to_string()),
);

let provider = OllamaProvider::new(config)?;

// List available models
let models = provider.list_models().await?;
for model in models {
    println!("{} - {} dimensions", model.name, model.dimensions.unwrap_or(0));
}
```

### OpenAI

OpenAI's embedding API with comprehensive error handling.

```rust
let config = EmbeddingConfig::openai(
    "your-api-key".to_string(),
    Some("text-embedding-3-small".to_string()),
);

let provider = OpenAIProvider::new(config)?;
let response = provider.embed("Query text").await?;
```

## Batch Processing

Efficiently embed multiple documents:

```rust
let texts = vec![
    "First document".to_string(),
    "Second document".to_string(),
    "Third document".to_string(),
];

let responses = provider.embed_batch(texts).await?;
for (i, response) in responses.iter().enumerate() {
    println!("Document {} - {} dimensions", i, response.dimensions);
}
```

## Model Discovery

Query available models with rich metadata:

```rust
let models = provider.list_models().await?;

for model in models {
    println!("Model: {}", model.name);
    if let Some(dims) = model.dimensions {
        println!("  Dimensions: {}", dims);
    }
    if let Some(family) = &model.family {
        println!("  Family: {:?}", family);
    }
    if let Some(params) = &model.parameter_size {
        println!("  Parameters: {}", params);
    }
}
```

## Error Handling

Comprehensive error types with retry hints:

```rust
use crucible_llm::embeddings::{EmbeddingError, EmbeddingResult};

match provider.embed("text").await {
    Ok(response) => println!("Success: {} dims", response.dimensions),
    Err(EmbeddingError::AuthenticationError(msg)) => {
        eprintln!("Auth failed: {}", msg);
    }
    Err(EmbeddingError::RateLimitExceeded { retry_after_secs }) => {
        eprintln!("Rate limited. Retry after {} seconds", retry_after_secs);
    }
    Err(EmbeddingError::Timeout { timeout_secs }) => {
        eprintln!("Timeout after {} seconds", timeout_secs);
    }
    Err(e) => eprintln!("Error: {}", e),
}
```

## Configuration

### Builder-Style Configuration

```rust
let config = EmbeddingConfig {
    provider: ProviderType::Ollama,
    endpoint: "https://llama.krohnos.io".to_string(),
    model: "nomic-embed-text-v1.5-q8_0".to_string(),
    api_key: None,
    timeout_secs: 30,
    max_retries: 3,
    batch_size: 100,
};
```

### Convenience Methods

```rust
// Ollama
let config = EmbeddingConfig::ollama(
    Some("http://localhost:11434".to_string()),
    Some("nomic-embed-text".to_string()),
);

// OpenAI
let config = EmbeddingConfig::openai(
    "sk-...".to_string(),
    Some("text-embedding-3-small".to_string()),
);
```

## Testing

Mock provider for testing without external dependencies:

```rust
#[cfg(test)]
mod tests {
    use crucible_llm::embeddings::mock::MockEmbeddingProvider;

    #[tokio::test]
    async fn test_embedding() {
        let provider = MockEmbeddingProvider::with_dimensions(768);
        let response = provider.embed("test").await.unwrap();

        assert_eq!(response.dimensions, 768);
        assert_eq!(response.model, "mock-test-model");
    }
}
```

## Architecture

### Trait-Based Design

The `EmbeddingProvider` trait enables swapping providers without code changes:

```rust
#[async_trait]
pub trait EmbeddingProvider: Send + Sync {
    async fn embed(&self, text: &str) -> EmbeddingResult<EmbeddingResponse>;
    async fn embed_batch(&self, texts: Vec<String>) -> EmbeddingResult<Vec<EmbeddingResponse>>;
    fn provider_name(&self) -> &str;
    fn model_name(&self) -> &str;
    fn dimensions(&self) -> usize;
    async fn list_models(&self) -> EmbeddingResult<Vec<ModelInfo>>;
    async fn health_check(&self) -> EmbeddingResult<bool>;
}
```

### Response Structure

Rich metadata for every embedding:

```rust
pub struct EmbeddingResponse {
    pub embedding: Vec<f32>,         // The actual embedding vector
    pub model: String,                // Model used
    pub dimensions: usize,            // Vector dimensions
    pub tokens: Option<usize>,        // Token count
    pub metadata: Option<Value>,      // Provider-specific metadata
}
```

## Roadmap

- **Phase 1 (Current)**: Embeddings with Ollama and OpenAI âœ…
- **Phase 2 (Q2 2025)**: Chat completions and streaming
- **Phase 3 (Q3 2025)**: Vision, function calling, structured output
- **Future**: Fine-tuning, RAG patterns, multi-modal support

## Design Validation

This crate's architecture was validated by rust-architect analysis as **best-in-class** for embeddings:
- Superior metadata handling vs external libraries
- Cleaner trait design than `genai` and `llm` crates
- Production-grade error handling with retry hints
- Type-safe compile-time guarantees

## License

Proprietary - Part of the Crucible knowledge management system.
