[package]
name = "crucible-llm"
version.workspace = true
authors.workspace = true
license.workspace = true
edition.workspace = true
rust-version.workspace = true
description = "Embedding providers for Crucible - local (FastEmbed, llama.cpp, Burn) and cloud (Ollama, OpenAI)"

[lib]
name = "crucible_llm"
path = "src/lib.rs"

[features]
# Default features - include fastembed for easy out-of-box experience
default = ["fastembed"]

# Enable test utilities (MockEmbeddingProvider, MockConstrainedProvider) for integration tests
test-utils = []

# Enable e2e tests that require local model configuration (GGUF models in ~/models/)
# These tests are skipped unless this feature is enabled AND a local model exists
local-config = ["llama-cpp"]

# Enable local embedding tests (requires local embedding model)
local-config-embedding = ["llama-cpp"]

# Test tier flags (see justfile for usage)
test-fixtures = []       # Tests using docs/ kiln or examples/test-kiln
test-infrastructure = [] # Tests requiring Ollama, embedding endpoints
test-slow = []           # Performance benchmarks and timing tests

# FastEmbed local embedding provider (CPU, easy setup, no API keys needed)
fastembed = ["dep:fastembed", "dep:ort"]

# Burn embedding provider with different backend options
burn = ["dep:burn", "dep:burn-tensor", "dep:burn-import"]
burn-cpu = ["burn", "dep:burn-ndarray"]
burn-vulkan = ["burn", "dep:burn-wgpu"]
burn-rocm = ["burn", "dep:burn-hip"]
# Default burn feature enables CPU fallback
burn-gpu = ["burn-vulkan"]  # Alias for most common GPU backend

# llama.cpp backend for GGUF models with different GPU backends
# Using llama-cpp-2 crate which has much newer llama.cpp bindings (supports nomic-embed v2 etc.)
llama-cpp = ["dep:llama-cpp-2"]
llama-cpp-vulkan = ["llama-cpp", "llama-cpp-2/vulkan"]
llama-cpp-cuda = ["llama-cpp", "llama-cpp-2/cuda"]
llama-cpp-metal = ["llama-cpp", "llama-cpp-2/metal"]
# Default GPU backend alias - Vulkan works on AMD, Intel, and NVIDIA
llama-cpp-gpu = ["llama-cpp-vulkan"]
# Note: llama-cpp-2 doesn't have hipblas feature - use vulkan for AMD GPUs
# llama-cpp-rocm = ["llama-cpp", "llama-cpp-2/hipblas"]


[dependencies]
# Core types and traits
crucible-core = { path = "../crucible-core" }

# Config
crucible-config = { path = "../crucible-config", features = ["toml"] }

# Burn ML framework dependency for GPU-accelerated embeddings
# Main burn crate with optional features (pinned to 0.19.1 for consistency)
burn = { version = "0.19.1", optional = true }
burn-tensor = { version = "0.19.1", optional = true }
burn-ndarray = { version = "0.19.1", optional = true }  # For CPU fallback

# GPU backends (each optional, selected via features)
burn-wgpu = { version = "0.19.1", optional = true }   # Vulkan/Metal/WebGPU via wgpu
burn-hip = { version = "0.16", optional = true }    # AMD ROCm/HIP backend (0.16 is latest stable)

# Import safetensors directly into Burn models
burn-import = { version = "0.19.1", optional = true }

# HuggingFace tokenizers for text tokenization
# Note: We disable default features to exclude esaxx_fast, which has a hardcoded
# static_crt(true) in its build script causing runtime library mismatch with
# ort_sys (ONNX Runtime) on Windows. The esaxx_fast feature provides faster
# suffix array construction but is not essential for tokenization.
tokenizers = { version = "0.22", default-features = false, features = ["progressbar", "onig"] }

# SafeTensors for model loading
safetensors = "0.7"

# GGUF file format support
gguf = "0.1"

# llama.cpp bindings for GGUF model inference (optional, feature-gated)
# Use llama-cpp-2 which has much newer llama.cpp (from utilityai/llama-cpp-rs)
llama-cpp-2 = { version = "0.1", optional = true }

# For model file operations
walkdir = "2.5"

# Async
tokio = { workspace = true, features = ["time"] }
async-trait = { workspace = true }
futures = "0.3"
async-stream = "0.3"

# Error handling
anyhow = { workspace = true }
thiserror = { workspace = true }

# Serialization
serde = { workspace = true, features = ["derive"] }
serde_json = { workspace = true }

# HTTP client for API requests
reqwest = { workspace = true }

# Time handling
chrono = { workspace = true, features = ["serde"] }

# UUID generation
uuid = { workspace = true }

# Logging
tracing = { workspace = true }

# FastEmbed for local embeddings and reranking (optional, feature-gated)
# Use default-features=false to avoid native-tls (OpenSSL) dependency;
# we manually enable rustls-based TLS via hf-hub-rustls-tls + ort features below
fastembed = { version = "5.8", optional = true, default-features = false, features = ["hf-hub-rustls-tls", "image-models"] }

# ORT runtime binary downloads use rustls instead of native-tls (avoids OpenSSL)
ort = { version = "=2.0.0-rc.11", default-features = false, features = ["std", "ndarray", "download-binaries", "tls-rustls", "copy-dylibs"], optional = true }

# Path expansion (for tilde expansion)
shellexpand = "3.1"

# Directory utilities
dirs = "6.0"

# CPU detection for thread count
num_cpus = "1.17"

[dev-dependencies]
tokio-test = "0.4"
mockito = "1.7"
tempfile = { workspace = true }
tracing-subscriber = { workspace = true }
toon-format = "0.4"
criterion = { workspace = true, features = ["html_reports"] }
serial_test = "3.3"
wiremock = { workspace = true }
futures-util = { workspace = true }
proptest = "1.8"

